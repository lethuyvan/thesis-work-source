I'd check over at /r/amd for the motherboard option. The 8600k/2600X is a close call. Out of the box I prefer the 2600X as it comes with a good box cooler. So that's less investment. But if you plan to OC, you'll want a good cooler for either (there goes the price advantage), and the 8600k has more headroom.

So if OC is your thing, 8600k > 2600X IMO. For out of box, the 2600X is simply a better value at slightly reduced performance.
Nope. For you, it would be just like the GN246HL - no adaptive sync feature.
This is a guess, but I would suspect that the pump is failing if the behavior is intermittent.

The reported pump speed, IIRC, is what the system is TRYING to run it at, not what it is actually running at. But, I could be wrong.
I'm a huge fan of the 8700k and the 8600k. Despite that, I would urge you to consider the Ryzen 5 2600 ($199) or 2600X ($229). They are both 6c/12t, and while gaming performance is slightly inferior to the 8600k, you also save by not having to buy the 3rd party cooler. The cooler supplied is more than adequate for the rated boost clocks, especially in the case of the 2600X.

That said, they're all great options, so whatever you choose, enjoy. Below are my other critiques on your build:

-----

PSU - The EVGA G3 series is excellent, but they've been surpassed. If I were buying today, I'd get the SeaSonic Focus Plus series. Normally I'd recommend the Gold 550, but that's back up to MSRP and the Plat version is on sale at Newegg for $99.99 ($79.99 after MIR). You get a higher quality power supply. As for wattage, your total system draw will be ~350W with that build (assuming 8700k), likely less. A 550W is more than enough.

https://www.newegg.com/Product/Product.aspx?Item=N82E16817151193&ignorebbr=1

Monitor - The Acer GN246HL is an older model and one of the worst 144hz displays. Considering how much money you're spending elsewhere for a premium gaming experience, you're really cheaping out on the item you're going to be looking at the most. If you can swing it, go for the Dell S2417DG. Cheaper options that would be an improvement over the Acer include the ViewSonic XG2401/2402 and the Samsung C24FG73.

The Dell is a higher quality (8-bit, improved viewing angles) 1440p, 144hz TN panel with support for a 165hz overclock and G-Sync. The Viewsonic is today's preferred TN alternative to your listed Acer. It sports FreeSync, which you won't use today but could be useful if AMD ever offers a GPU that interests you. The Samsung is comparable to the ViewSonic in many ways, but offers a VA panel for deeper blacks, better contrast, better viewing angles and color, but at the expense of slightly more motion blur. It too is a FreeSync option that you won't make immediate use of.

-----

Enjoy the build :)
Look up "Noctua Chromax."

They are black versions of their more popular fans with interchangeable rubber corners in 6 different colors. They also sell Chromax kits for their CPU coolers.
I just bought 4 Chromax fans for my case. It was world changing. I now see why people spend so much on these.
Chromax...
> The same fans in black would be enough.

Noctua Chromax is a thing. Just not for these fans yet.
I'm sure they'll have a Chromax variant in short order.
Semantics. Yes, there are things that could deprive you of it. I could bet it all on the wrong mutual fund and deprive myself of most of my 401k. Doesn't make it a welfare program.

Welfare programs are paid out to people who may not have paid into them, using a common pool (federal taxes). Earned benefits are programs like SS and Medicare where people who pay directly into them are the ones who eventually draw from it.

Also, SS/Medicare aren't part of the standard budget because they are self funding.
Can you point me to an official website for the product, and a US web store listing from an official retailer?
Those aren't welfare programs. You get benefits that you paid into.

You are confusing earned benefits with welfare.
> Yeah it's crazy. I'm one of the few with the rx480. I like the card, but in the end regret it a bit and would have preferred the 1060. I thought at the time the extra 2 GB memory and better Vulkan and dx12 performance (negligible) would future proof it better. Stupid lol.

You weren't stupid, as certain echo chambers repeated that ad nauseum.

I actually bought 2 GTX 1060s and 2 RX 480s. I returned one 480 as defective, and thanks to out of whack supply/demand, I sold two cards at a profit, keeping the one I wanted (and effectively paying little for it). I fully expected to prefer the RX 480, but ended up liking the 1060 more.
No, I thought that the speculation was too high (or that the people speculating were high themselves). But I did expect more than $30, when current versions go for $20-$25.
I was shocked at the A12 price today. It's only $5-$10 more than I paid for my Chromax fans. Rumors pegged it at $80-$100 per fan due to a 4x uptick in cost of materials (not to mention the huge R&D they put into these fans).
As I noted in my first post, it's well known that Steam's data is not reflective of the total market. There are more variables than I could possibly cover in this space, so I'll just note a few.

1. The results favor the enthusiast. I have a GTX 1060, and my wife has a GTX 950. We both have Steam accounts, but we don't run them at startup, only when in use. I use it more than her. I've done 3 HW surveys in the last 2 years. She's done zero. Those who run at startup and in the background likely got far more survey invites than me. Bottom line is that enthusiasts are over-represented and casuals are under-represented in these surveys. That said, the 1060 and 480/580 target the same market, so this point doesn't apply for this comparison.

2. Miners buy a large portion of GPUs that never make it into these surveys. I'm not talking about people like me who mine on existing hardware, but GPU mining farms that own hundreds or thousands of GPUs. I suspect that this would slightly close the gap between AMD and Nvidia, because AMD hardware was in more demand for miners both before and during the boom, which means more unreported hardware. Despite this, it would be highly unlikely for AMD to have outsold Nvidia based on the information that we have. But the 12:1 ratio definitely narrows. That's logical speculation.

3. I was wrong to compare the 1060 to the 480/580 only. The 1060 in the Steam HW survey includes both 3GB and 6GB models, and the 3GB model is essentially a different GPU due to the ~10% reduction in CUDA cores. And what's a ~10% CU cut down version of the 480/580? The 470/570, which should be included as well. That gives us:

* GTX 1060 = 11.88%
* RX 580 = 0.28%
* RX 570 = unlisted
* RX 480 = 0.62%
* RX 470 = 0.25%

This really brings home point 2. We know that the RX 580 has been out longer than the RX 480 was on the market. And we know that the RX 570 was a fairly popular GPU for miners. This tells me that it's possible that as much as half the sales are unreported due to mining.

This shows that it's 11.88% vs. AT LEAST 1.15%, and by doubling that (and rounding up) we get ~12% vs. ~2.5%, or closer to a a 5:1 ratio than the reported 12:1 ratio.

Again, these are all speculative, but I think my speculation is well grounded. You may very reasonably disagree.
> Steam users only reflect a certain part of the total GPU sales, and within those parameters, the amount of sales is 12:1. The Steam HW survey doesnt show anything more than that.

Correct. That's what I said.

> I never stated anything nor implied anything. The fact you have a dataset (that isn't reflective of the total market, that we do know) doesn't mean you are right.

Correct, but we can only analyze the data in front of us. And we're discussing that date in this thread about that data.

>  never even mentioned AMD!? I don't agree with people extrapolating Steam's HW survey to the total market

To be clear, I NEVER extrapolated to the total market. I was very clear about that in my first post. Your disagreement is noted, but in the end, you're arguing to agree. But I'll take my analysis a step further in a subsequent post (bear with me).
I'll be totally honest, I've had zero complaints going from W7 to W10. Hopefully if/when you make the transition, it's smooth for you as well.
There are problems with Vulkan too.

First, it's not as developer friendly as DX12. It lacks documentation when compared to DX, and MS will send teams of developers to a studio to help with a game if requested. Vulkan doesn't have a proactive team behind it to the same degree.

Also, Vulkan is subject to the same issues that plagued OpenGL, namely, falling behind DX in terms of hardware support. While DX12 supported mGPU on day one, Vulkan got it roughly a year later.

Make no mistake, I'd prefer to see Vulkan be the dominant API. Vulkan is supported or has support announced in some way on Windows, Mac OS, Linux, iOS, Android, Nintendo Switch, and PS4 (just to name a few). The one outlier is the Xbox One, which doesn't actually use the exact same DX API as the PC anyway despite some misconceptions.

id Software's Axel Gneiting said it best, IMO, when he said:

> Speaking on Twitter, Gneiting said that developers using DirectX 12 over Vulkan ‘literally makes no sense.’ Elaborating on his stance, and in response to some questions, Gneiting pointed out that with Windows 7 forming a major chunk of the PC gaming market, and with DirectX 12 being incompatible with Windows 7, using DirectX in an attempt to have ‘one codebase’ makes no sense, since developers would need to create two separate ones anyway. He pointed out that the argument that programming for Xbox One and Windows 10 becomes easier by using DirectX 12 is moot too, because DirectX 12 on Windows and on Xbox is very different, necessitating two separate code paths anyway.

[SOURCE] (https://gamingbolt.com/id-software-dev-puzzled-by-devs-choosing-dx12-over-vulkan-claims-xbox-one-dx12-is-different-than-pc)
Because the arteries relax and open up to let more blood to flow in; at the same time, the veins close up. Once blood is in the penis, pressure traps it within the corpora cavernosa. Your penis expands and holds the erection.

Or something along those lines.
> Your source doesn't prove anything though, even if he doesn't provide additional numbers it doesn't mean your conclusions are right.

The source, the Steam HW survey, is a credible ballpark of hardware sales. Is it exact? Of course not. But it's all we've got.

My conclusion is 100% correct, because my conclusion was - The Steam HW survey shows that the GTX 1060 outsold the RX 480/580 by a roughly 12:1 ratio, but we know that this isn't the full picture.

Your implied conclusion however, that the GTX 1060 did not outsell comparable Radeon hardware, is based on your hopes, not actual data. Until you can show credible sales data, you are wrong.

And I know AMD fans don't like data (because the data is rarely favorable to that group), but if you want to convince others outside of your echo chamber, you're going to have to provide actual data. Your downvotes and pleas don't work outside of the echo chamber.
> Eh, do you have any data outside of the Steam survey to prove this?

One source has been provided. You're welcome to show a source that disputes it, if you have one. Yes, I'm aware that mining played a role, hence why I ended with:

> (I know these numbers aren't definitive)
That's one factor, but we were seeing similar results before the mining craze picked up. AMD simply wasn't making nearly as many GPUs as Nvidia.
The GTX 1060 being in ~12% of all systems, holy shit! And it really shows the missed opportunity for AMD. The RX 480 was launched 2-3 weeks earlier at comparable performance, and was effectively sold out for months. Turns out it was sold out because AMD wasn't making many of them.

GTX 1060 = 11.88%, and RX 480 + 580 = 0.90%. Nvidia sold roughly 12x the number of 1060s than AMD their x80s (I know these numbers aren't definitive).
It's Windows 10 exclusive, and Win 7/8.1 still make up a sizable chunk of systems. Basically, 50.8% have a DX 12 GPU and Windows 10, while the remainder don't. A game developer would be locking out roughly half the market by using DX12 exclusively at this time.
You mean like the shoe company?
More than enough.
Tom's did a comparison of multiple models, for what it's worth.

https://www.tomshardware.com/reviews/nvidia-geforce-gtx-1060-graphics-card-roundup,4724.html

I find the comparison to be disappointing for the following reasons:

* EVGA - they only compared a single-fan model and not the more popular SSC/FTW dual-fan models
* MSI - Ran with the reviewer bios instead of the retail bios (higher out of box clocks speeds than what you or I would get without using their gaming app)
* Asus - Not tested

That said, my recommendions are going to be either the EVGA SSC/FTW (I have the 6GB SSC) or the MSI Gaming X.

Get the MSI if you want less noise and lower temps. The cooler is just so much better then EVGAs, which is no slouch either. Get the EVGA if the warranty and support are more important to you.
>  I have FPS in Overwatch 120+ (low settings)

Default refresh rate is 100hz. OC is 120hz max. If you go over this, tearing will occur.
To clarify:

> consequences for violating the Hatch Act range from a letter of reprimand

That would be a high level appointed position or cabinet level position.

> to a civil penalty of up to $1,000

That's for SES (think executive level)

> to suspension

GS 12-15 (white collar workers and senior management)

> termination

GS 5-12 (blue collar workers)

> or even debarment from federal employment for up to five years.

GS 1-4 (grocery store employees, low level clerks)
Hence why this would be Nvidia's nuclear option. Nvidia would switch to the open Displayport Adaptive Sync standard and/or HDMI VRR (the underpinnings of FreeSync) if it made no sense to stick with G-Sync. And it would only make no sense to stick with G-Sync if monitor manufacturers bowed out due to Intel/AMD and the open standards taking a massive sales lead.

Basically, if NV is forced out, there won't be many (if any) G-Sync monitors to buy. But this is all a BIG IF right now, and not relevant for at least the next 3 years. Nvidia has a 1-2 generation lead on AMD right now, Navi appears to be a mid-range part launching 6-9 months after Nvidia's mid-range part (IE, Vega all over again), and Intel should not be expected to knock it out of the park with their gen 1 product.
> Strange that you say that since he ps4 offers supersampling on the PS pro. It was added in the most recent update and can be found under the display settings if the systems menu. With that being said it doesn't make sense that it would down sample an image for a inferior picture when I can enable super sampling on a 1080p display and get picture quality thats superior to 1440p ans very close to 4k

It's not strange at all. You're confusing internal rendering vs. output. On the latest update, the PS4 Pro can only select 720p, 1080p, or 2160p for output. The image is then downsampled or upsampled to the output resolution.

> Im not sure where you're getting interpolated images from. Do you have any sources to that information?

HDTVs upscale (this adds input lag, so game/pc mode disables this). Monitors use interpolation (uglier, virtually no input lag). You can see an example of it [HERE] (http://www.tftcentral.co.uk/reviews/dell_u2717d.htm#office).

Basically, you're combining the limitations of several systems, so I'll try to simplify as best I can.

* The PS4 Pro can output to 720p, 1080p, or 2160p. On a 1440p monitor, your best option is usually going to be 1080p (some will take a 2160p input).
* When you select the 1080p output, the PS4 will output a 1080p image.
* If you select 1440p in the game menu, the game will render internally at 1440p, then downsample (this is how SSAA works) to the 1080p output.
* The 1080p output is then interpolated by the monitor for 1440p display (and the link that I provided shows exactly what this looks like)

>  it doesn't make sense that it would down sample an image for a inferior picture when I can enable super sampling on a 1080p display and get picture quality thats superior to 1440p ans very close to 4k

Downsampling is when you render internally at a high resolution, then output at a low resolution. This is also known as super-sampling (or SSAA). 4xSSAA at 1080p = 2160p (4k). 2xSSAA at 1080p puts you just over 1440p internally.

So what you basically just said was, " it doesn't make sense that it would down sample an image when you could just down sample an image."
> I’m at the rock in lake taps currently.

Ha! I live in walking distance. I'd pay ya a visit if I wasn't about to turn in. We hit them up virtually every weekend.
I've owned the CF791 and the Alienware AW3418DW. The CF791 is a bad example for a FreeSync monitor. The range is 80-100hz. It has a wider range mode you can enable (44-100hz IIRC, with LFC), but there's a reason it's not the default. It can flicker horribly. I've heard Vega users have less flickering than Polaris users (I was on an RX 480 at the time).

As for the X34A, that's a 60hz monitor with support for overclocking UP TO 100hz. I'd encourage you to save up for the Alienware AW3418DW, Acer X34P, or upcoming LG 34GK950G (the latter of which will also have a FreeSync variant). These monitors will offer youa  true 100hz panel (up to 120hz OC supported) for ~$1,000, and I expect the FreeSync model to be ~$800 at launch.

Whatever you decide, best of luck with your new display!
> You in the 253 area?

Yup. Work in Seattle, live in Lake Tapps.

> We can go have a beer

I tell people I'm allergic to alcohol. I'll just get a soda :p

> about how we have extra monitors no one wants to buy.

I used to swap monitors with locals for testing, but I finally found "the one" (in my flair). I'll be sticking with this until the 3+1 years warranty runs out. Unfortunately, it also means that I'm locked to Nvidia for GPUs, even if AMD were to pull ahead, thanks to G-Sync.
I was just confused by the weird way he drew his dollar sign :p

I'm also in the Seattle area. Listed one of my monitor's on CL. Not having any luck, but that's partly because of a tame market for non-gaming displays (my old U2717D), and party because it's a great backup display and I won't let it go for less than fair value.
It amazes me how Rugby is every bit as brutal as the NFL, if not more so, yet no helmets, no pads, and fewer major injuries per capita (except spinal injuries).

https://www.theatlantic.com/notes/2016/10/rugby/504143/
You can typically find the BenQ EW2750ZL/2775ZH for under £200. It's 27", 1080p, VA panel (great contrast), very light AG coating, built-in speakers (low quality, but they do work), and ideal for your stated needs.

The 2775 is the newer revision, but they are mostly identical. Get whichever is available or cheaper.
Somehow I doubt he lives near Seattle given the currency he's citing :p
1440p internal vs. 1440p output are two different things.

If he has a 1440p monitor, he's still limited to 1080p output with the PS4 Pro. He can then select 1440p in-game, and something hilarious will happen.

The game will render at 1440p, then downsample internally to 1080p for output to the 1440p monitor, then be interpolated (blurry mess) by the display.

So please note, 1440p as a selected resolution is not the same as a 1440p output resolution. Until Sony adds proper support, as MS is doing, I can't recommend a 1440p monitor for PS4 Pro.
Not sold in the USA, so the FTC would have no jurisdiction over them.
Good luck and please report back. I'm curious about what's going on.
This is going to sound like a dumb question, but you'd be surprised how often this happens.

Did you remove any film from the baseplate and make a good connection between the CPU and baseplate with the pre-applied (or, if applicable, manually applied) thermal paste?
Is the pump plugged into a 3-pin or 4-pin fan header?

If 3-pin, do you have the header set to 100% speed in the bios?

If 4-pin, do you have it set to DC or PWM in the bios? If DC, is it set to 100%?

A pump can function erratically or not at all if set to DC and anything less than 100%. Please look at those, and if everything checks out, then there's the possibility of a malfunctioning pump.
> TN panel will be closer to 1ms than either IPS or VA panel.

True, typically in the 2.5-3ms range, but still not 1ms.

> Still, the difference between IPS and TN is still noticeable in terms of motion blur.

Not at their optimal settings. While TN's blur will be better than that of IPS, the difference is minor and most won't notice it. I can see by your post history that this is a major crusade of yours (and why you get downvoted a lot). So I'm just going to post this comparison for the readers here to decide. Open these 3 links in tabs and click between them to compare. All photos taken by [TFTCentral] (http://www.tftcentral.co.uk/).

* [Dell S2716DG] (http://www.tftcentral.co.uk/images/pixperan/dell_s2716dg.jpg) - A 144hz gaming monitor, TN panel measured at 2.8ms GtG average
* [Asus PG279Q] (http://www.tftcentral.co.uk/images/pixperan/asus_rog_swift_pg279q.jpg) - A 144hz gaming monitor, AHVA (IPS) panel measured at 5.2ms GtG average
* [Dell U2717D] (http://www.tftcentral.co.uk/images/pixperan/dell_u2717d.jpg) - A 60hz non-gaming monitor, PLS (IPS) panel measured at 8.6ms GtG average
The whole point of NVENC and AMD's equivalent is to have encoding done independent of the GPU's gaming performance. You're confusing NVENC with CUDA. The latter does use the GPU's horsepower, but the former doesn't even sniff it.

OT is correct. A GTX 1050 and a GTX 1080ti will perform identically in this situation. In fact, the GTX 950/960 outperform the GTX 970/980 in NVENC performance (for H.265), as they use a newer generation encoder.

However, the CPU is taxed somewhat when this is used, and that's the primary cause of dropped gaming performance, though it's nowhere near as significant as a full software-based solution, such as MS's GameDVR, or KinoConsole.
> Va panel so It's not 1ms

To be fair, no TN monitor is truly 1ms either.
Only if the market forced them to do so. Right now there's no reason because:

1. AMD isn't competitive in the GPU space
2. Monitor manufacturers aren't saying "no"

If Intel's entrance combined with a resurgent AMD forces Nvidia market share to dive, and FreeSync monitors start to meaningfully outsell G-Sync monitors, then Nvidia's attempt at a lock-in becomes a lock-out. That is the only situation that would cause them to make the switch.
Anything under 90° is acceptable for GPUs, and under 100° for Intel's CPUs (not sure on the thermal limit on AMD's recent CPUs).

The benefit to lower temps these days is in the innate boosting of some hardware. Except Intel, which will run at its boost until you hit the thermal throttle limit. But AMD's outstanding new boosting tech on Ryzen 2000 functions a lot like Nvidia's Boost 3.0 in that lower temps = higher clocks.

The funny thing about a specced temperature limit is - going over it will cause meaningful hardware degradation, while staying below it won't materially affect hardware life, whether below it by 10° or 30°.
A good air cooler will be quieter than almost any AIO. Also, the larger the radiator size, the less the fans have to work. 280mm seems to be the sweet spot. Stepping down to 240mm (your limit) means two faster spinning 120mm fans.

I got rid of my Kraken X61 ($120, 280mm) and went with a Cryorig H7 ($30, lol). Much quieter. Oddly, lower sustained temps too (that really shouldn't have happened).
Vega 56 offers [performance slightly better than the GTX 1070 (and slightly worse than the 1070 ti)] (https://www.techpowerup.com/reviews/MSI/GTX_1070_Ti_Gaming/30.html), at a [noticeably higher power draw (229W vs. 145W in a reference card comparison)](https://www.techpowerup.com/reviews/AMD/Radeon_RX_Vega_56/29.html).

Basically, if you can find a Vega 56 for a price similar to the 1070, you're choosing between more performance (Vega 56) or more efficient (1070).
> The companies that were warned in the letters were Sony, Microsoft, Nintendo, Hyundai, HTC, and ASUS.

No surprises on most of them, but wow, Hyundai! That takes balls considering that the Magnuson-Moss Act came about due to fuckery from the auto industry.
They are all using the same panel, and none are calibrated, so their out of box picture quality will be subject to the same variance. Dell/Alienware generally has the best build quality and warranty (advanced exchange, free shipping both ways), so that would be at the top of my list. Otherwise, I'd go with the lowest costing model.
> Do you have a link to the LG PR statements?

https://forums.overclockers.co.uk/posts/31733348/

Scroll up to post # 385. This person derails the thread (thread is about a different monitor) asking about the release date of the 34GK950G. Another user tells him 'August' based on the emails that went out [(as described here)] (https://www.reddit.com/r/Monitors/comments/8ayqre/lg_34gk950g_release_date/). And then LG's rep steps in (post #394) and says:

> More Like July - but might be late July into LG warehouse so Early August into the channel (OCUK most likely!) - was June but slight delay

While he says that June was the original date, that was never put out publicly by LG and August was the first time someone from LG confirmed a target date.

CC for /u/LightBenjamin, sorry, no confirmed June date as you stated/implied.
And EVGA just can't engineer a competitive board that is feature competitive at a similar price. You're paying for the support, basically.

I say this as a person who has owned 2 EVGA motherboards. Rock solid, no stability issues at all. But my most recent one (Z270 Stinger) had some curious design decisions. Manually install the wifi antennas? USB header above the GPU? Blinking eye of Sauron that can't be disabled?
Then report it and move on :)
The BenQ is a 27" 1440p display (a great size/res combo). The LG is 32" and 1080p, a not so great combo for desktop usage.

If you plan to sit in front of it closely like a traditional desktop monitor, you may not like the way that the LG looks. It's going to be grainy, and likely too large.

> But I want to have an option to play HoMM 8 or etc in the future.

Heroes of Might and Magic 8? HoMM 7 is the last official release, and there's been no official word on a numbered sequel.
The problem with going with a balanced CPU/GPU combo up front is that to get a real benefit next cycle, you need to upgrade both.

As shown [HERE] (https://www.techpowerup.com/reviews/AMD/Ryzen_7_2700X/14.html) the 7600k is already starting to fall behind even the 8400 when paired with a 1080ti. Not a lot mind you, but the CPU is already a limiting factor. Now what happens when he upgrades his GPU? The split becomes even larger.

Most people try to get 2-3 GPUs out of one CPU. Due to this, I always err on the side of a beefier CPU when you upgrade that. The difference between an 8600k (mid-range) and 8700k (high-end) is $100. That's it. The difference between a 1080 and 1080ti is $200 retail.

I'd pay $100 for a longer-term solution than an extra $200 for something I'm going to replace sooner. That's my take.
> With the psu what do you mean by overkill.

The PSU can only supply what your system requests. Your system won't need 650 or 750W. Online PSU calculators tend to overestimate what you need (and they have incentive to do so, as they're often on websites of retailers or PSU makers).

My system, for example, shows up as ~400W on most online calculators, but draws less than 200W when gaming (measured at the wall with a Kill-A-Watt P3 4400 wattmeter). That's with a Core i7-7700k and a GTX 1060.

An 8700k + 1080 will add ~100-150W to that total before overclocking. So 400W or less under gaming loads. You'll see at [JG's review] (http://www.jonnyguru.com/modules.php?name=NDReviews&op=Story3&reid=529), my suggested PSU hits 90% efficiency at 307W, and 88.6% at 465W. Your maximum gaming loads will be in that range, so there's no need for a higher wattage PSU.

> I was planning on getting the Corsair rm750x psu since it’s white/black with white cables since I’m going for a white/black theme.

I've got a white/black theme myself. I assumed you were going with the EVGA since that's what you listed in your part list. That said, your PSU won't be visible because it's going to be in the case's PSU shroud. The cables that come with the RM750X are all black, same as the SeaSonic. The PSU itself is all black, with some silver on the branding.

EDIT: Just saw the white RM750x. Hadn't seen that before. That's an absurd price though. You could get a higher quality PSU with my suggested model, get the [Cable mod extensions ($20)] (https://cablemod.com/product/cablemod-basic-cable-extension-kit-86-pin-series-white/) and still save money.
It's a good build. Everyone and their mother will have their own input into which parts you should tweak, so here's mine.

* CPU - I made a separate post on this.
* SSD - For close to the same price of your separate 500GB and 1TB SSDs, you can get [this single 2TB SSD] (https://www.newegg.com/Product/Product.aspx?Item=N82E16820226859&ignorebbr=1). It's an MLC drive (excellent speed and endurance), and a known good one. Performance is on par with the 850 Pro, beating out the 850 Evo.
* PSU - The EVGA G2/G3 series are outstanding PSUs. I have a G2-650 for my primary, and a G2-550 for my secondary. They've since been surpassed and at a lower price. [HERE] (https://www.newegg.com/Product/Product.aspx?Item=N82E16817151189&ignorebbr=1) is the SeaSonic Focus Plus Gold with a 10-year warranty for $80 ($55 after MIR), and [HERE] (https://www.newegg.com/Product/Product.aspx?Item=N82E16817151186&ignorebbr=1) is the 650W version for $90 ($75 after MIR). A 550W PSU is overkill for your desired system, and a 650W is needlessly overkill. When you don't need that much wattage, quality is better than more wattage. These SeaSonic PSUs are the best value today, IMO. I'd personally go with the 550W.
FWIW, the Quad Lumi adds another heat pipe, making it marginally better than the standard H7.

Just note that you're going to need to use a USB 2.0 header and the NZXT Cam Software for the RGB lighting to work properly.
Unless you're on a specific budget, the best one is the 8700k.

At 1440p, you're more likely to be GPU limited on a GTX 1080, so there won't be a major difference with most CPUs. As you can see [HERE] (https://www.techpowerup.com/reviews/AMD/Ryzen_7_2700X/14.html) from TPU's 1440p game benchmarks from their most recent CPU review, it's a fairly tight grouping among the top tier CPUs.

At this point, how often do you upgrade? If you plan to keep the CPU through 2-3 GPUs, get the 8700k. It will last you longer through those upgrades that lift the GPU limit. If you plan to upgrade your CPU sooner, go with something cheaper since the differences with a GTX 1080 are going to be minimal.

And here's a video that shows some differences when the CPU becomes the limit (IE, what you'll see when you upgrade past that 1080 in a few years) - https://www.youtube.com/watch?v=uwUEVEbZxI4
> I really believe 1440p is coming

Microsoft added it. Sony might want to keep up.

> I don't understand why MLB The Show 18 has a 1440p setting.

Games are allowed to use just about any resolution that they want, and will let the console handle the scaling to the output resolution. The PS4 has supported 900p since day one, internally, but there's no 900p output option.

It makes sense for Microsoft to add features like 1440p and high-refresh support, because their FreeSync support means early adopters will gravitate towards PC monitors to use this feature. Microsoft is also pushing synergy between the Xbox and PC gaming. Sony's goals may not match this, and I haven't heard them comment on FreeSync support. Since no TVs that I'm aware of offer native 1440p support, it's possible that Sony isn't concerned about this.

So it's 50/50 from my point of view, but we'll see.
The PS4 Pro does not output to 1440p. And there is no Digital Foundry video saying otherwise (hence why the other user can't 'find' it).

There was a PS4 Pro update a few months ago that added support for SuperSampling when 2k resolutions are used in lieu of 4k. This is what confused a few people because 1440p is often incorrectly called 2k. The DCI 2K standard is 2048x1080, so 1920x1080 (1080p) is the consumer variation of 2K.

What it means is that the PS4 Pro when set to 1080p can use up to 4xSSAA (same performance hit as 4K). Anyway, here's the actual video :)

https://www.youtube.com/watch?v=aiTn2VqT0Pc

------------------

As a result, you can select 720p, 1080p, or 2160p as the output. The system will internally scale the game to the output resolution even if rendered internally lower. If you use a 1440p display, and select 1080p output, you're going to get the typical PC monitor interpolation.
> I've got the 27UD68 and when purchasing, it said it was FreeSync via HDMI. Now, checking AMDs website it appears it is only via Display Port.

I purchased the UD68 on release. I've checked the LG section periodically to keep up with new releases and it's never been listed as FS over HDMI. I've even warned users over the past year who though it might work with the eventual Xbox FreeSync update.

The UD69, its successor, does support FS over HDMI. It's possible that you confused the two.
It's very out of date. They struggle to keep up with new releases, and many older releases have incorrect FreeSync ranges.
> It doesn’t just uninstall apps and drivers on purpose.

It does.

It removes them, does a clean install, then re-installs them. You can even witness the re-installation process.

> I think you missed my point.

Your point was either incorrect, or poorly worded.
> but I don’t think it deletes app or drivers.

It does, accidentally and rarely. It's happened to me, and numerous others.

Also, you can witness the re-installation of video drivers during its numerous reboots. In my case, I have a 21:9 display, so the "Do not reboot, progress" screen was low res and stretched, before coming into focus and using the proper aspect.

So yes, the drivers are removed and re-installed during the process.
Since introducing support for higher bit-depths in the NVCP cards, the setting hasn't exactly been infallible. I've had it reporting incorrect bit-depth support for awhile. On one system I have it reporting 8- and 10-bit as options for a monitor with a known 8-bit panel (Apple Cinema Display). I've had others where it would only show 8-bit support for panels that otherwise offered 10-bit.

This is one area where AMD's drivers have an advantage over Nvidia's, unfortunately.
Just did the update on both of my systems. Graphics drivers are fine on both, and no missing programs that I can see on the primary system. I don't use the secondary system as much, so it's premature for me to say definitively that no programs are missing.

But even those who didn't lose their driver entirely, the reinstall could reset the settings. Here was the PSA that I posted awhile back (coming up on 2 years) when that happened:

https://www.reddit.com/r/nvidia/comments/4vzv5h/psa_windows_10_anniversary_update_messing_with/
It's a bit over my head, but the way that it was explained to me was that during these major updates, MS basically lifts all the stuff you installed, does a clean Windows install underneath, then reinstalls everything that you manually installed yourself. The downside is, this causes some programs/apps to disappear EVERY time that I go through one of these major updates. Because I use the start menu tiles for my most used programs/apps, it's very obvious what is missing after these updates.

Only once was it an Nvidia driver.
> What are you going to use to drive the 34GK950F though?

Nothing. I ended up with an Alienware AW3418DW. I Was just curious about the prior poster's source for the release date of the 34GK950G, something that he has yet been able to provide.

This line here:

> Some of the listed models will not be available until Q2 2018

Is not a confirmed release date, unfortunately. Would be cool if they came out that soon. One of LG's PR reps specifically stated that they were aiming for July, before walking that back to August-ish.

Then there's the issue of release quarters in general. We the general public consider them calendar quarters, IE, Q2 would be April - June. But PR (especially in a case like this were LG hasn't even announced some of those models) often mistakes a fiscal quarter for a calendar quarter, meaning that Q2 could be different depending on the way that company aligns their quarters (not every company is the same). The quarters for the company/agency that I work for are Oct - Dec (Q1), Jan - Mar (Q2), etc.

So I wouldn't take that as a confirmed release date until LG actually comes out and says, "We expect to release model number LG 34GK950G on such and such date."
Appreciate the model number. Seems to be a leak from a Far Cry 5 promo. No official announcements about it from LG.

Also, still waiting on confirmation of the June release for the 34GK950G from /u/LightBenjamin. He didn't respond to my prior request, but it seems he's an infrequent poster.
MSI and EVGA use the same third party processor for mail in rebates, and the process is identical. They're even mailed to the same address. I've done MSI and EVGA rebates often, and in one case simultaneously.
> I'd like to hear an immunologist weigh in here since our doctor gave us the option of spreading it out and we did because we felt it better.

Not an immunologist, but I'll at least tell you what I've heard from doctors who practice this.

It's an appeasement strategy for those who are reluctant to vaccinate or seek to delay or spread out vaccinations. You ever had that friend who said "I'm just going to take a year off of school before going to college."? Many people who say this end up not going to college.

Same with those who delay vaccinations. They delay, put off, procrastinate, then don't end up doing it. The alternative schedule allows doctors to appease these people while still having a firm schedule.
If your GPU is a blower style, disregard what I said. It will completely bypass the issue caused by an ATX PSU in this case.

The GPU that you linked to is not a blower style GPU.
> Yes, I mentioned BlurBusters in my post.

I was providing the full article for context. The chart alone can be perceived as misleading without the full context.

> This is incorrect. I've already explained what the 'brightness values' mean.

Your explanation was incorrect, which is why I corrected it. The article from BlurBusters was cross-referenced with [THIS] (http://www.tftcentral.co.uk/articles/motion_blur.htm) article from TFTCentral (they shared a lot of data between updates).

Or, as they put it, "LightBoost Setting (Higher is Brighter)."

They then measured the actual luminance value. I personally prefer 120 cd/m2, and that's what I calibrate to. I've been told by most gamers that this is too low and that they prefer higher.

On this particular monitor, 100% Lightboost/brightness was 87.14 cd/m2. Or as TFTCentral stated:

> You can see the the luminance range of the screen is now much lower with LB enabled, ranging up to a maximum of only 87.14 cd/m2 and with an adjustment range of only 38.93 cd/m2. The average contrast ratio was 634:1. The limited brightness range of the screen was an obvious limitation of enabling LB on this screen and is a common pattern on models using the LightBoost hack method.

Basically, very few people who want to use lower than 100% on that monitor. But better monitors have come out, and the PG258Q, the most recent ULMB-enabled model they've tested, shows up to ~275 cd/ms @ 100%. I(t's ~140 cd/m2 @ 50%. I'd probably run it just below that, but most gamers would probably target the 75% setting.

> Just because people choose not to use lower values doesn't mean the MPRT value is incorrect.

First, you didn't show a single 1.0ms measurement, so I'm still correct. Second, it absolutely does matter what the user is going to do. My monitor claims 5ms GtG response times, but actual measurements show it in the 8-9ms range for the way that I (and most users) use it. That is absolutely relevant.

>  And for the record, I'm writing this message on a BenQ XL2411Z running at 1.5 ms persistence. :)

I doubt this, but you could prove me wrong with a link to a respected review showing proper measurements that reflect this.

> newer monitors are perfectly usable at 1 ms persistence.

They don't, as you and I have both already proven with actual math, charts, and data. Because the data shows that you were wrong, you've resorted to making claims that even you know cannot be backed up.
Your photo doesn't disprove me. I'll try to explain this better than last time. First, your photo is taken from [HERE] (https://www.blurbusters.com/zero-motion-blur/lightboost/).

The chart was provided for illustrative purposes, and is not an actual measurement of any monitor. Context is key :) They did, however, use a lot of VERY GOOD data to back it up.

The @ x% is the brightness setting. Most gamers won't run their monitors at 10% brightness even with lightboost/ULMB/strobing off. With even lower brightness caused by strobing, they're not going below 50%. Most run at or near max brightness when strobing is turned on, and even in this best case sample, that's 2.4-2.8ms.

And what was my original point? That 1ms is misleadingly advertised and you still won't get an actual 1ms even with strobing. Is it closer than without? Absolutely! But I never argued against that. I simply stated that, "it's not quite 1ms."
> Not sure what you mean here. The whole point of backlight strobing is to reduce the perceived motion blur of sample-and-hold panels. So while a 120 Hz monitor running with an always on backlight has a sample hold time of 8 ms when you introduce backlight strobing you can reduce that value significantly, taking it down to 1 ms or lower (iirc some panels allow 0.5 ms persistence now).

The only way that backlight strobing can truly reduce perceived motion blur to an effective 1ms is to strobe at a rate of 1,000 times per second (1ms per strobe). By strobing at 120 times per second, there's an effective interval of 8.33ms. Even when properly timing these strobes with the monitor's refresh rate, you still can't get an effective 1ms.

Yes, it does reduce perceived motion blur. But it doesn't reach 1ms.
As an H200i owner, I have two recommendations for you based on GPU temps.

1.  Ensure that the GPU's heatsink has a horizontal fin arrangement (yours appears to). If it's vertical (like my EVGA), it causes the front fan to circulate air back to itself. In my H200i, my 1060 SSC runs at 80-81° under a heavy load, whereas it was 72° in my older H440.
2. SFX power supply. An ATX power supply will completely fill the basement beyond the front fan of the GPU. This starves the front fan of air. An SFX PSU allows air to pass through to the front GPU fan, helping with overall GPU temps.

The worst combo is an ATX PSU + Vert fin array on the GPU (as I have). You can get away with the ATX PSU, but you don't need to. A Corsair SF450 ($90 @ Amazon) or SF600 ($115 @ Amazon) will be better for you than what you picked out.
Neither.

1ms GtG is them claiming a best case (unrealistic) measurement.

1ms MPRT is typically used with a non-TN panel and some form of backlight strobing to simulate 1ms. And because the best backlight strobing I've seen is 240hz (4.17ms), and 120hz (8.33ms) being typical, 1ms isn't possible via this method.

If you want accurate numbers, find a review of your desired monitor (or a monitor using the same exact panel) at TFTCentral. They get the actual response time measurement over numerous transitions, they report an average, and they highlight which transitions go over the frame interval and lead to ghosting (or inverse ghosting, where applicable).
And this is why liberal strategies fail. Claims against the right for hypocrisy roll off them like water off a duck's back.

They should be using a different tactic. They should instead be CHEERING the NRA for their admission that a gun-free zone increases safety.

Those on the far right are reacting to the current stance in a defensive manner. But if liberals used the strategy that I outlined, it would cause those on the right to react in a confused manner. They wouldn't know who to support.
> Thanks for the advice. The cable is what shipped with the monitor. I could make a stab of swapping it with the cable matching my (defunct) X360 to the TV, which would allow me to test the monitor and cable at the same time.

Start with the cable first. If you swap out 2 of the 3 (input device, cable), and it gives you a different result, you now have to further troubleshoot to figure out which was giving you the issue.

1. Swap out cable between PC and display, if issue persists
2. Use different input (Xbox) on same display, if issue persis
3. Use different output (TV) for same PC

> The GPU is a very recent model, a Sapphire Radeon RX560 Pulse, so I would be surprised if it somehow did not support HDMI, or that a failure in it transmitting HDMI would not appear via other modes.

It's not an issue of support, but rather, the possibility of a defective unit.
> Neither has as much spectrum as Verizon and AT&T.

Sprint has the most spectrum held of any US carrier. [SOURCE] (https://www.fiercewireless.com/wireless/2017-how-much-low-mid-and-high-band-spectrum-do-verizon-at-t-t-mobile-sprint-and-dish-own)

Their weakness is in low-band holdings. In that area, they're dead last, while T-Mobile is competitive with (and slightly behind) AT&T. You could argue that the combined low- and mid-band holdings of TMO and Sprint would put them on par with AT&T and Verizon, and such a claim would be technically correct, so long as we ignore the high-band holdings.

> 5g requires far more towers and density of them in that area.

It does require more density for what they're trying to do. If they want to maintain the status quo, a 5G-enabled tower using the same spectrum currently available could handle more concurrent connections, faster speeds to the same number of concurrent connections, or they could find a compromise between the two, just as they did with the leaps from 2G to 3G to 4G.

But it looks like with 5G the carriers want to go with much higher speeds, and towers won't get the job done. They're looking to go with small cells. These have the benefits of being cheaper, more portable (they get moved around for sporting events a lot), and don't trigger the "NIMBY" movements that towers do.

That said, if they were to go the route of towers, they would be leasing tower usage as they currently do. Carriers have been selling off their towers for years, reducing their holdings, and then paying a lease to use the same towers.

http://wirelessestimator.com/top-100-us-tower-companies-list/

* AT&T - Not listed in the top 100 tower holdings
* Sprint - 236 towers, ranked 31st
* T-Mobile - 403 towers, ranked 20th
* Verizon - Not listed in the top 100 tower holdings

So if density is a concern, Sprint and T-Mobile both tower over Verizon and AT&T on an individual basis. And here's one example of Verizon selling off their towers - https://www.fiercewireless.com/wireless/verizon-offloads-towers-to-american-tower-for-5b

So let's recap. Your claim was that Sprint and T-Mobile need each other for spectrum because they fall way behind Verizon and AT&T. That was false. Sprint holds more spectrum than any other US carrier.

Your second claim was that they need each other for tower density. That was false, as neither AT&T nor Verizon approach either of those carriers in number of owned towers.
You don't. DCI 2K is a standard that doesn't apply to consumer monitors.
They already have both the spectrum and the towers.
> Would checkerboard rendering on the PS4 Pro increase input lag?

No. The processing is done on the PS4. Now, if you used the TV's scaling (say, 1080p output on a 4k display), this can and often does introduce input lag. TVs use upscaling, which adds lag, while monitors use interpolation, which degrades image quality relative to native resolution but introduces no noticeable amount of lag. Many TVs have a "Game Mode" or "PC Mode" that disable this.

When the upscaling is done via the console (IE, Xbox running 900p game on 1080p display), the input lag is avoided.

> Does 4K and or HDR introduce input lag?

4K alone doesn't. See the above on resolution scaling. As for HDR, yes, it does add some additional processing delay. This is why Nvidia and AMD have both made a fuss about their adaptive sync standards and HDR integration, as they both largely negate the additional input lag associated with HDR.
1280x720 = HD (High-Definition)  
1920x1080 = FHD (Full HD)  
2560x1080 = WFHD (Wide FHD)  
2560x1440 = QHD (Quad HD)  
3440x1440 = WQHD (Wide QHD)  
3840x2160 = UHD (Ultra HD)

DCI 2K is 2048x1080, which is still 1080p, hence why 1920x1080 is often referred to as 2K. DCI 4K is 4096x2160, which is why 3840x2160 is often referred to as 4K. QHD is neither 2K nor 4K.
This monitor only takes FreeSync over HDMI. Can't use DVI, with or without adapters.

Since you said that HDMI wasn't working for you, try swapping out HDMI cables first. After that, try different devices, IE, Xbox/Playstation to monitor over HDMI (if this fails, you know the monitor is at fault), or, plugging your PC into your TV via HDMI (if this fails, you know it's the GPU).

Then, RMA/return/exchange the faulty component.
1. Every monitor, even within the same product line, has variance. You can't use the same settings on two monitors and get the same results. They need to be individually calibrated.
2. The Dell U2718Q is factory calibrated and is fairly accurate out of the box. Any tweaking that you do without a colorimeter is pretty much guaranteed to make things worse. Leave the settings alone unless you have proper calibration tools. The only thing that you should change is the brightness setting to get it where you prefer.
Fire away, but I'm about to call it a night, so I may be slow to respond.

Also, these threads may be useful:

* [Initial/Subjective Impressions] (https://www.reddit.com/r/Monitors/comments/812iuq/my_first_day_with_my_new_alienware_aw3418dw/)
* [Calibration Measurements/Results/Analysis] (https://www.reddit.com/r/Monitors/comments/82dcey/aw3418dw_calibration_measurements_results_and/)
Wouldn't be surprised to see it use CUE to match RGB accents with their peripherals.
From my limited reading, it seems to impact mainly people who have GFE installed and are applying the update through that. At least that's all I've seen (again, in limited reading).

On both of my systems I avoid GFE. I had no issues on the 1060 system (the other system is 950-based, also no issues).
The Dell/Alienware for the warranty, everyone else based on the lowest price you can find.
No, sorry. I don't game on consoles.

While the Xbox is getting support for 1440p and high refresh, Sony hasn't (AFAIK) commented on the same. So you can only use the PS4 for 1080p or 4k. If the PS4 is a large part of the equation, then the 4k is your best bet between the two.

You can use it with the Ultrawide in 1080p, but you'll have black bars on the sides.
You're comparing 4k 16:9 to 1080p 21:9. They're completely different displays.

Ultimately, it comes down to personal preference. The 4k 27" will give you a larger virtual workspace due to its higher resolution, but you will likely want to scale it down so that UI elements and text aren't too tiny. This will lead to scaling issues in some programs.

The 34" Ultrawide will be physically larger, but a smaller workspace due to the lower resolution. It's like a 27" 1080p, just expanded horizontally.

Neither is ideal. If possible I'd go with a 27" 2560x1440 or 34" 3440x1440. These get you the ideal PPI without scaling or pixilation issues. The compromise here is cost.

Dell Ultrasharp models with these specs will come factory calibrated, making them ideal for your primary usage. They are also adequate for non-competitive gaming. I gamed on my U2717D for about a year.

The issue here will be cost. Hopefully they're not too far out of your budget.
It's among the cheapest high refresh ultrawides. It's decent, but as stated the pixel density is disappointing for some. It's equivalent to a 27" 1080p monitor. Up to you if that's a deal breaker.

It's also slightly over your stated budget, which is why no one initially recommended it.
They are likely identical in every measurable way, but there is no way to confirm without proper objective measurements.
Civil suit, not criminal.
> Just always heard that they always had lower response times than other majority monitors

You probably heard this from marketing. Response times are limited by the panel, and they get the same panels as everyone else. It's become real consolidated.
No lower than any other monitor using the same panel. In fact, BenQ's monitors have previously done so poorly in objective measurements that BenQ has stopped sending them to TFTCentral for testing.

They rely entirely on marketing and e-sports sponsorship now. It works. If you can fool consumers into thinking it's the best, why risk allowing objective measurements to ruin that?
Because BenQ sponsors these events and the monitors are given out. It's similar to why you see NFL sidelines loaded with Surface tablets and Bose headsets.

Don't let this mislead you into thinking that BenQ monitors are any better than the competition. Its mostly due to sponsorship.
> Very low gtg input lag as well.

No such thing as GtG input lag. You're confusing input lag and response time.
Back in my Dreamcast days, I had the official mouse and keyboard. When playing Q3A or UT, I'd use left side of the controller in my left hand (analog stick for movement, d-pad for certain bindings), and the mouse in my right hand.

So basically, same thing as /u/TopMacaroon
They announced their price. $159.99.

While cheaper than prior generation Stinger boards (being H-series instead of Z-series probably plays a role), it's way too expensive given the cost of Z370-based Mini-ITX boards with more features.

Worth it at the eventual price drop. At $119 or less, I'd consider it with an i5-8400.
> The whole longevity bs talk can only fool the casuals,

And yet, the stuff isn't breaking down from even the 1st gen parts that used it.

Pathetic cooling? Yes, that's a legit problem with Intel's ~~toothpaste~~ TIM. But longevity isn't an issue.
In Mini-ITX, dammit.

Would go perfectly with the NZXT H200i.
I didn't read this comment. Can someone tell me what it said?

EDIT: For those late to the party, previous deleted comment was saying he couldn't be bothered to click the link, and asked someone to tell him what the article said.
I have mixed feelings. I hate that MS is doing this, but I understand that they basically have to. And I don't think that they could have worded their statement any better.

That said, talk is cheap. If they aren't going to SELL us MCC, then I'll feel no sympathy when the community finds a way to keep this running. They get the benefit of the doubt for now, but my patience will wear thin if we don't get an announcement soon.

At the very least, take the already-released Halo 1 + 2 for PC, and put them on the Windows Store or something. Throw us a bone.
Someone could, but it's a bad idea. As I stated in my calibration report for the AW3418DW:

https://www.reddit.com/r/Monitors/comments/82dcey/aw3418dw_calibration_measurements_results_and/

> As I've said before, uncalibrated displays vary from unit to unit. An ICC profile is a means to correct for the hardware deficiency via software means. If I were to apply the ICC profile supplied from TFTCentral, I would get a very different outcome than they had given the differences in our out of box results. This is why you don't use ICC profiles supplied from someone else.

Additionally, the CCMX files that they are requesting are meant to help with wide gamut displays (their inclusion of standard gamut displays seems to contradict this). The XB271HU is not a wide-gamut display and wouldn't need this correction, based on their stated use.
It should be, and those cables are typically < $10.
I've checked the manual for your monitor, and it states that 144hz is supported over DP or DVI-D.

However, the HD7950 is a different story. I've been looking it up. Some models come with a DVI-D port AND a DVI-I port. Others come with a single DVI-I. If yours is in the latter camp, DisplayPort is your only option.
I'd be partial to the NZXT H400i (I own the 200i). It's a Micro ATX tower.

You could step down to the H200i, but you'd need to buy a Mini-ITX motherboard for your setup.
MRSP per item:

* CPU - $189
* GPU - ~$249
* Mobo - $99
* Total - ~~$537~~ $599!

The Armor OC was MSI's mid-level GPU in the stack. They charged the most for the Gaming X version, with the blower and OCV models closer to MSRP. $229 was the reference/blower/OCV price, and $279 was the Gaming X price, so I put the Armor OC close to the middle based off memory. The MSRP above for the mobo and CPU are the current list MSRPs for both products.
That's what I love about my NZXT H200i. The front panel connector is one block, like the USB connector. Can't screw it up.

Except that I initially plugged it into my USB 2.0 header...
It was a test designed to pushed the CPU by removing the GPU bottleneck. The 6-core will still hit at or near 100%, but will reach a higher frame rate.

You won't go below 100% in a test like this until you get to a CPU with more cores than the engine can make effective use of. The 8400 is seen by the OS as a 6-core CPU, while the 2600 is seen as a 12-core CPU. Because consoles this generation started off allowing access to 6-cores and were eventually bumped to 7-cores, most PC games have only recently started making effective use of more than 4 cores. This is why is took so long for 4c/8t Core i7s to pull meaningfully ahead of 4c/4t Core i5s.
The OS sees all cores, logical or virtual, as being a physical core. Due to this, it see the 8400 as a 6-core CPU, and the 2600 as a 12-core CPU.

The CPU being utilized at or near 100% doesn't matter much. All that matters is price, performance, and power draw for your stated usage. As of today, the 8400 wins in all categories for pure gamers.

That does change for specific use-case scenarios. Gaming while broadcasting? The 2600 takes the performance crown. Doing a lot of high-end productivity work? Again, the 2600 wins in those areas. Watching videos on one display while gaming on the other? That takes almost no power on modern systems. Web browsing? You can do that on a Pentium G or Athlon.

What I find most appealing about Ryzen is something that goes unsaid by many - the consistency. When watching Digital Foundry's reviews of last gen's CPUs, I saw that in many games Ryzen didn't dip as low in the framerrate department. But in easier scenes Intel pulled way ahead, which inflated the average FPS numbers. Ryzen gave a more consistent experience in most of the games tested.

As I've said in other posts, I'd have a hard time picking between the two for my needs, and all I do is game and browse the web. But let's not act as if CPU load is the be-all-end all as some are doing. It's not a good indicator for gaming performance. My CPU load in World of Warcraft hits 25% and I become CPU bottlenecked. That's just game development.
> But you don't understand.

I do understand. Was just pointing out a common misconception and clarifying it.

>  Since when does Intel have a 120mm Asetek AIO?

[Since at least 2011.] (https://www.extremetech.com/computing/107147-intels-liquid-cpu-cooler-is-water-worth-the-cost)
Most people don't realize the the box coolers are different for i3/i5/i7. They just look similar.
Perfect :)

I'm seeing now that you're running at DDR4-3000 (Same as me).
I did some quick Googling.

DOCP is Asus' implementation of XMP on AMD motherboards. So yes, use DOCP. That will (should) pull the RAM's XMP profile, thus using the advertised speed that you paid for, provided that the motherboard has no issues with the memory that you bought.
Per Userbenchmark, you use the Asus Prime X470 Pro. If you use a different motherboard, please let me know. Otherwise, these instructions are pulled from their manual for the above board.

* Enter BIOS by spamming the DELETE or F2 key on your keyboard during post
* Look for the Ai Tweaker menu
* Under that you should find the Memory Frequency setting

The manual doesn't go very deep into this. On Intel motherboard you're looking for XMP. But this manual refers to it as SPD. So look for either of those.
By default, memory will run at the JEDEC standard (for DDR4 this is 2133, though some newer modules will run at 2400 or 2666). Memory that is sold at a higher advertised speed will have what is known as XMP (Xtreme Memory Profile) support. Go into your motherboard bios settings under memory and turn on XMP (some memory has multiple profiles, if so, use XMP Profile 1). Upon save and reboot, your memory will run at the rated speed (so long as your motherboard has no issues with that memory, something that was more common than I'd like with 300-series mobos).
There's always variance within any product. Additionally, background tasks can meaningfully impact your results. Just look at your prior results under run history. Your last two runs show noticeable swings (though still in the same ballpark).

I wouldn't worry about it. But here's some over analysis just in case:

* CPU (26th percentile) - You're lumped in with only 41 other benches (at this time, more to come). Enthusiasts are usually the first to upgrade, and that's likely a higher rate of overclocking. That's what you're up against right now, pushing your score down. **Also likely impacted by your memory (see below)**
* GPU (90th percentile) - This is ideal, nothing to worry about here.
* SSD (22nd percentile) - While AMD wasn't hit as hard as Intel, SSD speeds have suffered with recent security patches. You're up against a lot of results taken BEFORE those patches, so you're going to score worse. Still, I had similar results with my 960 Evo awhile back. Installed the Samsung software, updated the firmware, and ran drive optimization. Made my score jump.
* HDD (95th percentile) - Another ideal/envious result.
* **RAM (50th percentile) - Spot on average. Of note is that you're at 2133, so either you got uber cheap RAM (relatively speaking), or you forgot to enable its profile to run at advertised speeds. That could help you a little.**

Look into the RAM, install Samsung's SSD software, update SSD firmware, run drive optimization (from Samsung's UI, or the file explorer where defrag is/was), and then your results should improve a little.
> I limited the fps to both games to 119, because (please don't hate me) my gtx 760 does not seem to support 144Hz refresh rate and maxes out at 120Hz.

That's why.

G-Sync/FreeSync ranges are presented to us in refresh rate, because it's easier for us to understand. But it actually works based on frame interval. At 120hz, that's an interval of 8.33ms per frame. If it's greater than that, then you're in the G-Sync range (under 120fps). If you're at or below 8.33ms, you leave G-Sync and tearing will occur.

Your frame limiter isn't an interval limiter. That means your interval can cross that 8.33ms threshold even if your average fps stays at or below 119.

If you want to completely eliminate tearing, get rid of the limiter and use V-Sync. V-Sync will engage at 120fps/hz, and anything below that will be handled by G-Sync, preventing V-Sync induced stuttering.

If that's an issue due to input lag (real or perceived), use a lower limit. Most use 2-3 fps below max, not 1. The lower you go, the less tearing you'll see. Try 116 or 117.

EDIT: You can also try a compromise of leaving V-Sync on but still using the limiter. This means you'll have V-Sync trigger instead of tearing in those rare cases. You'll have to choose between intermittent tearing, or input lag spikes. Again, the lower your limit (116 or 117), the less this will occur.
It's basically the template for nearly every modern WearOS device, with a few outliers that went their own way. Take the LG Style's guts, add a larger battery, change the outer styling, and you have every watch from Fossil Group from the past year.
> Not many people are willing to spend several thousand to litigate for a few hundred dollars.

It's misinformation (usually unintentional) like this that scares people out of seeking help to get a company to honor the warranty.

MOST products where you would encounter this issue fall under small claims court. Small claims court is much easier for the common person to navigate. It's affordable, and most jurisdictions do not allow an attorney to be present. For my county it's $29 to file, and, per their website:

> attorneys are not allowed to represent either side in small claims.

I'll use myself as an example. Several years ago I filed my first (and to date, only) small claims case against a large, well known company for failure to honor their warranty. The case was settled and there is an enforceable NDA, so I cannot disclose the company name, branding, product, etc. I can, however, give you a rough idea of how I pursued it.

For starters, here were the monetary terms I claimed:

* Cost of product
* Filing fee
* Loss of pay from work (8 hours * number of days expected to be in court), this is typically 1-2 days - because my jurisdiction schedules a mediation attempt and a trial date, I put in 2 days worth of pay with an addendum of 1-day's pay for each additional day beyond the first at trial
* Treble damages (all of the above tripled) due to the action on their part being willful

Basically, when you factored in what I was claiming, it was basically ~4x what I paid for the product. Also, they'd have to fly someone up here and lodge them, and they couldn't have an attorney represent them (although you know they consulted one).

For me, I had my invoice, copy of warranty terms that came with the product, copy of email correspondence, and date/times of phone calls (I did not record them). And of course a pay stub to validate my claim for loss of pay. It was my word against theirs unless they could produce the recordings. Oh, and burden of proof was on them to disprove my side of the phone calls (though it helped that the emails matched the content of the calls anyway).

My point is, I was (legitimately) making it more costly for them to show up to and lose in court than it would have been to simply offer the legal minimum for customer support.

They offered me a settlement. It was basically a full refund. I declined, stating that I was now out a filing fee, and my time. I told them I was guaranteed 4x what they were offering me when I won in court, and that the finding of a Magnuson-Moss Act violation in court would look REALLY good in my follow-up complaint to the FTC just in case they're doing this on a larger scale.

Needless to say, their settlement included the typical NDA, no admission of wrongdoing, and enough money to shut me up on specifics :)
> Ultimately, which you buy is up to you and both are outstanding options. As for which one I'd pick? I don't stream, and despite that, I've backspaced over my answer and gone back and forth numerous times. I can't decide.

I'm going to expand on this here, because I REALLY wanted to keep my personal opinion out of the (already long) post that was meant to be impartial.

For me, a choice between these two CPUs would absolutely come down to the motherboard. I use an NZXT H200i, so mini-ITX it is.

Also, I'd want a 400-series board if going AMD due to the memory issues on the 300-series boards. And right now the only X470 mini-ITX boards I can find are ~$200. Ouch! So much for value. That said, if I went that route I'd probably spend the extra $30 for the 2600X (essentially pre-overclocked, saving me the trouble), and at least DDR4-3200.

On the 8400 side, the best Mini-ITX spec/price wise is the Gigabyte Z370N Wifi, which is a beautiful board for ~$150. It's also relatively unique with dual M.2, one of which is on the front with a thermal guard (again, this is mini-ITX).

So if I had to pick TODAY, it would be the 8400, that board, and something in the neighborhood of DDR4-2666 to 3200, depending on value since Intel isn't AS dependent on memory speeds as AMD here.

I'm just glad I'm not in the market today, because this is a nail-biter. Did I mention that competition is a beautiful thing? :)
For shits and grins, I decided to make a table showing the framerates for each game. However, since there's no average a user could easily cherry pick any point where their desired CPU looks best. Due to this, I'm choosing 3 points for each game: the first visible frame where data is clear, the last visible frame, and as close to possible as I can get to the midpoint between the two. I am human, so some mistakes will be made in this regard, but I am trying to be as impartial as possible.

The last column of the table is meant to show the percentage difference in framerate. Because the 8400 came out first, I'm going to use that as the base. So, +10% means that the 2600 is 10% faster. -10% would mean that the 2600 is 10% slower. I hope that's clear. I **bolded** results where Ryzen ties or came out ahead.

Game|Intel Core i5 8400|AMD Ryzen 5 2600|% Diff
:--|:-:|:-:|:-:
Project Cars 2 (Measurement 1)|225|197|-12.4%
Project Cars 2 (Measurement 2)|233|205|-12.0%
Project Cars 2 (Measurement 3)|261|214|-18.0%
Kingdom Come Deliverance (Measurement 1)|90|88|-2.2%
Kingdom Come Deliverance (Measurement 2)|88|88|**0.0%**
Kingdom Come Deliverance (Measurement 3)|109|105|-3.7%
The Witcher 3 (Measurement 1)|136|133|-2.2%
The Witcher 3 (Measurement 2)|133|132|-0.8%
The Witcher 3 (Measurement 3)|121|119|-1.7%
Grand Theft Auto V (Measurement 1)|159|144|-9.4%
Grand Theft Auto V (Measurement 2)|110|103|-6.4%
Grand Theft Auto V (Measurement 3)|91|88|-3.3%
Assassin's Creed Origins (Measurement 1)|110|106|-3.6%
Assassin's Creed Origins (Measurement 2)|108|109|**+0.9%**
Assassin's Creed Origins (Measurement 3)|100|97|-3.0%
Far Cry 5 (Measurement 1)|119|110|-7.6%
Far Cry 5 (Measurement 2)|129|114|-11.6%
Far Cry 5 (Measurement 3)|176|157|-10.8%
Arma 3 Apex (Measurement 1)|83|69|-16.9%
Arma 3 Apex (Measurement 2)|81|81|**0.0%**
Arma 3 Apex (Measurement 3)|76|63|-17.1%
Battlefield 1 (Measurement 1)|200|172|-14.0%
Battlefield 1 (Measurement 2)|199|182|-8.5%
Battlefield 1 (Measurement 3)|198|170|-14.1%

With 8 games measured, that's 24 total measurements. The average difference of those measurements was: **8400 > 2600 by 7.43%.**

**Notes/Analysis:**

* The 8400 bencharks were done with overclocked memory on a Z-series motherboard, which destroys the value proposition of choosing an 8400. Keep that in mind.
* These benchmarks were done at 720p, which represents a worst-case scenario. In real world comparisons in typical gameplay scenarios with today's games, the gap narrows.
* As for projecting future viablity based on 720p benchmarks, there are two points to consider. If games remain the same as today in terms of multi-threaded workloads, the gap should stay about the same. But if games become more heavily multi-threaded, the gap will narrow further (with the possibility that the 2600 would even overtake the 8400). Each individual should make their own guess as to which direction game development will go. It's your money, after all.
* The i5-8400 can currently be had for $179 on Newegg, while the R5 2600 is $198.

And here's a power draw chart (total system draw, measured at the wall) courtesy of [TechPowerUp] (https://www.techpowerup.com/reviews/AMD/Ryzen_7_2700X/18.html) (NOTE: 2600 not tested, so 1600, 1600X and 2600X numbers used for comparison, will expand below):

Scenario|i5-8400|R5 1600|R5 1600X|R5 2600X
:--|:-: |:-:|:-:|:-:
Idle|49W|47W|48W|49W
Single-Threaded|60W|78W|84W|88W
Multi-Threaded|108W|137W|160W|186W
Stress Test|126W|139W|166W|192W
Gaming|335W|353W|364W|376W

First, let's acknowledge that the 2600 wasn't tested here. We can see that power increased from the 1600X to the 2600X. We should expect a similar result going from the 1600 to the 2600. As such, we should expect the 2600's numbers to be at or marginally higher than the 1600. Because the 2600 isn't tested, we'll give the benefit of the doubt and use the 1600's numbers compared to the 8400 (for now, until TPU gets a 2600 to test).

From here we can see that the 8400 holds a consistent advantage compared to the AMD options. In fact, even the 8700k beat out the 2600X (both are 6c/12t) in most power comparisons. So while AMD has made incredible strides with Ryzen (both generations so far), Intel is still overall more efficient.

That said, power draw doesn't matter as much for desktop usage as it does in laptops. And while it matters to me for GPUs (due to the small case I have and my hatred of noise, I'll take a 1060 over an RX 480/580), it doesn't work the same with CPUs. AMD's use of solder instead of ~~toothpaste~~ TIM means better heat transfer. You'll likely see better than expected results in terms of temps, fan RPM, and fan noise due to this. Basically what I'm saying is, power draw shouldn't be a significant factor in this comparison.

**Conclusion:** The 8400 comes out slightly ahead in terms of performance, is slightly cheaper, and draws slightly less power. On paper it's the clear winner at this time. Also, choosing memory is easier for the 8400, as XMP on Intel boards is more of a sure thing than it is on AMD's boards (though this issue appears to be almost completely gone this generation, it still hobbles 300-series AMD mobos). But there are solid reasons for going AMD here. First, the higher thread count is likely to mean more for future performance than you would get from a 720p test. Second, while the CPU is +$20 over Intel, B-series motherboards are more readily available and much cheaper (current B/H-300-series Intel boards are scarce, and $100+). Ryzen also offers a noticeable improvement for those of you who tend to broadcast while gaming.

Ultimately, which you buy is up to you and both are outstanding options. As for which one I'd pick? I don't stream, and despite that, I've backspaced over my answer and gone back and forth numerous times. I can't decide.

Competition is a B-E-A-U-T-I-F-U-L thing!
I had one guy doing that to me on a message board. And then he pm'd me my address!

So I took it a step further, looked him up. Responded with his address and told him my next call was to the cops.

I got banned from that forum (justifiably), he didn't (friends with a moderator), but he backed off so I was happy.

And that's why I post on Reddit and not HardOCP :)
~~The LG Watch Style is my preferred watch for small wrists. I have one and it's only marginally larger than my 39mm Seiko 5.~~

I'm a moron and discounted your need for an HR sensor. I apologize, and thanks prisoner number /u/12401 for the correction :)
I love when an ad forces me to comment on it, thus boosting the algorithm that improves its relevancy!
> Oh well you should have specified that!

He sort of did. He provided the GPU model in his title but neither of us searched it. It's a blower-style cooler that lacks DVI output :(

Also, the product that you linked won't work. It has one review that says it's limited to 60hz.
An HDMI 2.0 to Displayport adapter will limit you to 60hz, because due to the pins connected, it will terminate in single-link, even if you have a dual-link adapter/cable.

Some have reported using DisplayPort to DVI adapters and getting up to 120hz on the GN246HL. YMMV. So you could try that.

But if you MUST have 144hz, you have a few options and both will cost you. The most obvious is to upgrade your monitor. The next best option is to swap your 1080ti with someone else (or sell yours and buy another used model), as many aftermarket models come with DVI.

I would sell the monitor used on CL, HWS, or any other platform, get what you can for it, and use that towards a more modern display. But whatever option you choose, best of luck!
In order to run 144hz on the Acer GN246HL, you need to do two things:

1. Ensure that you are connected via the DVI port (white port). You cannot get 1080p 144hz on this monitor with the VGA port (blue port), or the HDMI connector.
2. Once you've ensured that DVI is used, open your NV Control Panel, and change the refresh rate (under Display --> Change Resolution)
The way that LFC works is that it refreshes a frame multiple times when you fall below the FreeSync range.

So let's say that you had a monitor with a FS range of 30-60hz. If you dropped to 29fps, the monitor would operate at 58hz, refreshing the frame twice. This gives you an effective range of 1-60fps.

The problem is when you run into a display with a 40-60hz range. At 39fps you can't run that display at 78hz, so you lose FreeSync benefits. Even if the display supports a higher refresh rate, if the output device is locked to 60hz, the display doesn't compensate for it.

AMD used to require the max refresh to be 2.5x the minimum refresh for LFC to kick in, IE; 30-75hz, 48-120hz, etc. However, that changed and AMD now certifies displays for LFC so long as maximum refresh is at least 2x, like pretty much most of Samsung's high refresh displays and their ~70-144hz range with LFC.
There are plenty of 32" 144hz monitors out today. It's just that for the longest time, 24" and 27" were the best selling so monitor manufacturers put more investment in those segments.

Now larger format and ultrawide displays are seeing the largest growth, hence the surge in features/quality in those segments.
> I wonder if he has to give back the bonus if the Eagles pass on his option

All indications are that he keeps it. He only has to pay it back IF;

* The Eagles decide to keep him at $20mil, AND
* He'd rather be a FA

It basically becomes a buyout at 1/10th the price. It's an incredibly generous and fair deal that seems to address everything - Nick's desire to start somewhere and be paid like a starter, the immediate issue of Wentz possibly not being ready for week 1, and the (God no!) potential catastrophe of Wentz regressing or becoming the next Sam Bradford.
Nothing different there, and not a theory.

To paraphrase, I stated, "the Xbox One is still a 60hz device, and here's what you need to consider in a FreeSync display because of that."

That hasn't changed yet. It will change with the May update. Kudos to Microsoft for rapidly addressing the issue. And shame on you for resorting to lies and deliberate misrepresentation to bolster your viewpoint.
Ah, I was waiting for this :)

You are, once again, deliberately misrepresenting what I stated. No surprise there.

I made two points that you argued against:

* The April update was still limited to 60hz (and I went into why this was an issue)
* And that MS had not at that time discussed the possibility of high refresh support.

And I was correct. MS is now publicly talking about it (they didn't before), and they are saying that it will be included in the May update (because it wasn't in the April update).

Anyway, I know you're going to argue, lie, and misrepresent my stance, so to save us the trouble, I'm not going to engage your further (unless you engage in civil, truthful conversation), and I'm just going to link to the prior discussion for anyone who wants to confirm what I've just said.

https://www.reddit.com/r/hardware/comments/8at1xl/freesync_on_xbox_one_now_available_for_testing/dx19pe0/
There are no speakers, and no audio output for speakers. You're going to have to run the HDMI out from your console to a receiver which then splits to this monitor and separate speakers. Or some other way of getting audio that works for you.
Intel has slowly been transitioning its series' up the core/thread count chain. For 7th gen, the Pentium G moved up form 2c/2t to 2c/4t. For the 8th gen, we saw the i3, i5, and i7 all move up in core count. There have been rumors (of varying credibility) that Intel's ultimate goal is to sell each series with Hyper-Threading, so by 9th or 10th gen it could look like this:

* Pentium G = 2c/4t
* Core i3 = 4c/8t
* Core i5 = 6c/12t
* Core i7 = 8c/16t

It would be nuts to think that by the 9th or 10th gen, the unlocked i3 could offer performance comparable to the i7-7700k. And while Intel is working to raise the core count, AMD is working on IPC, latency, and clock speeds for Zen 2 and beyond.

Do I ever love competition!
The Acer GN246HL is one of the first 144hz monitors released, and one of the worst by modern standards. It should be avoided. There are a lot of good ones with similar specs, better overall quality, and the same ballpark price. The top recommended were the ViewSonic XG2401/2402.

The MSI uses a VA panel. This will give you better color and viewing angles than the TN panels used in the Acer and the ViewSonic models, but will have slower real-world pixels transition (response) times, leading to some additional motion blur in specific cases. Overall subjective reviews on the MSI models have been positive. They appear to be a great value offering.
This is important, and it's good to see MS announce it. Even if games still target 60fps, the mere support for high refresh means improved LFC support.

Example - Monitor is 120hz with a 40-120hz range with LFC. With the April update, it is limited to 40-60hz, thus disabling LFC. But with the May update, you'd get the full range and LFC, so even a 60fps game can benefit if there's a sudden drop below 40fps.

Good on MS for quickly responding to community feedback.
>  they've used it to push out some pretty good features, and it's much easier to make it an app update rather than a full system update which would have to go through the watch maker.

That's the trick. These updates were coming from the Android Wear app to begin with. They just realized they could also update the version number the same way, thus reducing the perception of fragmentation.

They could just as easily do the same on Android using the Google Play Services app (yes, I'm aware that Play services is on the watch as well). Play services and other background updates allow for increased functionality. Using this to alter the version number would also reduce perceived fragmentation.

Again, that's the trick. They didn't alter how updates were delivered. The underlying OS and security updates are still delivered via OTA. But by raising the version number via the app update, they improve the perception from users.
I think them updating the version number via the app update is a nice trick, and one that they might extend to Android on phones eventually. It's a great way to make it look like they're combating fragmentation. And perception is reality.

The software updates aren't any different than they were before, but now they simply raise the version number when they do it.
> No they did not

They absolutely did, and that's why you are being downvoted. You may have missed it, but they posted A LOT in the megathreads. They basically said, and I'm paraphrasing, 'While we trust our results, we acknowledge that our results don't mirror the rest of the community, so we are in the process of re-evaluating and will have an update as soon as possible."

> And now when I wanted to find exactly that cocky reply it was gone, but the thing is the user reply to the cocky comment is still there...

Link to the reply? I'd like to see this.
Battery usage will always be a bit weird for a day or three after any major updates.

Downloading and installing an app seems to take a lot out of the watch as well, so battery usage will always tank on days where you update/install multiple apps.
Fimrware update, or was it a WearOS app update via the Play Store?

If the latter, we won't know for awhile. The release notes on Google app updates usually only reflect the prior major release and not the current release.
As for the legitimacy of it, my stance is simple: If MS wanted our money, they'd release the game.
To be fair, Intel wasn't much competition in this segment. I've used the Tag Connected 45, and it is a janky piece of crap in terms of UI responsiveness. Comparing it to my LG Watch Style is like comparing the first sub-$100 Android phone to a modern iPhone or Pixel. It's that bad. Even my 1st gen Moto 360 was smoother and more responsive.

To be clear, I'm not judging the Tag's cost, styling, or any of that. I'm merely talking about performance.
Depends on the jurisdiction. My county does not allow attorneys in small claims court. You can consult one, however.
If a quality, non-lottery monitor had a decent range, I'd still be an AMD user.

The LG 34UC98 had a pitiful 55-75hz range and the model that I had spazzed out if you tried to alter it with CRU. But that monitor had better picture quality and build quality than my current AW3418DW.

Give that beast a 30-75hz range and AMD has a winner. But AMD and winner doesn't normally go in the same sentence when we're talking graphics. Great job on Ryzen though.
Take Allo as it is, and add SMS/MMS fallback to it. This isn't rocket science.

Or stick with Hangouts.
And Apple can refuse to support it, thus limiting support to a lot of customers. Basically, Apple to Apple would be iMessage, Android to Android would be RCS/Chat, and Apple to Android would be SMS fallback.

And while this appears to be equal footing, finally, Android's solution would still be carrier dependent, whereas Apple is closed off and works on all iPhones.
If using Windows 10 (and presumably older versions as well), click the start button and then start typing 'calibrate." This will bring up the 'Calibrate Display Color' wizard, which will guide you through an interactive process that will get you in the ballpark.

After that, visit [Lagom] (http://www.lagom.nl/lcd-test/) for more fine tuning.

Will these be 100% accurate? No, but they'll get the display to where you subjectively prefer it. It should offer a better experience than using online settings that won't really apply to the monitor in front of you due to variance.

I hope this is helpful to you, and best of luck in tuning it!
You really shouldn't use ICC profiles and settings from the internet because even with the same model, revision, and batch, monitors will vary from unit to unit. I go into it more in this post here:

https://www.reddit.com/r/Monitors/comments/82dcey/aw3418dw_calibration_measurements_results_and/

> As I've said before, uncalibrated displays vary from unit to unit. An ICC profile is a means to correct for the hardware deficiency via software means. If I were to apply the ICC profile supplied from TFTCentral, I would get a very different outcome than they had given the differences in our out of box results. This is why you don't use ICC profiles supplied from someone else.
Look for the most recent benchmarks of the GTX 960. That GPU is just over 3 years old now (released early 2015). Today should give you a ballpark idea of how AAA games will fare on a 1060 in a year or two.

That said, there are other variables. So again, this is a ballpark.

A GTX 1070 will give you more peace of mind if you can swing it.
I've actually used this before in some situations. It helps that I'm very creepy looking.

Basically turned a negative, my horrible looks, into a super power. And only once has it let to police involvement. Still no regrets.
I'm waiting for the Adored video where he explains how Anand's results are correct and everyone else is the outlier.

Joking aside, I'm still impressed with this release.
The most interesting feature is the 30fps mode. Basically, if you have a 40-60hz range, it can adapt that to also work at 20-30fps, basically behaving similarly to LFC.

But for 60fps games, it's still 40-60fps. For 48-75hz monitors, this becomes even more restrictive as the Xbox is still a 60hz device. The supported ranges become 48-60 and 24-30.
> (Unless I'm entirely missing the point of what you're saying.)

You are, unfortunately. Nvidia invests money into marketing their co-branded products, and that includes MSI's Gaming X lineup. And the average consumer would have a hard time distinguishing [THIS] (https://us.msi.com/Graphics-card/GeForce-GTX-1060-GAMING-X-6G.html) from [THIS] (https://us.msi.com/Graphics-card/Radeon-RX-580-GAMING-X-8G.html) on a store shelf. As a result, the marketing dollars spend by Nvidia can inadvertently help their competitor.

And yes, I'm aware that AMD markets it as well.
I wanted the case, so I paid the ridiculous cost, but I ripped out the smart device and the lighting strip to make cable management easier.
NZXT H200i as as a compromise? Mini-ITX, slightly larger than his. Can take 4 case fans (2 front, 1 rear, 1 top), a full-size GPU, and an ATX PSU.

Or the H400i for a slightly larger variant that can house a Micro-ATX mobo.
I have a 7700k with my 1060. To many that seems like an odd combo, but I get a new GPU every generation. Doing it this way allows my CPU to last me through 3-4 GPU generations.

If you try to make them balanced from day one, you'll be replacing everything at once. I prefer to upgrade my builds piecemeal.

A good compromise right now would be the i5-8400, or (likely) the Ryzen 5 2600. Those would have enough headroom to serve you well through at least one GPU upgrade without being grossly unbalanced at the start. They're also an excellent value.
> No, there's been absolutely no news. Not even the hint of news about a new smaller nanometer architecture SOC

Qualcomm paid lip service to it at a conference in Hong Kong late last year. Just the general, "smaller, faster, more efficient" for their next IOT/Wearable SOC, but nothing specific.

Here's a sensationalized article on it, but the quotes from Qualcomm's director of product management are worth the read.

https://www.techadvisor.co.uk/feature/wearable-tech/qualcomm-teases-next-generation-snapdragon-wear-features-3665591/
You are absolutely correct.

Which is why I made the "in a vacuum" statement. If Nvidia had simply said, "Look, we don't want our marketing dollars going towards your gaming brands that aren't Nvidia exclusive, so let's make a coherent brand behind GeForce," I'd be on board.

But since they are co-opting brands that were built up over years, it's all take and no give. Additionally, there's more to GPP than just this clause.
I really don't want to defend the GPP, as I see it as strong arm tactics. But I do see one valid issue on the part of Nvidia.

If MSI is going to use the Gaming X line for both Nvidia and AMD, why should Nvidia put any marketing muscle behind it? AMD would partially benefit from Nvidia's marketing push (not every gamer is as informed as the typical person on this subreddit).

"You have to have a separate gaming brand for your Nvidia and AMD GPUs," sounds a lot more fair than, "You can't sell AMD GPUs and still sell our product," as they supposedly pulled on XFX.

Again, I disagree with it, but I can see that point if it were in a vacuum.
FX-9590 + quad crossfire Vega 64.
No power supply is 100% efficient. At 90% efficient, a 1600W load on this will draw 1778W from the wall.

You have to at least account for that.

DISCLAIMER: I didn't look up the actual efficiency of this model. I used a nice round number for illustrative purposes only.
I'd honestly never want to use these on an 8700k. I think an i5-8400 or Ryzen 5 2600 would be the ideal match for these in an SFF build.

You'd have to be really ok with noise if you used these with a higher end CPU.
Mine was delivered by Fedex. Shipped straight from Dell, there was no brown box over it. You could see what it was.

The Fedex driver was asking questions. Normally they're all, "chuck it at the house and go, I have a quota to meet." Not this guy. He spent a few minutes talking to me about it and, if I hadn't mentioned "getting back to work, will unbox it tonight or tomorrow" about 3 times, I think he would have asked to come in and try it out. It was awkward, to say the least.
My opinion is that if they can be found for about the same price, go with fewer sticks. This makes future expansion/upgrades easier.

Also, there are some examples where 4 sticks can be worse than 2 sticks, such as but not limited to:

* On one of my older nForce motherboards, filling the 4th memory slot reduced the ability to overclock
* On some motherboards (I cannot confirm if this is universal or an anomaly), going to 4 sticks over 2 sticks raises the command rate from 1T to 2T (part of the timings), which means slightly increased latency, and therefore, slightly less performance. You won't notice it though.
* Supposedly populating all 4 slots puts more stress on the CPU's integrated memory controller, thus marginally limiting overclocking headroom

But really, I'm grasping at straws here. The future expandability is the big thing here as the other issues above are minuscule and/or isolated. But if you don't plan to upgrade your amount of RAM for that build in the future, and you can get 4x4 cheaper than 2x8, go for it.

Amazon is way too lax on returns. Given the AHVA ('IPS') panel lottery, shenanigans like this happen all too often. Same issue with GPUs being swapped out.

The problem is two-fold:

1. Amazon doesn't properly check returned items for these sorts of shenanigans
2. Amazon repackages returned items and tries to sell them as new to unsuspecting customers
Exhaust.

You can tell the fan direction by how it 'scoops.' If the scoop is facing you, the air would blow in your face (exception, EVGA GPU fans, which spin backwards).

Example - http://www.cryorig.com/qf120_us.php

Scroll down to the 3rd fan picture (first one that isn't spinning, says "HPLN" on it). Because of the way that the scoops/blades are facing, it would spin clockwise and air would come towards you.

Now scroll down to the next fan picture (laying flat, says 'Rubber' on the corner). This one would blow air downward, or away from you.
> Wrong, Jaguar is a CPU, it does not have an API!

I apologize if I was unclear. I never intended to say that the CPU has a built in API. I understand what an API is. And developers have been using low level APIs to interact with Jaguar more efficiently than on the PC side.

> That "low level approach" is very similar to DX12 and Vulkan

Correct. However, this low-level approach has been available on these consoles since day one. Vulkan and DX12 have issues.

* Vulkan isn't as developer friendly, and doesn't have a team (like MS) that will help developers integrate its usage into their game/engine, so uptake has been glacial.
* DX 12 is limited to Win10 only. The Win7/8 userbase is holding it back.
* Low level development is hard when you have one target platform. It's much harder when you have numerous hardware targets (PC).

>  the GPU does not "balance" draw calls, the CPU generates draw calls and sends them to the GPU, that it!

I didn't say that the GPU balances the draw calls. I said that developers have found ways to balance draw calls between the CPU and GPU. [Or as one developer put it] (https://forum.beyond3d.com/posts/1974650/):

> depending on the developer and the way the games are designed, CPU tasks can be offloaded to GPU. A majority of CPU time is very much used towards the render block and the more complex the scene the more the CPU is loaded up.

> We do have functions in place to move draw calls over to the GPU; i'm pretty sure a lot of work is being done in that space by the API holders, vendors and developers.
If we're successful in moving draw calls over to the GPU, then the jaguar has plenty of CPU to do other tasks.
1. Cable management
2. You have the top fan set as an intake. It should be an exhaust, and ideally, directly over the CPU.
3. It's hard to tell from the side, but ensure that the front fan is intake. You want intake on the front, exhaust on the rear/top (rear fan is configured correctly).

I'm sure you'll get other tips, but that's what I've got. Good luck!
That makes much more sense. I'd say go for it.
> Still has a potato CPU

Yes, and no. Technically, it's a potato CPU. No way around it.

But developers have commented before that their low-level approach to console development allows them to get more draw-calls out of this CPU than on modern mid-range desktop CPUs. It's pathetic, really.

Additionally, and I don't see much discussion on this happening in the PC world, but for the Jaguar APU developers are able to balance draw calls between the CPU and GPU side of it as needed, freeing up whichever is the current bottleneck.

This makes the current gen consoles the ideal 1080p (or 900p, for the Xbox One) 30fps gaming device on an extreme budget (especially with today's GPU/RAM prices). And the upgraded consoles can do 4k or pseudo 4k, again, at 30fps comfortably.

Where they struggle is in trying to reach 60fps without sacrificing too much.

Again, you're not wrong, I just wanted to further elaborate. The Jaguar APU, due to its low level API and draw call advantage, is an ideal pairing with the GPUs used in these consoles, and a superior CPU wouldn't benefit them as much as many think, simply because the GPU would then hold the system back.
If you like that style of CPU cooler, look into the low-profile Cryorig C7 instead. Similar profile, shouldn't mess with the top fan, and also has that white fan to match the case (again, if you went with the white H200i, as I did).

On the downside, the C7 is not as good a cooler as your current one, I believe. It's better than Intel box coolers, and marginally better than AMD's newer Wraith coolers. I wouldn't use it with an i7-7700k/8700k (it will work at stock frequencies, but you won't like it). For mid-range CPUs (i5-8400, 8600k stock, Ryzen 5 1600/1600x stock) it should be fine.
I had a similar problem with my H200i, but my CPU cooler wasn't the issue, it was the 8-pin on the motherboard that made it a tight fit.

Your best bet would be to switch from a low-profile CPU cooler to a tower CPU cooler. This will give you lower temps, less noise, AND the clearance needed for a proper top-mounted exhaust fan.

I'm using the Cryorig H7 as my CPU cooler, and the Cryorig QF120 Balanced for the front, rear, and top. They're affordable, effective, and best of all, they match the H200i in white. The QF120 Balanced is the same fan used on the H7, which helps with matching in terms of color and noise.

Just an option for you, if you don't find the low-profile fan you're looking for.
In that case, the 212 is better than the Wraith Spire (as shown in the linked video), though as mentioned, definitely go for the R5 2600 which comes out 4/19, rather than the 1600.

As for a cooler, my suggested H7 isn't worth spending $25-$30 for if you already have the 212. I'd go something higher end if you're going to upgrade.

Get your R5 2600, use the 212, and if the noise bothers you, upgrade at that point. And by then, you may even have more than $70 to throw at it.
> Wraith spire or an extra hyper 212 led, and in time what liquid cooler under 70$ to upgrade to if need be.

In terms of how Hyper 212 would stack up against some of Ryzen's stock coolers, here's a solid video that outlines the differences.

https://www.youtube.com/watch?v=VNjkDoYZYjU

TLDW (looking at just the R5 1600 numbers):

* At stock, the 212 beat the Wraith Spire under load, 54° to 63°. They were both 32° at idle.
* At a 4ghz OC, the 212 won at idle (32° vs. 42°) and load (68° vs. 89°).

Basically, if you plan to run stock, don't waste the money on a custom cooler. The Wraith Spire is fine, and much better than what Intel offers. If you plan to OC, an aftermarket cooler can help. That said...

**Unless you already have it, don't get the Hyper 212.** It's been trading off its reputation for years, but it's no longer the best budget cooler. There are many that are better for a similar price. The one I use, the Cryorig H7, beats it handily at a similar price, and even that probably isn't the best budget CPU cooler anymore, if it ever was.

And as /u/redditelder stated, don't get an AIO for $70 or less. A quality air cooler will run cooler and quieter than the typical AIOs in that price point.

Because I mentioned the Cryorig H7 as a budget alternative (MSRP $35, typically $30, often on sale for $25), here's some reviews:

* [Tweaktown] (https://www.tweaktown.com/reviews/6958/cryorig-h7-cpu-cooler-review/index.html) | "The fact that you can get all of this style, clearance, and performance for just $34.50 is outstanding."
* [Kitguru] (https://www.kitguru.net/components/cooling/dominic-moass/cryorig-h7-air-cooler-review/) | "Simply put, for a £37 air cooler, the Cryorig H7 is fantastic. We saw it go toe-to-toe with much more expensive air and liquid coolers, even coming out on-top when compared with the MasterAir Maker 8 from Cooler Master. It also beat the Hyper 212 LED by 5 degrees."
* [TechPowerUp] (https://www.techpowerup.com/reviews/CRYORIG/H7_Universal/) | "With perfect memory clearance, solid performance, and low noise levels, the CRYORIG H7 Universal is the mainstream cooler to beat."
* [HardOCP] (https://www.hardocp.com/article/2015/09/22/cryorig_h7_cpu_air_cooler_review/) | "CRYORIG has performed a feat that I thought was unattainable. It surpassed the mighty Scythe Mugen 4 as the cooler offering the highest bang for your buck. Scythe has held that spot since November of 2013; nearly two years!"
* [eTeknix] (https://www.eteknix.com/cryorig-h7-cpu-cooler-review/) | "The Cryorig H7 is by far one of my all time favourite coolers. It’s not the coolest, nor is it the quietest, but when it comes to getting a great return on investment, you’ll struggle to find anything better for less. Those on a tight budget who need a reliable and great looking cooler, this is the one to buy!"
* [Modders Inc] (https://www.modders-inc.com/cryorig-h7-cpu-cooler-review/) | "Performance wise the CRYORIG H7 is above average and can be made to perform better with an additional fan at the rear as observed in the thermal images having a few warm spots in the exhaust area. Acoustically, it is far from the quietest at full 1600RPM throttle, although quieter than the venerable CM Hyper 212+. "

Of note in the TechPowerUp Review, it ends up edging out the Noctua NH-U12s not only in performance, but also in noise (!?), while costing half the price.
Here's a recent direct comparison between the i5-8400 and the Ryzen 5 1600, which are the most directly comparable.

https://www.youtube.com/watch?v=4fd_GXFBUtk

Then on Thursday, see how the new 2600 stacks up against the 1600 (a 5%-10%  improvement is the expectation, anything else is a bonus).

Both are excellent budget/mid-range CPUs and you really can't go wrong either way.
> If you're concerned about it lasting, consider purchasing an extended warranty though it's probably not worth it.

To add to this, use an American Express if you have one. Visa/MC also add to the warranty, but their terms are less generous.

My AW3418DW has a 3-year warranty, and the Amex makes it 4. IF it dies between years 3 and 4, they'll either pay for the repair/replacement, or just credit your account the original purchase price (plus tax).
It's expected at this point. The Pixel 1 and Pixel 2 have both been the prior year's iPhone, but with Android.

This isn't a bad thing for many people, but there are going to be design choices that we don't like. I love my Pixel 2 for what it is.
It's up :)

https://www.reddit.com/r/buildapcsales/comments/8cw4j4/psu_seasonic_prime_ultra_gold_550_5837_8337_25mir/dxjjskm/
Here are my results. Let me know what you think.

##Contents:

* Power Consumption Table/TLDR
* Testing Methodology
* Why These Results Differ from an Online Calculator

##Power Consumption Table/TLDR

**Columns:**

* Test | Shorthand for the test used, see methodology for a full description of each test.
* Power Draw (Wall) | The power, in watts, measured at the wall with a [Kill-A-Watt P3 4400] (http://www.p3international.com/products/p4400.html).
* Fluctuation | The amount up or down that I saw from the mostly stable displayed wattage, IE, 35W +/- 1W = a  range of 34-36W. I tried to run tests where the power would be stable so this number would be as low as possible.
* System Draw (87%) | Estimated system power draw based on low estimate of PSU efficiency.
* System Draw (91%) | Estimated system power draw based on high estimate of PSU efficiency. See methodology for why 87/91% were used and what they mean. In both cases, I rounded up to the next whole watt. So, 1.01W would be 2W.

Test|Power Draw (Wall)|Fluctuation|System Draw (87%)|System Draw (91%)
:--|:-:|:-:|:-:|:-:
Idle|35W|+/- 1W|Null|Null
WoW @ 60hz|160W|+/- 1W|140W|146W
WoW @ 100hz|189W|+/- 1W|165W|172W
WoW+HB|205W|+/- 2W|179W|187W
Aida64|194W|+/- 2W|169W|177W
WoW + AIDA64|216W|+/- 1W|188W|197W

## Testing Methodology

The goal was to obtain a relatively flat power reading by simultaneously taxing the CPU and GPU to get a realistic reading of where my power draw would be during gaming. It's almost impossible for a game to fully max out the CPU and GPU simultaneously (one has to be the limiting factor), but in the end I was able to push them both to 100% with some fuckery.

In the chart above you'll see wall readings and 87/91% readings. What are they? Well, mankind hasn't invented a way yet to transfer power at 100% efficiency. Your PSU has an efficiency rating that tells you essentially how much power is lost. So, if you have a 500W PSU that is 50% efficient, and your system actually draws 500W, the PSU will pull 1,000W from the wall to supply the PC with 500W. Due to this, 500W is dissipated as heat (as bad as this sounds, many PSUs were in the 60-75% efficiency range before the 80+ initiative).

The first reading, at the wall, shows what I'm actually drawing. This is not the PC's power draw. Because I am using an EVGA G2-650, I'm using the test figures from JonnyGuru's cold box testing, which closely approximates my usage. [(SOURCE)] (http://www.jonnyguru.com/modules.php?name=NDReviews&op=Story3&reid=429). At a load of 133.8W, it measured 87.6% efficient. At a load of 331W, it measured 90.2% efficient. Since all of my non-idle results are between those values, I'm using 87% and 91% as the lateral limits. That means that the actual system power draw is somewhere between these two numbers in the table. Idle wasn't estimated because, at such low power draw, system efficiency tanks. Modern PSUs are at their most efficient from ~15% to 100% load (yes, even a full load is rarely off from peak efficiency these days).

**WoW @ 60hz**

World of Warcraft isn't known for being a demanding game. But in my prior testing, running at high settings and high res will push a GPU's load and temps higher than any other modern game I've tested. It also helps that the GPU is already my main bottleneck, being a GTX 1060 6GB paired with an i7-7700k and an Alienware AW3418DW (3440x1440 100hz G-Sync). I normally play at 60hz with V-Sync, as this gives me a smooth experience and, most importantly, keeps the fan noise low (my biggest trigger). These results had the game at 60fps, the GPU at about 70% load, and the CPU at a low load. This test approximates a light gaming experience.

**WoW @ 100hz**

Because I didn't have much GPU headroom left, this pushes me to 100% GPU load and 80fps, but also increases the load on the CPU. This would simulate a moderate gaming load on my system, and the power draw rises accordingly. I did try disabling V-Sync and lowering settings to tax the CPU more, but no combination that I tried brought the power draw upwards.

**WoW + HB**

I decided that if I was going to max out the CPU and GPU together, I'd have to get creative and use a background program known for eating up CPU cycles. I started by doing an encode using Handbrake. This was a failure because it drops to maxing only 2 cores/threads (I have 8 threads) while the game is in the foreground per MSI Afterburner's monitor. As a result, while power draw was higher the results were disappointing and I'd have to look for more tests.

**AIDA 64**

I decided to stress the CPU, FPU, memory, GPU, literally everything except the disk drive. Unfortunately the GPU test ends before I could get to a higher temp (which would result in more leakage, thus higher power draw). These numbers were lower than they should have been and therefore should not be trusted.

**WoW + AIDA64**

Success! First, I loaded WoW and let it run until my GPU got into the mid-70s. Then, I loaded AID64. I was able to sustain 100% load across all cores/threads and 98-99% on the GPU. This was the highest sustained number that I could get, with an absolute peak of 217W at the wall.

##Why These Results Differ from an Online Calculator

The majority of online PSU calculators are presented by retailers who sell PSUs, or companies who make/sell PSUs. Asking an online PSU calculator to tell you how much power you need is like asking a salesman on a used car lot which car you should get.

That said, there's more to it than that. There is a noble side to what they do, and it's the same reason you see Nvidia and AMD recommending more power than you should really need. Let's look at two "similar" power supplies:

* [EVGA SuperNOVA G3 550W] (https://www.newegg.com/Product/Product.aspx?Item=N82E16817438095&cm_re=evga_g3_550-_-17-438-095-_-Product)
* [Logisys PS550ABK 550W] (https://www.newegg.com/Product/Product.aspx?Item=N82E16817170010&ignorebbr=1)

They are both 550W PSUs. They both (on paper) can support a standard GTX 1070-based system, and they meet Nvidia's requirements to do so. But while the G3 could comfortably handle a 1080ti even overclocked, the Logisys can't.

The majority of system power today is over the 12V rail(s). The EVGA can handle UP TO 549.6W over the 12V rail. That makes it a legit 550W PSU. The Logisys? 300W max. It's a 300W PSU with numbers trumped up to seem like it can handle 550W (and honestly, I doubt it could handle the rated loads, but that's another story).

Basically, AMD, Nvidia, and these online PSU calculators are protecting you from human stupidity. If you're buying a no-name budget PSU, then yes, absolutely go for the calculator's suggestion. But if you're buying a quality PSU (and with the SeaSonic Focus 550W Gold models hitting ~$50 lately, there's ZERO excuse for going cheaper), those numbers are grossly inflated and not applicable.

The low end estimate that calculator gave me was 316W under a full load. The highest end measurement I have is 197W. That means that they over-estimated my power draw by AT LEAST 60.4% (and that was with me using numbers most favorable to the calculator).

In your case, Anandtech shows a wall measurement (again, higher than actual system draw) of 335W when using a 1080ti FE + an overclocked i7-4960X [(SOURCE)] (https://www.anandtech.com/show/11180/the-nvidia-geforce-gtx-1080-ti-review/16), so there's no way your system needs 650W, no matter how much RGB you have, and even with typical overclocking.

As always, feel free to ask questions or, if you feel I made a mistake somewhere (certainly possible), please let me know.
Ok...I said that. Were you trying to expand on my point?
Alrighty then, this is preliminary as I'm still working. Here's the numbers that I get from inputting my system into the calculator. And here's the answers that I gave it.

Note, I used a "high" and a "low" because some parts can be interpreted various ways.

* Motherboard | I used Mini-ITX for the low estimate, and Desktop for the high estimate. Mobo is an EVGA Z270 Stinger Mini-ITX, and most would agree that this was designed for desktop usage and at least mild overclocking.
* CPU | i7-7700k, I used 4.2ghz/1.2V (the stock config for the CPU and for this calculator) for the low, and 4.4ghz/1.25V (what my system reports during gaming) for the high. It recommended 90% for TDP, I used 80 and 100.
* Memory | 2x8GB DDR4
* Video Card | EVGA GTX 1060 SSC 6GB, I used 1607/2002 for the clocks as that's the out of box, but 1898/2002 for the high setting since that's what it runs at.
* Storage | 1x M.2 SSD (Samsung 960 Evo 1TB)
* Optical | None
* PCI-E Cards | None
* PCI Cards | None
* BTC mining Modules | None
* Other Devices | USB 2.0 device x2 (Logitech receiver for wireless G703, Xbox One Wireless Controller Dongle)
* KB+M | 1x Gaming Keyboard (Logitech G810), no mouse (wireless, receiver accounted for above)
* Fans | 5x 120mm, all Cryorig QF120 Balanced (2 front intake, a rear exhaust, 1 top exhaust, and 1 included on the H7 CPU cooler)
* Liquid Cooling | None
* PC Utilization | Always On

Low Estimate = 316W load wattage | 366W recommended PSU wattage  
High Estimate = 397W load wattage | 447W recommended PSU wattage

The latter number I'm sure we'll disagree on. Everyone has their own idea for how much overhead they want/need, and their reasoning will be their own. But the first number, the actual power draw I guarantee will be wrong. And if it's not, I'll eat my GPU (don't wager anything, I already have prior numbers that tell me it's way off, but I'm using this as an excuse to get current, more accurate numbers).

Anyway, all my parts that are inside the case or otherwise directly connected to the PC are accounted for above. If you have questions, or think my numbers are wrong, let me know. Sometime after I get off work (I'm on the left coast USA), I'll plug in the Kill-a-Watt and get you the actual measurements.

EDIT: The 6-ft lightstrip is plugged into a my monitor, which is not plugged into my PC as a USB device. That's why it wasn't counted.

EDIT 2: And it's up - https://www.reddit.com/r/buildapcsales/comments/8cw4j4/psu_seasonic_prime_ultra_gold_550_5837_8337_25mir/dxjjskm/ (link is in case the reply gets buried below)
A lot of them have also moved to the AMD subreddit, and one is a moderator there. They use the same propoganda and even recently doxxed and harassed Ryan Shrout, a tech reporter.

AMD has become the unofficial PC parts brand of the alt-right.
I actually like it. It makes direct comparisons easier. I'd love for them to do the same with Radeon as well, ie:

* RX 650 = GTX 1150
* RX 660 = GTX 1160
* RX 670 = GTX 1170
* RX 680 = GTX 1180

And for any TI parts, AMD could use GT. For Titan class products, they could use Rage, Fury, Rampage, etc.

Their naming schemes up to this point have been inconsistent and are harder for casuals to follow. For example:

* R9 280 = GTX 770
* R9 380 = GTX 960
* R9 290 = GTX 780
* R9 390 = GTX 970

And their Fury lineup went against the 980/ti, occupying the former x90/x slot.
I could tell you why online PSU calculators are usually bunk, but right now I'm just some guy on the internet to you. What makes me more credible than them?

So tell you what, please link me your preferred calculator(s), up to 3. I'll put my system in there and report back with the wattage requirements that they give me for my system.

I'll then measure my total power draw from the wall with a Kill-A-Watt P3-4400 (actual system draw is lower than what is reported at the wall).

This should give you a good idea of how far off these calculators are.

> I also have tons of RGB.

Believe it or not, this adds VERY little to the system power draw. As an example, Cryorig's QF series non-LED fans draw 0.263A (3.16W) at max speed. The LED version is 0.45A (5.4W). My 6-ft lightstrip maxes at 5V 1A (5W).

But anyway, I look forward to your links.
Sounds like the type to double down. When it underperforms, he'll just blame Nvidia and switch to AMD.
An 8700k + 1080ti FE doesn't draw as much power as many think. Anandtech measures total draw at the wall, which gives higher numbers than actual system draw, and they got 335W from a 1080ti FE + i7-4960X overclocked. [(SOURCE)] (https://www.anandtech.com/show/11180/the-nvidia-geforce-gtx-1080-ti-review/16)

Most will need 400-450W if overclocking both the CPU and GPU considerably, and using an aftermarket 1080ti with a raised power limit. Users who don't overclock will use considerably less.
>  My 7700k at 5ghz and a 1080ti plus a 360mm aio liquid cooler only need 650 watts.

It needs less than that. People tend to grossly overestimate their power needs.
The edit came way after the original post, and by that time, I had already been harassed. So, you had it backwards.

But this seems to be intentional on your part. An analysis of your post history shows that 8% of your posts are in the AMD subreddit, and it's your 3rd most posted sub (behind 2 mining subreddits). Basically, you clearly have an axe to grind.
Correct. It's all a series of tradeoffs. The user needs to pick their own poison.
> Yeah thats what i have right now.

If you have it installed and working now, then you don't need to rollback your driver. However, if you don't have it installed, check your motherboard manufacturer's website, as they would likely have an outdated driver that would actually work.
Do you have the Realtek HD Audio Manager? There's an issue with the current version of the drivers that causes it to not install for many users. I'm using an older version of the driver supplied by my motherboard manufacturer (for one I'm glad my mobo manufacturer stops updating their drivers page after ~6 months).
> Actually no, that is not what empirical evidence means.

Right. You're ignoring what the company said, "It's not MS's fault, it's ours," because it doesn't suit your narrative. In yours eyes, you're not wrong because you choose not to be wrong.

AMD and MS never released a scheduler patch, but since that doesn't suit your narrative, your response is along the lines of, 'Well, I choose to believe that was the problem, and that it was patched and they lied to us. And I don't have to support my view with facts.'

> I find it extremely childish to support a for-profit brand as if it has some personal value. I also find pathetic passive-aggressive personal attacks in place of intelligent discussion equally childish.

As for me, I find it hilarious when my stance is that, "these people hate well sourced evidence," and you come in here telling me I'm wrong because you don't like my well sourced evidence. I'm not being passive aggressive, if that's what you're implying. I'm merely pointing out that you've served as proof for my stance. I actually appreciate you coming out and doing that for me.

> Incidentally i find it hilarious that you are arguing that i am an AMD fanboy because i dont trust AMD.

I never said that you were an AMD fanboy.

I've never insulted you. All I asked is for ANY evidence to back up your claims. And instead, you resort to:

> You really should check your logic before making a total ass of yourself.

So, you have a wonderful day. I sincerely appreciate you coming to the thread and proving my points for me. But if you ever do come up with actual evidence to support your claims (you can't, which is why you switched over to insults), I'd be glad to re-engage on that basis.
>  i am going to go with empirical evidence of dozens of reputable sources

I'd like to see these sources.

> and i am not too concerned of the technical details behind it.

And you just nailed my original point without realizing it. It's emotion over fact, just like T_D. You don't care about the facts, so long as something can be construed to make you feel correct.

Your "empirical evidence" that you can't cite wreaks of, "I've been hearing from a lot of people..."
> Isn't that kind of polarizing mentality only contributing to the overall problem of fanboyism though?

Possibly, but I honestly don't care.

> As you stated there are fanboys anywhere there are humans, but why not just ignore them as you would ignore a troll?

I am, by choosing not to participate in the entire subreddit, or spend money on the products that they worship.

>  Be it tech or sports, the few rabid fans present doesn't make all those fans rabid and delusional.

Correct, but when the entire community gets taken over by them, then the community becomes a problem.
> Incidentally the win10 scheduler was responsible for a huge performance loss, that is not even arguable.

And yet, AMD even cam out and said that it was not true, and that there was no problem with the scheduler and Ryzen!

> The performance gains on 7 vs 10, and 10 after it received patches to fix the scheduler

There was no "scheduler fix."

https://www.pcper.com/reviews/Processors/AMD-Ryzen-and-Windows-10-Scheduler-No-Silver-Bullet

Per official statements from AMD:

> We have investigated reports alleging incorrect thread scheduling on the AMD Ryzen™ processor. Based on our findings, AMD believes that the Windows® 10 thread scheduler is operating properly for “Zen,” and we do not presently believe there is an issue with the scheduler adversely utilizing the logical and physical configurations of the architecture.

and 

> Finally, we have reviewed the limited available evidence concerning performance deltas between Windows® 7 and Windows® 10 on the AMD Ryzen™ CPU. We do not believe there is an issue with scheduling differences between the two versions of Windows.  Any differences in performance can be more likely attributed to software architecture differences between these OSes.

So, if you're still refusing to admit the truth about something that AMD themselves have already clarified, how can I assign any credibility to:

> They have been vocally denouncing any form of fanboyism or denial of reality for at least a year and a half, including a lot of the wild guesses about ryzen pre-release included.
My understanding is that no, there is no difference. However, I don't use them so cannot verify in person. I use an X-Rite i1 Display Pro.
There are tradeoffs. Pick your poison :(
The 34UC98 comes pre-calibrated. It's not exact, but it's good enough for non-professional use.

Your example of multiple monitors is the best use case for it, IMO. He's want to match them.
I hope that to be the case, and that it's a trend. That subreddit can only be reformed from the inside.

I don't want to go through my entire post history to look for petty examples, so I just went through my submissions. Here you go:

https://www.reddit.com/r/Amd/comments/5yqjf7/psa_neither_amd_nor_microsoft_have_confirmed_a/

This was when AMD's userbase was blaming the Win10 scheduler for Ryzen's gaming performance. They took their speculation as fact, so I made a PSA. I then updated it when AMD commented. It got a LOT of hate.

My contention in my prior post was that this ehco chamber hates sourced information that counters their viewpoint. This post is an excellent example of that.
> The AMD community was toxic way before Adored ever showed up.

True. Jim is just using a tried and true formula (think Alex Jones) of harvesting ignorance + hatred to drive revenue. But these become cyclical, each fueling the other.
The steps you outlined apply to many things. I've joked about it before, but the AMD subreddit is VERY similar to T_D now.

* They will brush off any facts/research that don't suit their viewpoints
* They will actively harass media members that don't tout their viewpoints
* They're not above doxxing people for the above (think Ryan from PCPER and his family a few months back)

It is absolutely disgusting the way that some of them behave, and it's a damn shame that such a minority can poison the well to such a degree.
Adored is finally coming around it seems. His statement there mirrors what I said over a year ago when I unsubbed from and left the AMD subreddit.

**EDIT: I've since watched the entire 2-part video. He hasn't come around. He spent the first 90% bashing Steve in a conspiracy-style, and most of his (Jim's) points were bunk. He had one good point but he handled it like the drama queen that it is. The last few minutes was just a token "OH, I can be the rational one here" stance. He's still the same arrogant SOB that gleefully helped doxx Ryan.**

Basically, I plan to avoid AMD products for the foreseeable future. I generally trust the community more than paid reviewers, because the community gives you a more accurate understanding of where a product truly lies relative to the competition. Reviewers test a small sample of popular games, and they all test the same games. Thus, AMD and Nvidia optimize their drivers for these games. But go off the reservation and you start to see some oddities. The worst for me was Redout. I owned an RX 480 and a GTX 1060. The 1060 was nearly 2x as fast in Redout. Again, this was the worst example, but Nvidia beating AMD's equal part in games that weren't used in benchmark suites became obscenely common. So I made the switch and went back to Nvidia fully.

Let me tie that together. I'm stating that I need to trust the community. But the AMD subreddit is a community that cannot be trusted. There will be fanboys in any subreddit. But they take it to a very delusional level. And it's very hard to get facts out of them.

Basically, unless I can confirm that an AMD product meets my needs as well as or better than a competing product from Intel/Nvidia, I won't go with it. And AMD's fans make it VERY hard to give their products the benefit of the doubt.

AMD's fans are probably the biggest detriment to the company right now.
In one game that I tested, yes, but don't get too excited. There are other factors are play here, such as but not limited to:

1. Disabling game bar disables the alt-tab speed enhancement for fullscreen games, meaning you'll want to switch to borderless windowed mode if this is important to you. It's a trade-off.
2. When a game runs in exclusive fullscreen mode, the game has access to your monitor, bypassing the OS. If the developer wants to ignore color profiles, they can, and this 'fix' won't do anything.

The most commonly successful to fix this issue is to do the following:

1. For games that support borderless windowed mode, use it, and
2. Use "Borderless Gaming" to force the mode in games that don't support it. [Steam Link] (http://store.steampowered.com/app/388080/Borderless_Gaming/) | [Github Link] (https://github.com/Codeusa/Borderless-Gaming/releases)

Obligatory warning that the developer of that app is a known arsehole.
If the game has a setting allowing you to toggle between fullscreen and windowed, do that and see if the display changes.

I have my OSD set to "good enough" as seen via the link that I posted. My calibrated gamma is 2.22, but my uncalibrated is 2.42. When I launch WoW, it first unloads the ICC profile (you can see the desktop get slightly darker) right before the game launches.

Basically, if the game launches in exclusive fullscreen mode, you can be pretty sure that it bypasses the ICC profile. I only say 'most' to cover my arse in that rare case where a game might exist that breaks that rule.
Just to add to this:

1. I highly recommend using [DisplayCal] (https://displaycal.net/) over the included software. It's free, open source, more powerful, and doesn't software-lock features on any of the Spyder calibration tools, as the included software does. (Basically, the Express, Pro, and Elite are all the same hardware, with the features locked out by the included software, so DisplayCal will treat each unit the same).
* If you calibrate, an ICC profile will be created for your system. Many/most games running in exclusive fullscreen will bypass this calibration. Run your games in borderless windowed if you can. Also, try to get your settings as close to ideal using the OSD before letting the colorimeter create the ICC profile.

I talk more about this in my AW3418DW calibration results post here - https://www.reddit.com/r/Monitors/comments/82dcey/aw3418dw_calibration_measurements_results_and/
A person of below average intelligence isn't going to understand your point.
I had a 4590 before my 7700k upgrade. Paired with a GTX 1060 at 2560x1440 @ 60hz, the GPU was the limit in most games, with the CPU being a limit in World of Warcraft.

The GTX 1080 obviously has more headroom, but you're still going to be GPU limited more often than not. If you were to lower your graphics settings to hit higher frame rates, then you'd start to see the CPU bottleneck.

For MOST games at 60hz, the 4590 is more than enough for today.
At 1080p 144hz? Yes. At 4k 60hz? No. Anywhere in between, depends on the game and the situation.

And by 2k, do you mean actual 2k (1920x1080), or 2560x1440, which is often incorrectly called 2k?
If you received it via Amazon, can you return/exchange it through them?
Speculation was January through December, give or take. /s

I don't take stock in any of the rumors, just have the money ready for when it drops.
These are the primary advantages to EVGA motherboards, IMO. Top of the line, no gamer-y bling, and great warranty. Sounds like you may want to look into those (on the Intel side, only, unfortunately).
I've always targeted 60fps and this has been fine. You might have to go from Ultra to high to maintain ~60fps in newer games. Maybe medium/high mix for the most demanding titles.

I have no problem with my 1060's performance right now. It's good enough for my to justify holding out.
Effectively 100mhz faster than the 8700k out of the box. Base and single-core turbo numbers are results you won't actually see in gaming outside of a quick burst, so those numbers are propped the highest for marketing purposes.
And yet, mine does.
Same monitor, running a GTX 1060. I can afford a new GPU, but I'm holding out for the next generation.
I had that in one of my prior edits. The Corsair RM550X, while a solid unit, compares closely in performance with the FOCUS Plus Gold above, but is priced closer to the FOCUS Plus Platinum, so I didn't see a point in recommending it at this time.
I'm always and never satisfied. But I make timetables/milestones for each upgrade, and I stick to them. I just finished most of the aesthetic aspects of my system (the latest were color-matched fans that don't suck). Here's my upcoming upgrades:

* GPU - I usually get the x60 from each generation, and hand my old one down to my wife (who is a more casual gamer). Her GTX 950 is getting long in the tooth. She'll get my 1060 6GB when I get an 1160. I've been considering an 1170/1180, but the noise from heat/fans made me reconsider. I'll stick with low wattage cards. I'll continue with the x60 of each generation, and will consider AMD if they ever get back in the same league.
* Chair - I REALLY want to get rid of this back-breaking racing chair this year. I'm looking at everything from the Eurotech i00 (~$600) to the Steelcase Gesture (~$2500). Money isn't the issue.
* CPU/Mobo/RAM - My 7700k/Z270/16GB DDR4-3000 are overkill and will be for a long time. I'll keep this combo until DDR5 comes out. At that point, I'll look at the modern i5-8400/Ryzen 5 2600 counterparts. Stepping down for lower heat and fan noise. I want a mid-range silent system. I recently traded down to a stylish mini-ITX case, which now sits <3 feet from my head, so noise and thermals are a bigger deal for me.
* PSU - A great PSU should be retained at least until the end of its warranty. Mine expires in 2022. I won't upgrade before then.
>  This model will be $100 cheaper, but will only have one year warranty, and not carry any of the support bells and whistles that the Dell S2716DG will have as purchased on the Dell website.

The S2716DGR comes with the same 3-year warranty as the S2716DG. The [Best Buy Listing] (https://www.bestbuy.com/site/dell-27-led-qhd-gsync-monitor-black/5293502.p?skuId=5293502) has it under 'Specifications', at the bottom. Amazon's listing, however, is less helpful. The included warranty PDF refers you to the product's packaging.
EDIT: Added Links for the recommended products, and for the cited reviews. Mea culpa.

-----

In terms of actual wattage, you'll be comfortably under 300W under a full load (maxed out CPU and GPU, something games rarely do). My 7700k (similar power draw to 8600k, maybe more) and GTX 1060 (~30-50W lower than 1070) draws ~185W from the wall when gaming (verified with Kill-A-Watt P3-4400).

That said, I always recommend a quality power supply. The same components that improve overall quality end up also increasing the capacity. You'll rarely find a high-quality PSU below 500W, and the ones that are (SeaSonic Platinum/Titanium) costs as much as 3 times what a very good 550W PSU will cost.

Because I'd recommend 550W (even if you grossly overclock, this will still be more than enough), I'm going to recommend 3 products based on price point. From good to best, they are:

* [SeaSonic FOCUS Plus Gold 550W] (https://www.newegg.com/Product/Product.aspx?Item=N82E16817151189&ignorebbr=1) - Gold efficiency, 10-year warranty, $55 after MIR, $80 out of pocket. There is nothing cheaper worth getting when this unit exists at this price, IMO.
* [SeaSonic FOCUS Plus Platinum 550W] (https://www.newegg.com/Product/Product.aspx?Item=N82E16817151193&ignorebbr=1) - $100. Upgraded to Platinum efficiency, which might save you $2-$3/year. Is it worth double the prior model? Honestly, no. But if you've got money to spare and want to make a tiny contribution to energy conservation, here you go.
* [SeaSonic PRIME Ultra 550W] (https://www.newegg.com/Product/Product.aspx?Item=N82E16817151218&ignorebbr=1) - The big daddy in this segment, $120 gets you Platinum efficiency, an uber quality running fan, and a 12-year warranty. You're basically paying 20% more than the above for a 20% longer warranty. These units are tanks and will last you a LONG time. My last SeaSonic was used for 9 years then sold. It didn't quit. That had a 5-year warranty and was Bronze efficiency.

You can go higher wattage for a higher price, but you really shouldn't unless you plan to go multiple GPU. Most of the reasons for upgrading the wattage are based on very bad information, such as:

* **PSUs are most efficient at 50% load** - This is true, but it's also exaggerated. A PSU that is 90% efficient at 50% load tends to be 89-89.5% efficient at 20% or 80% load. I've done the math for people before, and specific examples have shown 30-80 years at US average (12 cents per KWH) to save enough on electricity to make up for the extra cost. It's just not worth it.
* **You need XX% headroom due to degradation** - No, you don't. You SHOULD have headroom because, while parts improve in efficiency between generations, we have seen parts also raise power draw. Ryzen was more efficient than Kaby Lake, but it still drew more power for that extra performance. The GeForce 700 series (Kepler rebadge) was more efficient than the 600 series, but drew more power for higher performance. Also, while you have a 1070 now, maybe you get an 1180/1280/2080 as your next GPU? That headroom is good for THOSE reasons. But no, your PSU won't degrade 20-40% over 5 years. Quality PSUs are meant to sustain their RATED capacity for the length of the warranty period. It's why the EVGA G-series PSUs have been tested at ~10% over capacity. At 7-10 years in, when their warranty has expired, they would have degraded enough that they can still handle that rated capacity.
* **Overclocking GREATLY increases power draw!!!** - In some cases, yes. In others, no. Nvidia's GPUs, for example, have strict board power limits. They also limit how much you can raise the power limit via software. The reference GTX 1070 has a board power limit of 145W. The highest end EVGA model has a 215W board power limit. Most allow UP TO 20% over the power limit, or ~260W max from the grossest OC you can think of on that 215W model (assuming it even allows +20%). That's 1080ti territory and you won't be hitting that on a 1070. As for your 8600k, I've checked a pair of reliable reviews ([Tom's] (http://www.tomshardware.com/reviews/intel-coffee-lake-core-i5-8600k-cpu,5264-11.html) |  [TPU] (https://www.techpowerup.com/reviews/Intel/Core_i5_8600K/16.html)), and the power draw isn't anywhere near as bad as people think. Tom's got <80W at 4.1ghz, and ~103W at 5.0ghz. TPU got 257W...but that was TOTAL SYSTEM DRAW while gaming. A GTX 1080 was used. Yup, 257W, at stock, with a GPU more powerful than yours.

Bottom line:

550W is fine, go for quality (brand, make, efficiency, warranty) over pure wattage numbers. My recommendations are great, but not the be-all-end-all. There are plenty of other quality recommendations out there too.
They are 100% the same. The "R" denotes "retail." It's the model you'll find at most brick and mortar places, like Best Buy.

It also allows them to deny price matching because of the difference in model number. Most users have reported success with price matching, but some managers will pull those shenanigans.

Basically, it's used for them to get out of a price match based on point #5 in this TODAY article - https://www.today.com/money/6-ways-retailers-get-out-price-matching-guarantees-1C9939213
To be fair, the word "remaster" isn't used once by Sega in announcing or describing this game. It's the media mis-using the word.

* Port = Game is brought from one platform to another, typically supporting a user interface, controls, and resolution(s) offered by that platform. Example - Shenmue 1 and 2 in its current state.
* Remaster = Game has visual/audio fidelity upgraded to match current platform(s), but the core gameplay is unchanged. Example - Fable Anniversary
* Remake = Game is redone from scratch, likely having gameplay and/or story changes, but is largely based on the source material. Example = Final Fantasy 7 (remake)
Ok, but what does the power outlet look like?

(I'm really fishing here, seriously, amazing work!)
One of my favorite videos on that subject. ~45 minutes, but totally worth the watch.

https://www.youtube.com/watch?v=d-7o9xYp7eE
That was one consideration.

In the end, I swapped out my case fans earlier this week. I'm now able to run the monitor at 100hz, with the GPU at full load and not being loud. Temps hit 76-78° on the GPU with a ~40% fan speed, which is barely audible during gameplay, but EVGA's coolers aren't known for being the best in the business. With the old case fans, it was hitting 81° with 50-55% fan speed, which was VERY loud.

My problem is solved for now, and when the new generation comes out, I'll probably go MSI.
The G series are branded as part of the 2nd gen, hence why they are 2200G/2400G, and not 1200G/1400G.

The full Ryzen 2000 series is considered the 2200G through the 2700X. However, AMD intends to continue selling a few 1000 series SKUs to ensure that every price point is covered. The full product stack, per their leaked slides, is:

* Ryzen 7 2700X | $329
* Ryzen 7 2700 | $299
* Ryzen 5 2600X | $229
* Ryzen 5 2600 | $199
* Ryzen 5 1600 | $189
* Ryzen 5 1500X | $174
* Ryzen 5 2400G | $169
* Ryzen 3 1300X | $129
* Ryzen 3 2200G | $99

EDIT: Revised prices. The leaked slides were higher for some models than the actual pre-order price.
> Welcome to being a fucking niche part of dGPU market, really.

It's a growing market.

> Not really. As long as I get good perf/$, I don't really give a fuck.

No one cares what you think. You don't represent the majority.

> Probably no.

Yea, call me a liar. Gotta move to insults once you realized you were wrong :)

You won't outright admit to being wrong. You clearly aren't capable of that. But you inadvertently admitted to knowing you're wrong, so thank you for that.
> Who cares?

You'd be surprised. I have a mini-ITX case. That thing fills up with heat, everything throttles. I actually had been running my AW3418DW at 60hz with V-Sync on just to limit my GPU, dial back the temps and fan speed to reduce noise. And that's a ~120W GPU. I would never put a ~225W GPU in there.

That means, current gen, I could use a any GTX 1060, or at best, an underclocked RX 470/570.

Power draw matters. Small differences are ok, but if your $200-$300 GPU draws as much as their $700 GPU, you've got a problem.

> No one cared about it when NV produced #1 firestarters in the world.

I sure as hell did. Long time Nvidia user, but I switched to the HD6850 over Thermi.
I have no problems getting through the day, but I'm a VERY light user. I still rotate between my Style and Moto 360 v1. Despite being 2.5 years old with frequent use, the 360 has MUCH better battery life. And this is a watch that was also panned for mediocre battery life relative to its peers.
>  The 550 to 580 matches their competitors at MSRP. 

I evaluate cards on the three P's - Price, Performance, and Power Draw.

The GTX 1060 vs. RX 480/580 gave us the clearest indication of Pascal vs. Polaris, and it wasn't good. At launch, the [GTX 1060 was ~10% faster than the RX 480] (https://www.techpowerup.com/reviews/NVIDIA/GeForce_GTX_1060/26.html) |  [while drawing 47W less during gaming] (https://www.techpowerup.com/reviews/NVIDIA/GeForce_GTX_1060/24.html) (116W vs. 163W). While they were equal on Price, the 1060 won in Performance and Power Draw. Polaris was not Pascal's equal.

The common theme is that the RX 480 gained more performance via drivers. So did the 1060, to a lesser degree. By the time that the RX 580 launched, the [580 > 1060 > 480 in performance] (https://www.techpowerup.com/reviews/Gigabyte/AORUS_RX_580_XTR/29.html) | [but power draw had taken a disgusting turn for the worse (116W vs. 198W)] (https://www.techpowerup.com/reviews/Gigabyte/AORUS_RX_580_XTR/27.html). At that point, however, the 580 was competitive - winning on Performance, losing on Power Draw, tie on Price. But that was nearly 9 months after the 480 launch, and nearly 8 months after the 1060 launch.

As is typically the case, AMD is late. And that lack of efficiency (again, 116W vs. 198W!!!) shows that Polaris isn't on the same level as Pascal. The Aorus RX 580 in that review, at 227W, drew 4W less than a GTX 1080ti. That is nuts!
And you're further illustrating my point, which is that everyone applies their own criteria.

I'm going to simply fall back on Nvidia's criteria. The GeForce gaming page lists 1050 through Titan Xp, with the 1070ti being the exact mid-point. The Titan V is a data center product per Nvidia.

>  I wanted to throw in, because it is used for gaming by some people, it's a hybrid product, but it is definitively the best gaming gpu.

I'll bet you could game on a DGX system, and it would beat a single Titan V. If someone has the money to blow, they could add that in as well. That was my point, that your criteria, my criteria, and someone else's criteria are all equally valid. While I, again, prefer to go with Nvidia's placement, you could have a person considering the Xbox One S, Xbox One X, and a GTX 1080. In their mind, among their choices, the 1080 is the high-end option. They wouldn't be wrong, and I'd welcome them to the community :)

And FTR, the downvote/brigading on your posts is absurd.
> Thanks for putting all the options out there, but you missed the most important one.

I missed many. My post was long enough and I was only adding a few as examples. It wasn't supposed to be comprehensive.

> Generally speaking, especially for retailers, the criteria for midrange is the most popular, best selling models, which happen to be the ones roughly in the $200-$300 ish range.

That's one criteria. I don't disagree. My point was that different people will apply different criteria, and their criteria is no less valid than yours.

> So no, 1070 is not midrange, 580 certainly is.

Keep in mind that we're in an odd cycle. I bought a GTX 970 in late 2015, nearly a year before Pascal came out. I paid $289. Prices for Maxwell and Radeon equivalents were in the typical before-new-product price depression, something that we didn't get to experience here.

In a normal product cycle, the 1070 would be ~$350 or less by now, definitely closing in on $300.

For the record, Nvidia's GeForce gaming product page lists the GTX 1070ti as the middle of their gaming stack, for what it's worth.
Well, don't stop there! If you add in all the DGX systems, the 1080 is pure peasantry.

EDIT: For the record, Nvidia's own gaming products page goes from the GTX 1050 through the Titan Xp. And it lists the 1060 and 1050 as singular product lines (whereas the 1070 and 1070ti are broken out as separate):


* Titan Xp
* GTX 1080ti
* GTX 1080
* GTX 1070ti
* GTX 1070
* GTX 1060
* GTX 1050

By THAT logic, you're correct. The 1070 ti is the exact middle, 1 up and 1 down being the 1070 and 1080. Those are all fairly close in performance and are the middle of Nvidia's stack. The 1050/1060 are the low-end, while the 1080ti/Titan XP (fairly close in performance and a clear tier over the 1080) are the high-end.

Like I said, it depends on which criteria that you apply. But Nvidia's most closely matches yours, aside from the desire to lump non-gaming data-center hardware in :)
Downloading now. Haven't played it in years.
* 1280x720 = HD (High-Definition)
* 1920x1080 = FHD (Full HD, also sometimes called 2k)
* 2560x1440 = QHD (Quad HD, as it is 4x 720p, also sometimes incorrectly called 2k)
* 3840x2160 = UHD (Ultra HD, sometimes called 4k)

And then you have your common ultrawide resolutions:

* 2560x1080 = WFHD (Wide FHD, wider 1080p)
* 3440x1440 = WQHD (Wide QHD, wider 1440p)
I know, I'm just saying that Dell is unlikely to offer advanced exchange and pay shipping both ways like they do in the US. So, the advantage is gone, as you'll likely get a similar RMA experience from both.

I'm saying that they are now on par in that area.
> You could tell us which company, product, and the payout despite the NDA

No, I cannot.

> NDAs just like warranty void stickers are meant to scare those who are ignorant of the legalities that exist behind them.

No, they are not.
* Open NVCP
* Display --> Change Resolution
* Scroll down, it's under 3. Apply the following settings, listed as Output Color Depth

It doesn't always work correctly. For example, my 1060 on my AW3418DW properly detects 8-bit output. However, my wife's GTX 950 detects both 8- and 10-bit on her Apple Cinema Display, which has an 8-bit only LG IPS panel.

Nvidia's consumer GPUs struggle to properly detect and support it, YMMV. If you want proper support for 10-bit, you'd need an Nvidia Quatro, or an AMD Radeon.
Your situation negates the RMA advantage. So it's potentially better build quality (Alienware), vs. potentially better color (FRC), optional speakers, and the ability to adjust gamma in the OSD.

But those are small differences, so I'll just repeat my last line:

> Realistically though it's so close, get whichever is cheaper and/or satifies your preference for aesthetics.
The Alienware can have the same scanlines. They are VERY hard to see, and they go away when you turn off G-Sync and the overclock. This is a common thing in all G-Sync monitors, but it's so hard to see, most don't notice it. Mine disappears (or is just INSANELY hard to see) at 100hz.

Just Google "G-Sync scan lines" and you'll see reports of this issue on every major G-Sync monitor.
Advantages for the Alienware:

* better RMA policy (as you stated, largely if not entirely negated by you shipping to outside the US)
* typically better build quality than Acer's equivalents (less edge bleed), but trust me it's not Ultrasharp quality

Advantages for the Acer:

* FRC to simulate 10-bit (Nvidia's consumer GPUs are hit and miss when it comes to properly supporting this, and you don't get the benefit if you don't select 10-bit output in the NVCP)
* Built-in speakers (for those who want it)
* Gamma adjust (a major oversight for Dell)

I prefer the Alienware, but for you, the Acer may be the better buy. Realistically though it's so close, get whichever is cheaper and/or satifies your preference for aesthetics.
> GP102, GP100, GV100 all are higher end. If a 1080 is high end, what does that make those?

Most of the time when someone is talking about low/mid/high, they're talking about available options. Comparing the GeForce 10-series to a $1,200 Titan is understandable/debatable. Comparing it to the $3,000 Titan V is a bit much. When you expand the criteria that far out, anything can be made to appear less than high-end.

Like your new i7-8700k? Yes, it's the highest end gaming chip out today, but let's be honest, it's low-end. A simple super computer blows it out of the water. [And yes, it can probably play Crysis.] (https://techreport.com/news/30963/cray-xc50-supercomputer-can-probably-run-crysis)

When most gamers look at the GTX 10-series, they see the 1050 through the 1080ti. And if you count every card in that series and had to draw a circle around the one in the middle, it wouldn't be the 1080.

That said, as I've said before, the criteria changes the position. Do we include or exclude Titan-class products? If we include them, up to what budget? Titan Xp, or all the way up to V? You CAN game on it, and it IS faster, so it COULD be counted.

It's very difficult, again, to pin down a low/medium/high-end in a product stack with clearly more than 3 performance tiers. No one is arguing that the 1080 is the fastest GPU (I would hope), but it is definitely not THE mid-point in the gaming segment when you look at the current gaming oriented lineup.

Calling a GTX 1080 mid-range is like calling a Core i7 mid-range just because Intel makes other CPUs that offer higher core-counts.

So I understand where you are coming from, but I respectfully disagree.
It depends on how you look at it, and no one ever agrees on this stuff anyway. Let me give you a few ideas of the criteria we could use here.

Let's look at the current GeForce GTX lineup (no Titans, no GT 1030).

* GTX 1080ti
* GTX 1080
* GTX 1070ti
* GTX 1070
* GTX 1060 6GB
* GTX 1060 3GB
* GTX 1050ti
* GTX 1050

How do we label 8 products as high-end, mid-range, and low-end? We could combine the 2 1060s as their performance delta is < 10%. That would put the 1070 as the mid-point in the product stack. But using that same logic, the 1070ti should be absorbed by the 1080. Using that logic leads us to the chips themselves:

* GP102 (1080ti, Titan products)
* GP104 (1070/1070ti/1080)
* GP106 (1060 series)
* GP107 (1050 series)
* GP108 (GT 1030)

By this logic, GP 106 (1060 series) is clearly the mid-range part. But many would argue to remove GP108. Now you have 4 tiers. GP 107 is clearly the low-end, and GP102 (even without the Titan products) is the high end. That leaves an upper middle (GP104) and a lower middle (GP106). That makes the 1080 the highest of the mid-range, or mid-high.

We could go off purely performance, [LIKE THIS.] (https://tpucdn.com/reviews/NVIDIA/GeForce_GTX_1080_Ti/images/perfrel_2560_1440.png) In this case, the low-end product (GTX 1050) is 22% of the performance of a GTX 1080ti. So, the exact mid-point would be 61%. That's right below and closest to the GTX 1070. So, when categorizing these GTX cards by performance, you'd get:

* 1080ti - 100%
* 1080 - 78%
* 1070ti - 71% (not listed, number estimated halfway between 1070 and 1080)
* 1070 - 64%
* 1060 6GB - 47%
* 1060 3GB - 42% (not listed, assumed 90% of 6GB model's performance)
* 1050ti - 28%
* 1050 - 22%

We're seeing 4 distinct performance tiers there, and no surprise, they line up with the chips we looked at before. But if we ignore the chip and just go off the numbers, the 1080ti is clearly high-end, the 1070 was the closest we had to a mid-point, and the 1050 was the low-end. Where does everything else fall? Well, the 1080 is closer to 61% than 100%, so it's more mid-range than high-end based on performance. But this gets absurd, because that means everything from the 1060 3GB through the 1080 is "mid-range."

Ok, what about price (try not to cringe, these are the official MSRPs we never see):

* GTX 1080ti - $699
* GTX 1080 - $499
* GTX 1070ti - $449 (?)
* GTX 1070 - $379
* GTX 1060 6GB - $249
* GTX 1060 3GB - $199
* GTX 1050ti - $139
* GTX 1050 - $99

If $700 is the high and $100 is the low, then $400 is the mid-point. Again, the GTX 1070 is the closest. And the 1080 is far closer to the 1070 than the 1080ti. So if we're using 3 tiers, it gets lumped in. This time, the tiers are pretty clear, with GP 102 standing alone at the top, GP104 in the middle, and GP106/107 as the budget options.

I could go on, but there's no point, so let's wrap this up.

**TLDR/Conclusion: A GTX 1080 is high-end or mid-range based on your criteria. Your criteria are different than someone else's criteria. Your criteria are equally correct. Therefore, you're not going to convince the other person that you are more correct, and vice versa.**

Low/Mid/High works when there are 3 defined products in a stack. This rarely happens, so low/mid/high rarely applies to GPUs these days.
And so was the 480, and the 380, etc.

They're saying, based on this rumor/speculation, that Navi should offer Vega 64/GTX 1080 performance. That should be right in the ballpark of the GTX 1160/2060 (I expect it to be between the 1070 and 1080, closer to the 1070).

The RX 580 is this generation's mid-range. They're speculating that Navi will be mid-range for next generation.
> The existing mid-range (Polaris 10 & 11) will be 4 years old in 2019

Just a reminder, these launched mid 2016. They'll be 3 years old in mid-2019.
This sounds so much like VBMS...
The transitions show it's a crop from the same footage. It's only applicable if FC5's UW support is HOR+. If FC5 even slightly alters to FOV in 21:9, then this footage becomes useless.
> How do you feel about the curve btw?

Threw me off at first. It's a 1900R curve, and the last curved monitor I used was 3800R. It's based on the radius if you made a circle out of the monitors, meaning that a lower number = steeper curve.

But I've been using it for about 6 weeks, and I briefly forgot about the curve until you asked about it. So I'd say that I got used to it.

If you're patient, LG is releasing their monitor with the same panel as the AW3418DW and Acer X34p. I don't think they'll match Dell on the warranty (LG is typically 1-year these days), but they'll be the first to offer factory calibration out of the box with this panel (not confirmed, but they've done it with all of their other adaptive sync panels so far). Speculated release is July/August and in the same neighborhood price-wise as these two. For the record, the Acer's MSRP is $1,099, and the Dell's is officially $1,499, but has dropped unofficially to $1,149. The LG is expected to be in the $999-$1,199 range at launch.

Basically if factory calibration matters, wait for the LG and compare. If it doesn't, go with the Alienware or Acer today (I'd recommend the Alienware, but the Acer does have gamma controls).
> It's a bit slower than a 970.

I'd put them on par. Looking at the resolutions tested (GTX 1060 for reference):

Resolution|GTX 1060 6GB|GTX 780ti|GTX 970
:--|:-:|:-:|:-:
1600x900|100%|83%|86%
1920x1080|100%|85%|86%
2560x1440|100%|86%|85%
3840x2160|100%|83%|85%

Given that 1080p/1440p are the most commonly used among the tested resolutions, they're within margin of error of each other.
> Screen: Matte finish

Almost every monitor today has a matte finish.
Given your budget, I'll just make the point blank best recommendations for your stated needs.

##ViewSonic XG2703-GS

27", 1440p, 144hz (165hz OC supported), G-Sync, AHVA panel (IPS-like). Of all the monitors offering this spec sheet, the ViewSonic is known to have the best average build quality. You can still get a dud, but the odds are more in your favor than with the other brands.

The main flaw is that these AHVA panels, while being similar to LG IPS and Samsung PLS, tend to have a higher rate of defects (worse panel glow, higher rate of dead/stuck pixels).

##Alienware AW3418DW

This one is a 34" curved Ultrawide, 3440x1440, 100hz (120hz OC supported), G-Sync, with an LG IPS panel. Acer is selling a model with the same panel (X34p), but they typically have worse average build quality than Alienware.

The main flaw here is two-fold. First, the panel has an issue with color temperature where one side can look slightly warmer than the other if you have an all-white background, you sway left to right, and you look for it. It's not an issue in gaming, but it is there. Second, the Alienware lacks gamma controls in the OSD, so if you get a wonky out of box gamma, you can'd fix it via the monitor's controls, a common yet stupid oversight for Dell. For reference, TFTCentral's model was 2.6, Lim's Cave was 2.4, and mine was 2.46 (2.2 is ideal). Most users won't notice or care, but more discerning users will.

You an get the AW3418DW for $999 on sale pretty much every other week. I got mine through Dell direct and had them price match to $999.
> Suing the companies for it would cost a single customer to much so they get away with it.

This is a gross myth, but unfortunately, so many people believe it that they allow these companies to walk all over them.

The Magnuson-Moss act has a few quirks to is that make it a beautiful law for the consumer, such as:

* no binding arbitration
* the burden of proof is on the company to prove that the damage was caused by the consumer and not a defect subject to warranty

In 2014 I filed in small claims court for a product that failed under warranty and the company refused. The filing fee was $29. I had no lawyer. Our jurisdiction doesn't allow for lawyers in small claims court. My claim was for:

* the full price paid for the product, to include tax and shipping
* the filing fee
* my hourly wage * 8 hours, to be applied to each day in court (small claims in my area is typically 1-2 days)
* treble damages due to their actions being willful (all of the above, times 3)

In the end, we never made it to a court date because the company settled in my favor. Due to the NDA that came with the settlement, I can't give the name of the company, the product, or the specifics of the settlement. But the post I've written so far outlines the process for small claims court in my jurisdiction, and with some variance most mimic this to a fair extent.

**TLDR: You generally do not need a lawyer and you won't spend a lot of money for a consumer electronics part in small claims. Cars, given their higher value, are a different matter.**
The law has been in place since the 70s. This is just a 'reminder.'


The law has been in place since the 70s. This is just a 'reminder.'
> Porsche of North America, for example, has an entire section in their warranty stating that by buying the car you agree to settle aftermarket part warranty disputes through Porsche's arbitration program before seeking remediation through Magnuson Moss.

Which is also illegal.

* [FTC Continues To Ban Mandatory Arbitration in Magnuson-Moss Warranty Claims] (https://www.ballardspahr.com/alertspublications/legalalerts/2015-06-03-ftc-continues-to-ban-mandatory-arbitration-in-magnuson-moss-warranty-claims.aspx)
* [FTC Affirms Consumers’ Right To Go To Court Over Warranty Disputes] (https://consumerist.com/2015/05/26/ftc-affirms-consumers-right-to-go-to-court-over-warranty-disputes/)

> t's legal though it has been defeated in court (by people who can afford to fight it).

It's been defeated in court precisely because it is NOT legal.
It's likely to be part-for-part similar to the XG2702. But ViewSonic tends to have a better reputation than Asus when it comes to build quality in their monitors.

You could also consider the [MSI Optix G27C2] (https://www.scorptec.com.au/product/Monitors/25plus-inch/70987-OPTIX-G27C2) through that retailer. It's basically a clone of the Samsung.
If you want 1080p 27" and 144hz, look into the [Samsung LC27FG73] (http://www.samsung.com/au/monitors/c27fg73/) or the [ViewSonic XG2702] (https://www.viewsonic.com/uk/products/lcd/XG2702.php).

The ViewSonic will most likely fit into your budget. It's 27", 1080p, 144hz, supports AMD FreeSync, and uses a TN panel. Primary weakness is color accuracy and viewing angles, typical of TN panels in this segment. If you can't find the XG2702, last year's XG2701 should still be available, and the differences are negligible.

The Samsung appears to be $600 in your neck of the woods. But that's MSRP. You may be able to find it for less if you shop around. If you can find it under $500, the main difference is that it uses a VA panel, resulting in better viewing angles, color, and contrast, at the expense of some slower pixel transitions (ghosting).
There's no such thing as a monitor with a real world response time of 1ms. It's a falsely advertised and misleading spec. And many consumers tend to confuse response time and input lag, so telling us "1ms" doesn't clearly tell us what you want in a monitor.

Please provide the following information:

* preferred budget (amount you'd want to spend)
* stretch budget (amount you're willing to spend)
* preferred resolution (1080p, 1440p)
* Do you care about G-Sync/Freesync?
We were discussing power draw. The implication was that the 290 drew significantly more power than the 780. I was simply showing that it did not.

You're discussing something else. This is known as "moving the goalposts".
/u/DongleNocker

> the R9 290 was a powerful GPU, and has aged better than the nvidia counter parts... But it uses too much dang power.

-----

/u/HubbaMaBubba

> The 780 and 780ti also used a lot of power.

I think a lot of people overlook the power draw of the 780/780ti despite being very close to the 290/290x. It goes back to the prior post about people being easier on Nvidia when the products are otherwise comparable.

For example, [per this review from TPU] (https://www.techpowerup.com/reviews/NVIDIA/GeForce_GTX_780_Ti/25.html), the power draw during gaming is:

* GTX 780 = 199W
* R9 290 = 214W
* GTX 780ti = 229W
* R9 290x = 236W (246W in quiet mode)

When you go to the performance summary page, the 290 offers 1% more performance than the 780 for 15W more power (~7.5%), making them nearly equals in power efficiency.

The 290x doesn't fare so well, drawing slightly more power than the 780ti, but being a little slower as well. That said, the power draw differences between these cards aren't what many make it out to be.

Please note that this doesn't take into account modern performance gains. I think AMD comes out a little better in that regard.
I purchased that monitor shortly after release. It had two majors flaws that caused me to return it.

* Didn't support 2560x1440 for 16:9 legacy games, forcing them to 1080p (a blurry mess). I've heard that subsequent revisions fixed this.
* FreeSync range was a horrific 55-75hz, and I got one of the lower quality ones where altering the range via CRU led to extreme flickering.

It was $999 at Costco shortly after launch, and I wasn't willing to pay that price given the above flaws. If it had a wider range and 16:9 1440p support, I'd still be using that and an RX 480 instead of my current setup.
I feel for anyone with such a problem. And I don't doubt that there are some bad employers out there who would do this.

But they are in the minority. Most of these problems would stem from the individual not acting like an adult. "Hey Mr./Mrs. Manager, I have a court date in 6 weeks. Could I get the day off, or switch a shift with someone?"

If they wait until the last minute to worry about accounting for the day on the calendar, then they've brought it on themselves. You will know the court date a few weeks in advance, at minimum.
Nope.

https://www.ballardspahr.com/alertspublications/legalalerts/2015-06-03-ftc-continues-to-ban-mandatory-arbitration-in-magnuson-moss-warranty-claims.aspx
Magnuson-Moss act strictly forbids forced arbitration for warranty claims.

God bless the Magnuson-Moss act :)

https://www.ballardspahr.com/alertspublications/legalalerts/2015-06-03-ftc-continues-to-ban-mandatory-arbitration-in-magnuson-moss-warranty-claims.aspx
> Unfortunately **most** employers will see this as lost time and

No. MOST employers will allow you a day off, if planned in advance, if they or you arrange for someone else to take the shift. Are there bad employers? Yes. But not **most.**

You'll get your court date reasonably well in advance.
If you have a program or multiple programs that you know for a fact take proper advantage of specific wider gamuts, yes, I'd recommend buying a proper display that has support for that specific gamut.

But using a wide gamut monitor with standard gamut applications will yield wildly inaccurate results.
I can meet you half way on that one.

If the problem is lack of paid time off work, you can claim your lost wages for each court date. When I filed mine, I claimed:

* cost of product
* filing fee
* loss of wages (8 hours * hourly wage, with a note that it would be this amount for each day in court)
* treble damages because their act was intentional (all of the above, tripled)

However, if you merely can't get the day off of work, paid or unpaid, that's understandable.
Nope.

Small claims court.

Cheap. Fast. In many jurisdictions lawyers are forbidden. I've used this once for a similar situation. Company decided it was cheaper to settle than fly in someone from CA. Standard NDA though, so I can't disclose the company, product, terms, etc. But I got paid.

And the fee was $29 to file.
> You mean like Z170 and Z270?

Released to coincide with the 6th and 7th gen series. Your other examples were also released to coincide with their own series.

But it appears that Z370 an Z390 will both be Z3-series, and both likely for 8th gen core series. If this comes to pass, it will be fairly unique.

To apply this to your example, it would be like Intel releasing Z170 and then Z190 for the 6th gen. That didn't happen.

> You must be new here

If you're going to insult someone for seeming ill informed, might want to make sure that you actually understand the topic yourself.
> Televisions with HDMI VRR / FreeSync support are useful for more than just the Xbox.

Please take a look at the thread title. We were discussing the Xbox. I think you're veering off topic.

> I believe there was mention of this pre-release but it's likely wrong or intentionally misleading.

Per Mike Ybarra, Corporate VP - Gaming at MS, the Xbox One X has the hardware for HDMI 2.1 - [SOURCE] (https://twitter.com/xboxqwik/status/902562860370280448)

> Is that not essentially what G-Sync does, with LFC being handled inside the display rather than by the video driver?

Unfortunately, no. The GPU the driver, and the display all coordinate to keep the framerate output and refresh rate in sync, so long as framrate output is within the adaptive sync range. What you are describing is the display ignoring the video output (Xbox at 60hz, display at 120hz doing its own thing). This doesn't currently exist.

> LCD panels can't hold an image that long. Refreshing natively at 23Hz causes them to flicker, so no LCD with a "24p" mode is actually refreshing at 23/24Hz.

I never said that it was. While the Xbox has support for "24hz" listed, this is actually just a typical 24p mode, that requires that the connected display have a means for supporting it (typically 120hz panel refreshed 5 times per frame).
> I was talking about displays.

Once the display a has support for it, the Xbox has to be updated to support it, otherwise, it won't happen. Your talk about displays has to be inclusive of the Xbox as well.

> The current Xbox doesn't have HDMI 2.1

The Xbox One X has HDMI 2.1.

> The Xbox cannot output >60Hz, but that doesn't prevent the TV from accepting the signal and doubling or tripling the input itself.

Actually, it does. For variable refresh rate to work, all parts of the chain have to be participating. If the Xbox outputs at 60hz, the TV sees itself as 60hz.

What you are describing has never been done before in a monitor or television set. If you are so sure this can be done, you'd best patent it and get rich while you can.

> HDMI 2.1's seamless mode switching feature likely already does this rather than require the display to natively refresh all the way down to 23.976Hz.

It's called 24p mode, and it happens because all parts of the chain agree to run this way. It's not automatically done by the TV.

> To double the input refresh rate would not require hardware equivalent to the FPGA board that NVIDIA use in the G-Sync module.

Correct, but it would require a v-blank implementation. One that doesn't currently exist. All current implementations (G-Sync, FreeSync, DisplayPort Adaptive Sync, HDMI VRR, and the variant found on laptops, as well as others I'm probably forgetting) require that all parts of the chain be an active participant.

What you are describing is the display acting on its own volition to take over VRR. That would be awesome because it would also prevent bad actors from locking you in to a specific protocol. Unfortunately, it doesn't currently existing in shipping or announced product.
Reading up the T&C on it, the answer appears to be no. The 90-day limited warranty makes no mention of premium-panel benefits. The premium panel warranty implies that it's for new purchases only that comes with the 3-year warranty, or the extended 4/5 year warranties.
> Dell's warranty is ridiculously good. I wouldn't worry about it being used.

Dell's warranty on new is outstanding. Their warranty on certified refurbished is not so good, typically 90 days from date of purchase.

Now, if you get a refurb as replacement for your new purchase, you still have 3 years from the date of original purchase. The 90-day warranty is for those who originally bought a refurb in lieu of new.
I don't really worry about the downvotes. One or two of them from petty individuals is quickly offset by one quality comment.

Because I focus on providing helpful information in an easy to understand manner that is often backed up by sources, karma isn't a concern.

Plus, you can't spend it on anything.
I would hope that they would make that connection. But so far, they haven't commented on it.


All I'm saying, and this is what certain individuals are failing to comprehend, is that:

* The current Xbox One hardware, even with the preview update, is limited to 24, 50, or 60hz.
* Due to this, in order for LFC to activate, minimum range has to be 30hz or lower (when 60hz is used).
* While it's possible for Microsoft to change this in the future, they have not yet commented on doing so.
Just keep in mind that they guarantee 100hz. The 120hz overclock is a YMMV thing. If they could guarantee 120hz, every model would be sold with 120hz as the guaranteed refresh rate.

I would certainly attempt an RMA for this, but so long as 100hz works problem free, they can at any time decline to honor an RMA for this reason.

I know what I just posted is an unpopular opinion and will be downvoted to hell, but it's what you need to hear.
> This should all be solved next year with HDMI 2.1, as 60–120Hz should be easy for them to cover.

If Microsoft allows the Xbox to support refresh rates higher than 60hz. As of today with the current preview, it supports 60hz, 50hz, and 24hz. So even on a true 120hz TV (most '120hz' TVs are actually 60hz), the Xbox still requests 60hz as the refresh rate (unless 50hz or 24hz are selected and supported).

> Even if the minimum refresh rate the panel supports is something like 48Hz, they could theoretically still support LFC at 60Hz

Doesn't work that way.

LFC (and Nvidia's unbranded alternative) act by doubling the refresh rate within the supported range. So, if you have a range of 48-120hz, and you drop to 47fps, FreeSync/G-Sync counter this by running at 94hz to display that framerate. But because the Xbox runs at 60hz, this is an unsupported refresh rate.

> The TV could report that its supported range is 24–60Hz but convert that to 60–120Hz.

If you invent a TV that does this, one that circumvents the output device's refresh rate to force adaptive refresh, you'd be the first. It would also be able to circumvent Nvidia's lockout of Displayport Adaptive Sync because, as you're stating, the monitor would be controlling it, not the connected device.
You get that a lot around here. Really separates the children from the adults.
https://www.techradar.com/news/huawei-watch-3-launch-confirmed-but-it-wont-be-anytime-soon
Check with the retailer. They may do an after the fact price match. If they refuse, just say you'd like to take advantage of their amazing return policy and buy another one. They'll often cave when they realize that it's cheaper to do so (not always, as most low-wage CSRs really aren't too invested in the company's financial health).

Also, check with your credit card issuer. Many of them offer a form of purchase price protection, which will issue you a rebate equal to the discount if requested within a certain amount of time.
> The benefits of FreeSync/G-Sync require framerates to exceed the native refresh rate of the display.

I don't know where you got this, or maybe you struggled to convey your point.

If your frame rate exceeds your refresh rate, FeeeSync/G-Sync turn off. The benefit to FreeSync/G-Sync is that, so long as you are within the declared range, you will experience less stutter than V-Sync On, and no tearing caused by V-Sync off.

For a console, 30-60fps gains the most benefit. But, even games that run at 30fps with occasional drops will benefit from it.
Sarah Silverman is hot.
> I mean I’ll have a discussion,

Agreed. So long as it's civil. We rarely learn anything in an argument.
> The old testament only prohibits man on man, not girl on girl. Therefore, god must approve of these two as much as the rest of us.

I love loopholes!
> 1 Corinthians 6 says men who practice homosexuality will not inherit the kingdom of heaven, so yes the New Testament does say homosexuality is a sin

1 Corinthians 9, actually, but you are correct. But most of the people who are against homosexuality wouldn't like what the rest of that verse (or 10, or 11) has to say :)

> but I don’t think you want to get into a theological or religious debate on the Eagles sub

I'm ok with one so long as it's civil. But it is a bit hypocritical to make that statement AFTER you attempted to launch a debate :)
The most hilarious part to me is the "Obey Jesus." The Holy Bible doesn't present Jesus as saying ANYTHING about homosexuality.

In fact, only the Old Testament comments on the subject. And Jesus died for our sins, obsoleting the old covenants. Anyone who says that Christians have to be against homosexuals is literally saying, "I deny Jesus and everything that he stood for."
> huawei, Verizon, and Asus dropped out.

I don't think Huawei dropped out, given that they're the only ones with a watch on the Android P developer preview. Yes, it's provided by Google, but it's a strong indication of current/future support.

They've also openly discussed their plans for the Huawei Watch 3.
You explained it far easier than I could. Yes, you've got it :)
> Does that still happen when you lower the upper limit?

I tried 40-60 and it flickered. Then 44-60, 48-60, 52-60, and it got laughable. Only 55-60 worked.

> You'd think that a 48hz-144hz range monitor could do 30-60 no problem.

This is where things get overly technical, but I will try my best to explain it. Forgive me if this is a bit confusing (and ask questions if what I post makes no sense or needs clarification).

When we see a range of x-y hertz, that's NOT actually how adaptive sync works. That's just how it is conveyed to us so that we, the end user, can understand it. It actually works based on intervals.

A 60hz monitor refreshes one frame every 16.67ms, and it takes 16.67ms to do so. A 144hz monitor does the same task in 6.94ms. And adaptive sync monitor has to not only perform the same task, but be prepared to adjust the pacing on the fly.

A monitor with a range of 40-60hz has to be able to adjust between 16.67ms to 25ms on the fly. A 48-144hz monitor has to adjust between 6.94ms to  20.83ms.

When we lower the minimum range, some take this as being equivalent to an underclock. "It's a lower range, how could it possibly be harder?" It's actually a form of overclock. As you go lower with the minimum, the effective range constricts. So, a 48-75hz monitor like the Dell SE2717H (27mhz range) MIGHT be able to do 40-60hz (most should via CRU). But there's no guarantee.

So while a display with a range of 48-144hz should, in theory, be able to hit 30-60hz, MS would have to test every such monitor before adding in such a feature to the Xbox.

But because MS hasn't even discussed doing such a thing, and AMD doesn't allow it in their own driver, this is a far fetched wish at this point. Would certainly be cool if they did it though!
> CRU can shift the freesync range, MS could do something like that couldn't they?

Correct, but this is a feature that causes the monitor to run out of spec, with varying results. For example, when I tested the LG 34UC98, which has a default range of 55-75hz, going down to even 52 caused massive flickering on my unit. But, others were going as low as 35hz without issue.

Due to this, it's extremely unlikely that MS would run something that forces the monitor to run out of spec.

> They could colab with display makers to make custom profiles or something.

I think what we're going to see is FreeSync TVs go for the 24/30-60hz range, while monitors continue to do 48-144 (or similar), with the occasional exception (Nixeus EDG 27 being the current best FreeSync implementation we've seen).
> You theorized. 

I've explained how FreeSync works. I've explained how the Xbox works with FreeSync. I'm also a bit of a subject matter expert on this, having authored the last guide on this subject in this very subreddit, which took input from AMD's own Robert Hallock, as viewed here - https://www.reddit.com/r/hardware/comments/666i4e/gsync_and_freesync_a_primer_on_similarities_and/

There is no debate. I'm correct. Get over it.
Based on their prior models, they're using the same models. The 850 takes the same great monitors and adds:

* USB 3.0 hub (2x ports, barely a hub)
* USB-C
* Speakers

If you expect to use those features, then it's worth the upgrade cost. If you don't expect to use those features, then it's moot. Go with the cheaper option.
There are none yet, but Samsung has announced some. PC monitors are currently the only way to go for now.
That was me that you disagreed with, but there was no theory. I was making a factual statement that you got upset with.

Even with this update, the Xbox One is still a 60hz device. Due to this, the display needs to have a 30-60hz range for LFC to enable automatically.

A 48-144hz range becomes 48-60hz with the Xbox, and LFC doesn't activate.

I'd love for MS to enable high refresh support just to make Freesync that much better. But, it's not in this update, nor has MS even discussed doing it.

Again, not a theory. It's confirmed.

**EDIT: Further clarification.**

The poster above gets my argument wrong. Not sure if that's a misunderstanding on his part, or deliberate misrepresentation. So to further clarify, I NEVER said that the Xbox doesn't support LFC. I only stated that because the Xbox runs the display in 60hz mode, the display needs to have a minimum supported Freesync range of 30hz or lower for LFC to enable. That is all.

I expect the upcoming FreeSync TVs by Samsung to support this. But finding a FreeSync over HDMI monitor with a 30-hz minimum is very difficult right now. The best monitors, IMO, are LG's 4k/60 lineup, which have a 40-60hz range and support overdrive with FreeSync enabled.
> Such as the LG 27UK650-W?

I haven't used it, but I have owned one of its predecessors, the 27UD68-P. Overdrive and FreeSync did work simultaneously, but it did have some minor image retention issues. This was most noticeable on gray backgrounds (such as when you shut down Windows). Ghosting was tolerable from my point of view, but that's subjective.
Generally, most monitors have an adequate response time for a 60hz display. However, some of LG's latest IPS 4k monitors have some kind or weird image persistence issues that looks like a hybrid between poor response time and burn in. And it's only an LG display issue, because those some panels used by Dell, ViewSonic, HP, etc., don't experience this issue.

Also, you might want to disable FreeSync in the short term anyway. Unless you're on the preview build, your Xbox doesn't support FreeSync, yet. Just an FYI.
I haven't done it, but it's doable. It wont be exact, but close enough that you won't be able to tell the difference. Viewing angles will have the biggest impact on them looking different.
Correct
This is fairly common, unfortunately. Many monitors with FreeSync either disable overdrive entirely, or lock it to a specific level.

This is because overdrive has to be tuned to the refresh rate to balance between ghosting and inverse ghosting. If you had a 144hz display, for example, and the overdrive was tuned to perfection for 90hz, you'd experience inverse ghosting if your framerate went below that, and regular ghosting if your frame rate went above it. This is because your framerate matches your fresh rate with adaptive sync, but the response time doesn't adjust on the fly.

The ability to adjust response time on the fly is called "adaptive overdrive." It's a little known feature that Nvidia requires for ANY monitor that uses G-Sync. AMD doesn't require it, so as of right now, the Nixeus EDG 27 is the ONLY known Free-Sync certified monitor to support it.
> To put this in perspective, 16.2.1 was the FIRST driver released for the 5xxx/6xxx series since their support was discontinued in 2015.

> They dont support them with occasional drivers, they supported them with exactly 1 driver, and that was mainly so they could be used on windows 10. They have not released anything since then, and released nothing for them prior to this beta driver.

Here's the 16.1 driver for them for Windows 10.

https://support.amd.com/en-us/kb-articles/Pages/AMD-Radeon-Software-Crimson-Edition-16.1-NonGCN-Products-Release-Notes.aspx
> The longest lasting issue with nvidia I can remember was the chrome debacle, and that lasted about 4 months.

My issue was the HDMI range bug. Myself and several others reported it back in 2013 or 2014. Between then and the eventual fix, it had spread to even DisplayPort. And their fix was just a toggle, as the underlying problem still remains.

AMD initially didn't have this problem, but it started showing up in Crimson just before 17.x came out. Basically, anyone who gets an overly washed out screen after a monitor change, GPU change, or driver install should check their color output settings.
> The Windows Update one is newer

That's interesting. If it's done by MS, I think I'd prefer an official driver over a generic. If it's done by AMD, why would they release it on Windows Update but not on their website? (I'm not asking you, just in general)

> I doubt the linked one even works with the newer Windows 10 revisions.

Have you tried it? One user said that they couldn't get it to install, so I suggested trying with DDU. I'm waiting to hear back on that.
Try running Display Driver Uninstaller first. Make sure that you disconnect Ethernet and/or turn off Wifi before you finish it, to prevent Windows from installing a generic driver.
https://support.amd.com/en-us/download/desktop/legacy?product=legacy3&os=Windows%2010%20-%2064
Unless you go to the AMD drivers website. They offer 16.2.1 Beta for Windows 10, released February 2016. This is for the 5000/6000 series.

https://support.amd.com/en-us/download/desktop/legacy?product=legacy3&os=Windows%2010%20-%2064
To be fair, quality of drivers and length of support are two different things. No one with at least two brain cells is going to argue that AMD offers longer term driver support.

But as a person who swapped from an RX 480 to a GTX 1060, I do sort of miss AMD's drivers. They had their quirks, but I loved the UI. Would love to see Nvidia make a similar effort.

But it's not a deal breaker.
To add to this (because it's not incorrect), AMD discontinued regular driver updates for those GPUs. They still supported those GPUs with occasional updates.

The most recent driver supporting the HD 5xxx and 6xxx series is Crimson 16.2.1 Beta, released 2/29/2016.

Still not as good as Nvidia's support, but just wanted to add this little tidbit. I think this was a way to bring the legacy hardware onto the Crimson driver. The first Crimson driver launched after those GPUs had been dropped from official support.
In my case, the windows are on the wall facing the rear of my monitor, and at night I have rear-bias lighting. As a result, there's no glare on my wife's Apple Cinema Display. We bought that in 2009 or 2010 (I forget), and she's hoping that it never dies. She games on it.

If I didn't have small children, and a cat that likes to rum up against my monitor, I'd remove the matte coating in a heartbeat.
> I have to ask why glossy though?

Contrary to popular myth, glossy monitors have better picture quality **IF** you can control the lighting to mitigate glare and reflections.

A glass covering allows light to pass through unimpeded. The benefit to this is the underlying picture is not distorted in any way. The downside, as already mentioned, is glare/reflection.

Matte coatings are meant to diffuse light to reduce glare/reflection. Problem is that they diffuse light both ways, and LCDs are backlit. That means that the image that you are seeing is slightly distorted. That's why these photos are so famous - https://hardforum.com/threads/guide-how-to-remove-the-anti-glare-ag-coating-from-a-dell-u2312hm-lcd.1674033/

To be clear though, those photos are exaggerated. The closer the object, the lesser the distortion. Because the matte coating is placed on the panel, the distortion is minimal. Also, that's one of the heavier coatings out there. Today's matte panels are far better in this regard.

Overall, matte coatings are better for environments with lots of natural lighting, such as most office environments. Businesses buy more standalone monitors than consumers, so it's easier for monitor manufacturers to reach a wider audience by using matte (business AND consumer), rather than use glossy (targets specific consumers).
There are precisely zero glossy G-Sync/FreeSync monitors right now, and I've scoured the world for them.

I'm tempted to remove the matte coating from my AW3418DW, but due to my kids and cat, that would be a bad idea. It exposes the polarizer which is easily damaged.
This doesn't surprise me. At 6'5" and 237lbs, he's a prototypical **NFL** QB. But if a kid was near those measurements in my high-school, he would have been the biggest guy on our team, playing OL and/or DL.
Keep in mind that purely eyeball-based calibration is based on your subjectivity. You can get an image that might be pleasing to you, but accuracy is far from guaranteed. That said, here's a few tools for you.

In Windows 10 (and likely older version of windows), click start and begin typing "calibrate." It will bring up the "Calibrate Display Color" wizard. This is a useful and easy to use tool. It won't get you anywhere near 100% accurate, but for many it's better than nothing.

A second option that is more of an "at your pace" solution than an interactive tool would be [Lagom] (http://www.lagom.nl/lcd-test/). They use test images to help you approximate certain settings.

[Dead Pixel Buddy] (http://www.deadpixelbuddy.com/) is a nifty tool for searching for dead pixels. Just go full-screen with your browser (typically F11), click a sample color, look for dead/stuck pixels, then proceed through the other colors.

But if you want to accurately calibrate your monitor, you'll need a hardware tool. My budget recommendations are:

* Good = Spyder 5 Express (~$120)
* Better = X-Rite ColorMunki (~$180)
* Best = X-Rite i1 Display Pro (~$230)

The Spyder will give you an excellent baseline calibration for those who aren't doing the most color critical work. The ColorMunki will be more accurate, while the i1 Display Pro will offer similar accuracy to the ColorMunki, but with faster processing/calibration.

For the software, use [DisplayCal] (https://displaycal.net) instead of the included software. It's free, open-source, and more powerful.

Also, don't buy a Spyder 5 unit other than the Express. All Spyder 5 units are the same hardware, but with software locks built into the included software to limit features on the cheaper models. DisplayCal doesn't honor these locks, so all Spyder 5 units perform the same with it.
> As you can see here the Armor OC is deemed as the worst AIB card cooler very close to FE

Right. Slightly better than FE. Both are $699. Slightly better is still slightly better. Why  are you struggling to grasp this? Your original claim is that it's worse than the FE. Your own links shows that it is not.

> As you can see here the FE has better temperatures while idle

/facepalm

This is like one of those Old People of Facebook posts. I'm not sure if you'll understand this, but I'll try anyway.

Most dual-fan coolers use a fan-stop feature where the fans don't start spinning until 50-60°. That includes the higher quality coolers. The FE spins all the time. The FE will beat almost any GPU in idle temps due to this.

The fact that you're moving the goalposts so much tells me that you realize how wrong you were, so at this point you're just DESPERATELY arguing ANYTHING in the hopes that you'll be right about SOMETHING.

> BTW you say louder than FE as if it was a good thing lol.

No I didn't. I presented it as a tradeoff. Slightly faster, noticeably lower temps, at the expense of being marginally louder. Come on, TRY to keep up.

> And 2-3% more performance at HIGHER TDP?

Yup, like pretty much every other Pascal GPU. The FE is at peak efficiency, and the aftermarket cards tend to throw efficiency out the window for higher base and boost clocks. Here's a few examples, but it's pretty much every aftermarket card except those based on the reference PCB. These are all 1080ti examples pulled from TPU.

* [Asus Strix - 3% faster than reference, 52W higher in average gaming] (https://www.techpowerup.com/reviews/ASUS/GTX_1080_Ti_Strix_OC/29.html)
* [MSI Gaming X - 5% faster than reference, 51W higher in average gaming] (https://www.techpowerup.com/reviews/MSI/GTX_1080_Ti_Gaming_X/)
* [MSI Lightning Z - 5% faster than reference, 34W higher in average gaming] (https://www.techpowerup.com/reviews/MSI/GTX_1080_Ti_Lightning_Z/)

> You really don't know what you're talking about.

I've been correct about everything so far and have backed it up with credible sources. You've been incorrect, moved the goalposts, and when forced to provide sources, you've only provided more evidence to back up my claims.

When you started this debate, you originally stated:

> Please explain to me how MSI put R&D money into the 1080 Ti Armor OC. That thing has a higher than default TDP and the cooler is smaller than on a Zotac 1070 Mini.

> It's a fucking joke.

And I showed that at it's launch MSRP of $699, it was as good as or better than other $699 1080ti variants, providing the FE and one other specific example.

You have failed to debunk this, and have gone off tangents because you can't debunk it.

We're done. Now, get the last word in so that you can be wrong one more time :)
I proved actual numbers. A direct comparison test. You have not.

Let me know when you do.

EDIT: Just a quick guide for you:

* DELETED = user removed their own post
* REMOVED = moderator removed post, but it can still be viewed in the user's post history (or via websites like Ceddit)

I take pride in the fact that you were so upset over this, you actually paged the mods to bail you out. It's even funnier that you then tried to blame me for it.

So, quick recap:

* you made a claim that was easy to debunk
* your claim was, predictably, debunked with actual sources
* you tried to move the goalposts, got called out, so cried to the mods
* after you got the mods to remove the post that offended your feminine sensibilities, you then tried to pass it off as me deleting the post and running from you

Your username does not check out :)
> and I don't think the EVGA has a shittier cooler than the armor OC.

It's the same style cooler as the FE, but rebadged. Thanks for confirming. You are being intentionally obtuse.

You're like those people who believe that the Earth is flat, that vaccines cause Autism, or that climate change is a hoax. Your feelings matter more than the evidence right in front of your face (like the numbers provided from a reputable source).

If this is how you normally are, then I need to discount your entire rant on MSI, because I can't be sure that it actually happened. You seem like the type to fabricate or embellish on this sort of experience because you didn't get your way.

Basically, you have zero credibility. If you're lying to support your stance that is easily debunked with a credible source, then you are potentially lying about other things as well.
Calibration drifts over time. It's best to do it on a regular basis for best results (at least once a year for most people is fine, though some  do it monthly).

The cost of a professional doing it will be as much as or more than the one-time purchase price of the suggested budget option above. Honestly, if you can swing $1k for a monitor, this is a pittance for calibration.
I was going off their listed MSRP at launch. The MSI GTX 1080ti was $699, which was the base MSRP. PRices now are out of whack for everything.

> I just don't see MSI as a company putting R&D money into their GPU if MSI is literally the only company putting out a 1080 Ti with such a shitty cooler and it's not even the cheapest card.

I just showed you that their cooler was better than the similarly priced FE and EVGA model. Are you just being deliberately obtuse at this point?
> They can buy the same panels but AW historically has sold better quality ones.

To clarify this a little better:

* they are the same panel
* they likely get the same batches, and therefore, the same quality
* the quality control comes in the build quality, materials, and edge reinforcement, an area where Asus and Acer typically haven't paid much attention

That said, Dell's build quality hasn't been great across the board. I'll provide 3 examples of recent monitors that have come across my desk.

* U2717D - Unbelievable build quality, lowest bleed (not to be confused with off-axis glow) I've ever seen
* S2716DG - While it looked like a near clone of the U2717D, the material quality was lackluster. The rear plastic felt like paper, whereas the plastic on the Ultrasharp was dense enough to kill a man. Several instances of backlight bleed due to poor edge reinforcement. Felt like an Acer/Asus gaming monitor.
* AW3418DW - My current display, replacing the Ultrasharp. Build quality/materials seems to be in between the U2717D and S2716DG. It's not as good as the Ultrasharp and there are a few bleed points (lower right corner being the biggest offender). But it is well build. It seems that they were a bit lax with the edge reinforcement around the power button (lower right) and glowing Alienware logo (upper left).

I haven't tested the Acer, but if it's anything like their other gaming models, it's a lottery. Both monitors suffer from the white point uniformity (where one side is warmer and one is cooler), and it seems that with each new batch this becomes less of an issue as LG perfects this panel.
I would avoid using someone else's profile, even TFTCentral's. Copy/paste from my prior post:

-----

Never use someone else's ICC profile. Monitors have variance from unit to unit. When you create an ICC profile, you are telling your system's software to compensate for your monitor's deficiencies. When you use someone else's ICC profile, you are over/under compensating for your monitor's deficiencies.

You cited gamma. TFTCentral's unit had a 2.6 gamma out of the box. Lims Cave's unit was 2.4. Mine was 2.46 (measured with i1 Display Pro and DisplayCal) out of the box, and 2.42 after OSD adjustments (no gamma adjustment, as you mentioned, but gamma can be slightly tweaked indirectly).

Using TFTCentral's profile causes my gamma to go below the 2.2 target, leaving a slightly washed out image. This is less than ideal.

-----

If you care about calibration but are on a strict budget, I'd recommend a Spyder 5 Express (~$120), and DisplayCal (Free). Don't get a different Spyder 5 unit. They're all the same exact hardware with an artificial software lock. DisplayCal doesn't enforce that lock, so all units are treated the same. If you do want to spend more, get an X-Rite ColorMunki (~$180), or an i1 Display Pro (~$230). The Spyder will get you in the ballpark and will be great for gaming, solid for color critical work. The ColorMunki will be more accurate, and the i1 Display Pro will provide similar accuracy but faster calibration.

Also note that games run in exclusive full-screen mode will typically bypass an ICC profile.
> Only differences are the aced has a one year warranty but allows gamma control in OSD

* The Acer has a 3-year warranty, not one-year [(SOURCE)] (https://us-store.acer.com/monitors/gaming/34-predator-x34-ultrawide-qhd-curved-monitor-x34-pbmiphzx?utm_source=us.acer.com&utm_campaign=CLM&utm_medium=referral)
* Dell's warranty advantage is that they support advanced exchange (they ship first), and they pay shipping both ways (super expensive with these displays)
* Acer supports FRC, giving simulated 10-bit support. Not a game-changer, but it is an advantage on the spec sheet
* The Acer also has built-in speakers for those who would want this. I could see primary-headphone users wanting the speakers as a fallback for when they're just browsing the web, IE, basic UI interaction audio. It's useless to me, but still a feather in their cap, so to speak.

> Also the gamma calibration is garbage out the box so you’ll need an .icc profile.

Never use someone else's ICC profile. Monitors have variance from unit to unit. When you create an ICC profile, you are telling your system's software to compensate for your monitor's deficiencies. When you use someone else's ICC profile, you are over/under compensating for your monitor's deficiencies.

You cited gamma. TFTCentral's unit had a 2.6 gamma out of the box. Lims Cave's unit was 2.4. Mine was 2.46 (measured with i1 Display Pro and DisplayCal) out of the box, and 2.42 after OSD adjustments (no gamma adjustment, as you mentioned, but gamma can be slightly tweaked indirectly).

Using TFTCentral's profile causes my gamma to go below the 2.2 target, leaving a slightly washed out image. This is less than ideal.

If calibration is even moderately important, a Spyder 5 Express is cheap (~$120), and DisplayCal is free.
> Please explain to me how MSI put R&D money into the 1080 Ti Armor OC.

Wasn't relevant to my comparison. I was comparing my MSI Gaming X to my EVGA SSC, though honestly, EVGA's FTW is their direct competitor to the Gaming X lineup. You don't compare a Prius to a Ford F350 in towing capacity. You don't compare the value GPU to the over-engineered GPU.

The Armor is MSI's budget line, typically matched the overall product MSRP, whereas Gaming X and FTW can be significantly higher than base MSRP. EVGA's equivalent to the Armor was [THIS] (https://www.evga.com/products/product.aspx?pn=11G-P4-5390-KR), which was their only 1080ti variant to be sold at MSRP, initially. And it performs like an FE model.

> That thing has a higher than default TDP 

And??? Most aftermarket cards have a raised power limit to support higher performance than reference/FE. Why is this a problem for you?

> and the cooler is smaller than on a Zotac 1070 Mini.

Yup. It has a bargain bin cooler. But at launch it was one of the few at the $699 MSRP, and it performs better than the FE per GamersNexus' review - https://www.gamersnexus.net/hwreviews/2927-msi-1080ti-armor-review-high-temps?showall=1

* 7° cooler than FE
* 1db louder than FE
* Typically 2-3% better performance than the FE (per other reviews, as GN didn't cover this part)

But the best part of the Armor is that it has the same PCB as the Gaming X. So, you can get a card at the same price as the 1080ti FE (in a normal pricing environemnt), but with a better board for overclocking. Throw a water cooler on it and you're set.

The stock cooler is adequate if you don't plan to OC.
> Can you please elaborate on that?

EVGA Z270 stinger (my motherboard) vs. other Z270 M-ITX boards. At launch, EVGA wanted $199 (I paid $99 way after), while other boards were $20-$50 cheaper.

While literally every other Z270 M-ITX that I could find placed the USB headers on the edge, the Stinger has it just above the GPU. Yay cable management! While other companies look for logical placement of the connectors/headers, EVGA just said "wherever our budget allows."

Then there's the cooler. I had an MSI RX 480 and swapped for an EVGA GTX 1060 SSC (made a profit due to the timing). The 480 ran 180-190W typically and up to 225W, while the 1060 had a 120W board power limit. That means more heat, and more noise for the 480, right?

Well, yea the 480 generated more heat, but MSI's superior cooler dissipated it much better. It ran at a barely audible noise in gaming at ~72°. The EVGA gets a bit noisy (not obnoxious) and 77-80°. Multiple reviews show the MSI Gaming X 1060, same cooler as the 480, running obscenely cool and quiet.

These companies are all trying to sell a similar product at a similar price point. They all have similar budgets for their products. EVGA allocates more on the customer service side, whereas MSI allocated more on the R&D, and it shows. They both excel where they should.

The customer should purchase based on their unique priorities.
I list "free" items as x amount, and then an equal discount if they pick it up.
For the exposure, of course.
And this is why EVGA (in the US) has such a devout fan club. Their motherboards aren't usually as good as other brands, and often cost more. But I still buy them because if things go bad, I know I'm covered.

Their GPUs are ho-hum. Not engineered nearly as well as the other brands, and even their dual fan coolers can't keep up with the competition. But again, if things go south, you know you're covered.
Visa and Mastercard also provided extended warranties, but theirs is more confusing, more limited, harder to use, and subject to variance based on the issuing bank (the same applies to a co-branded Amex issued by a different bank). Discover was offering extended warranties, but they are currently in the process of scaling back those perks.
> google says they don't even make triple channel ram anymore. must not have caught on

Memory itself isn't dual/triple channel. It's the configuration. You can buy 3 identical sticks and run them in triple channel mode IF the motherboard supports it.

Here's a triple-channel kit - https://www.newegg.com/Product/Product.aspx?Item=N82E16820231356

Intel's X58 platform is the last I know of off the top of my head that supports it. They've moved to quad-channel on their latest X-series chipsets, and dual-channel on Z-series and below.
> no mb would allow 3x2gig sticks of a 2 and a 4.

If you have 4 slots for RAM, you can load them as 3x2GB with one open. You'll have 6GB total RAM, with 4GB in dual-channel, and 2GB in single channel.

If you load a 2GB and a 4GB side by side, you'll have 6GB. I believe they run in single-channel mode when set up like that.
Yup. I'm very careful about what I charge on each credit card. We have a USAA card with 2.5% back on ALL purchases. We use that for general shopping and Discover for the 5% categories. But any electronics purchase $100 or more goes on the Amex. Automatic warranty extension.
American Express. I've used it twice. Both times I had a credit on my Amex account for purchase price + tax.
The only proper response:

> If you truly love her as you claim, you would undergo sexual reassignment surgery and show her your vagina.
>  they just realise that not a lot of people will sue if they're denied a warranty (coz court is expensive).

It's not too expensive, people are just lazy. In my county:

* small claims court filing fee is $29, and you can recoup in the suit
* no lawyers allowed to be present, so this hurts the company big time
* Thanks to the Magnuson-Moss act, burden of proof is on THEM, not you

It's an easy win.
Magnuson-Moss Act in the US.


Burden on proof is on the company to prove beyond any doubt (higher burden than reasonable doubt) that the customer did in fact do something worth voiding the warranty for.
> first, you didn't use a /s, if theres on now its an edit

Generally, no asterisk means no edit (unlike this post). But you can edit within the first few minutes and not get an asterisk. I sincerely don't remember editing it, but if I did, a moderator can confirm this as they can see a post's entire edit history. Up to you if you want to request that from the local mods. But since you're now going into insults, you're more likely to get yourself in trouble (like a drug dealer reporting theft to the cops).

You likely saw what I said, and you're trying to project your fault on me. If you had any credibility in this debate, that lie of yours would have seriously hurt it.

> second, I was explaining the basics of variables vs constants, sadly it seems you lack the 2 iq points to rub together to spark on that one.

[You're...] (https://www.youtube.com/watch?v=FPD5q6DC43M)
To add to this.

If running at a constant temperature that is well within spec, doesn't matter if it's 35° (typical idle desktop use on a fan-stop model), or 75°, wear is the effectively the same (except on the fans, of course).

Mining conservatively (power limits) has no more impact on the board than desktop use. Gaming, especially when overclocked, is murder on a GPU due to the drastically changing temperatures from loading and quitting a game, not to mention some games have varying loads that cause noticeable temperature shifts.

Outside of the fans, mining properly doesn't wear down a GPU that badly. And I've had numerous cheap case fans last YEARS. I've never had a GPU fan fail (anecdotal, I know). My only fan failures were an Arctic CPU cooler after 6 years of continuous use, and a few NZXT fans from their Kraken x61 (those fans were hot garbage).
Ok, a few things.

One, /s = sarcasm. I was mocking you.

Two, you went from 'you can tell how a product was used based on wear,' to, 'due to silicon lottery (and other variance) they won't wear the same even with the same use.'

Basically, you just shot yourself down. But based on your prior posts, I'm not sure you're capable of seeing that.
Ah, that's why all fans and all memory chips fail at the exact same usage and not at different intervals. /s

You have no clue what you are talking about.
https://www.google.com/search?q=nzxt+s340+micro+atx+build&safe=active&tbm=isch&tbo=u&source=univ&sa=X&ved=0ahUKEwiRq5KFs6LaAhUnxFQKHdP6CL0QsAQILg#imgrc=LvkXLSjxBYMVpM:

Image results shows what you should expect. Up to you if that is too empty or not.
Pardon my poor typing. I sent the report (with typos) requesting review and reinstatement of the thread. Just wanted to clarify so no one thought that the OP made a separate account to do so.

Thank you.

EDIT: MOAR TYPOES!!!111
Based on their rules, the limited response that they provided, and my understanding of this issue, it's likely this:

The subreddit is specific to elipepsy. Your question was about insurance. You're best bet was on insurance-focused subs, not one for a specific condition.

It would be like talking about PC Games on /r/hardware. That wouldn't make sense. /r/hardware is for the components, and /r/pcgaming is for the games. Similar items with very specific communities.

I hope that makes sense.
If you are serious about rejoining the community, I would do the following:

* Delete from your history all posts/submissions related to the topic - This isn't meant to cover your tracks, but instead, remove material that the mods in question could consider personal attacks.
* Wait a week or longer, then
* Send them a modmail to the subreddit (NOT a private message to any mod specifically), saying the following:

> I sincerely apologize for my past behavior. Someday, I'd like reconsideration and another chance to be a part of this community. As a sufferer of epilepsy myself, this subreddit has been a great help to me and I truly believe that, in time, I can be of help to others as well. In the past, I was more focused on the reasoning for the rules and my deleted posts. I now understand that the rules are in place for a reason, and I should comply. I know that at this time, lifting my ban is not a preferable idea. But again, hopefully, you'll reconsider somewhere down the road. In the meantime, best regards and thank you for your time and attention to the matter.

Then stop. Don't post about them. Don't send them more messages. Just walk away. Maybe 6+ months down the road you try again.

And you have to be ok with rejection. Because the first time you fly off the handle after hearing, "no," you're going to be back to square one.

I sincerely hope this helps, and best of luck with this process.
My wife can attest. Everytime that I say, "just one more upgrade and it's done," she rolls her eyes.
It's up to the staff here if they want to reach out to the subreddit in question and try to mediate your issue. But your actions after the face, both now and one year ago, make mediation harder.
Again, I think you're missing the point. These threads that you make, and tagging the moderator, are making it so that they would likely never lift your ban.

And your response above doesn't show contrition. It shows that you're more hung up on the reasoning (IE, trying to prove him wrong) than you are on focusing on your error, learning from it, and becoming a contributing member of their community.

Based on what you've posted, I concur with their removal of your posts (violation of rule #1), their subsequent ban of you (repeated violation of rule #1, potential harassment of the moderator(s)), and their continuation of the ban in place (lack of contrition, targeted harassment of the moderator in question).
And what was your response when they declined? Aside from this thread, the other thread, and the potentially harassing tagging of the moderator in question?
I'e got something salty for him. And the older I get, the more and more it resembles his head.
After more than a year, the ONLY acceptable way to get unbanned from a sub is to say something like:

> Greetings,

> Over a year ago I was banned from your subreddit. I violated the rules and, in a fit of something, rage? Immaturity? I don't know, but I misbehaved and said things that I regret. I kindly request reconsideration of my ban and maybe a second chance to participate in this community. Regardless of your time, thank you for your consideration.

Should they have turned you down, that may be a test to see how you'd react (IE, have you really learned anything, or is this an act?). So, you'd ideally respond with.

> I completely understand. My behavior was completely unbecoming. I'm hoping that sometime down the road you will reconsider giving me that second chance. Regardless, best wishes!

And stop there.

But somehow, based on your other posts, I don't think that's the tone that you've taken. But you could prove me wrong with a screenshot of the message that you sent to them.
Thank you.

Unlike comments, the actual meat and potatoes of a submission is gone (for you or me to see, but not the mods of that sub). So we won't be able to see your content here.

I would state that your post does run afoul of their subreddit rules, based on what you've stated. Rule #1 expressly states no political posts, but your post (despite leading off with 'non-political' in the title) was very much a political topic. It was also off-topic as it was a question about insurance, specifically, and not Epilepsy.

So I concur with the moderator's decision to remove your submission. And I would guess that your ban didn't result from that, but instead, from any subsequent interaction between you and the moderator in question.

In fact, you confirmed as much here - https://www.reddit.com/r/complaints/comments/5o6d4z/boattailedgrackle_mod_from_repilepsy_has_decided/

However, if you were banned, it was lifted because you made this submission in their subreddit afterwords - https://www.reddit.com/r/Epilepsy/comments/5o5748/preexisting_condition/

Which was also removed, likely for the same reasons.

You then made 4 more submission complaining about the subjet (links below), OVER A YEAR AGO:

* https://www.reddit.com/r/seizures/comments/5o65nf/boattailedgrackle_mod_from_repilepsy_has_decided/
* https://www.reddit.com/r/obamacare/comments/5o6fmn/some_mod_from_repilepsy_has_decided_i_cant_ask/
* https://www.reddit.com/r/seizures/comments/5o60qh/apparently_im_not_allowed_to_ask_repilepsy_if/
* https://www.reddit.com/r/healthcare/comments/5o6b10/boattailedgrackle_mod_from_repilepsy_has_decided/

However, my main concern is this - https://www.reddit.com/r/subredditcancer/comments/89uc9g/uboattailedgrackle_banned_me_from_repilepsy_for/

You made this post and then tagged the moderator in question. That's considered harassment per Reddiquette. You're setting yourself up for more than just a ban from a single subreddit.

My advice would be to drop your crusade before you get in more trouble. It's been over a year. He hasn't shown any form of harassment against you (disagreeing isn't harassment). You, however, are beginning to lean towards harassing him.
I checked your post history through the past month and see zero posts in that subreddit. Even if a moderator 'removes' your post, it just becomes unavailable on that subreddit but remains in your history. It would only be gone from your history if you yourself deleted it, or if the post had never been made.

In my experience, people who delete their post after a moderator takes action on it are trying to hide something, usually guilt.

It's very hard for me to take your side without any evidence of your post and its specific content. If you have screenshots of the post and response(s), or if you have a link to the post in case I over looked it (a possibility, I'll admit, I'm not perfect), that would be really helpful.
> Wasn't the performance jump to Pascal much larger than the jump between earlier generations?

Yup. Typically, the old x80 stacks up against the new x70, not the x60 as in the case here.

But regardless, the 980 was quickly outclassed by the 1070/1080 launch. Anyone expecting the 1080 to keep pace when the new x70/x80 launches is sadly mistaken.

Why would I pay $550-$700 for a 1080 today, when I can likely get similar or better performance from the new x70 for likely less in a few months, or noticeably better performance for about the same (give or take) from the new x80? That's my point. The downvotes and intentional twisting of logic doesn't invalidate that.
It's subjective. I prefer it. You may very reasonably disagree.

My wife has always preferred squared watches, even before smart watches were a thing. She has an Apple Watch. When Android Wear first came out, I skipped the square G Watch and Samsung Gear in favor of the Moto 360, which was novel at the time. Now, good lukc finding a square WearOS device.
To piggyback on this.

Buy a ~$100 PC stick. Install Steam OS. Use it as a Steam Link from your main PC, but with additional functionality.
There are pros and cons to each.

In favor of ASICS:

* doesn't impact PC gaming as much (it still does by driving up the costs of sub-components, but this is less direct than killing pricing/availability of GPUs altogether)
* better for the environment (more efficient) in the short term

In favor of GPUs

* more decentralized, allowing people to mine with existing hardware
* (typically) lower up front hardware acquisition costs

EDIT: Not sure which part, but apparently this post of mine was capable of mining salt :p
> You are incorrect sir.

If you think that I'm incorrect, then I need to clarify my statement. I'll try.

The GTX 980 was outclassed by the 980ti, but not obsolete, as it offered lower performance at a lower cost.

But the GTX 1070 offered better performance at a lower cost, while the GTX 1080 offered much better performance as its successor. Anyone who bought a GTX 980 for MSRP a few months prior to that launch might have felt foolish.

The same applies here. The new x70/x80 should launch soon. The current 1080 is being sold at or above MSRP. A new x70 should offer comparable or better performance at a lower price, and the new x80 should offer much better performance for a comparable or slightly higher price.

Anyone buying a GTX 1080 today needs to be aware of this. I personally wouldn't buy one at or above MSRP this close to its successor launching.

I hope this has better clarified my point. The GTX 980 is a fine card, but it's decidedly mid-range today.
It was an ideal option, but with the Ryzen 2000 series now up for preorder, I'd wait for reviews. The 2600 ($199) and 2600X ($229) are very tempting.

The 8400 is 6c/6t, while the 2600s are 6c/12t.

If you must buy today, get the 8400. But if you can wait for initial reviews, do so.
As a longtime Android user, I purchased an iPhone 8 and used it for a week before returning it and waiting on the Pixel 2. It wasn't enough at the time to sell me on the switch, but I'll consider it again down the road.

I like having my phone, watch, and TV STB in the same ecosystem. Google has basically abandoned Android TV. So if Apple ever figures out how to make a circular watch, I might just give them another shot.
The same applies for a listed fair price. If I want $200 for an item, and the going rate is at least $200, I'll list $200 and people will constantly try to low-ball me.

But then I jack the price to $300, and after some negotiation, it sells for $225-$250.
Got mine for $119, and then $20 MIR, so $99 for me. Good value there. A far cry from the launch $199 price. My main issues are mostly cosmetic:

* poor USB 3 header location, results in laughably bad cable mnagement
* wifi module/antenna not pre-isntalled, gives you a less than ideal connection
* M.2 red LED cannot be turned off, super bight an annoying at night through a clear case

But this new Stinger model appears to be a downgrade in every way.
Same here. I have a 1060, would love 1080 performance, but we're just too close to the new product launch. Everytime I consider a 1080, I force myself to look at the 980. That's where the 1080 will likely be in a few months.
A post that lists a problem, AND a solution? Kudos!

My Pixel 2 didn't have the issue, but this is a great post for those who run into it.
No USB Type-C, and as an H370-based board, it's unknown at this time if this would support overclocking K-series CPUs (initial announcements indicate 'no'). It also lacks support for faster system memory. That's two, potentially three major downgrades over my Z270 stinger.

 

On top of that, it looks like EVGA's engineers are still struggling with:

 

* getting the USB headers around the edges, where they should be, and where everyone else places them (Kudos for at least moving the 3.0/3.1 gen 1 header to the edge this time)
* pre-installing the wifi card and antenna connections, like literally everyone else

Pricing has never been the strong suit of the Stinger series. You paid for a product with fewer features than the competition, but better warranty and customer service. But potentially lacking overclocking and support for > DDR4-2667, it's going to be VERY hard for EVGA to sell this at the typical Stinger launch price of $179-$199 when you can get the Gigabyte Z370N for ~$150 and have been able to for awhile. So I'll be curious to see what the MSRP is for this board. At $99 it has a market, but the Asrock Z370m is only $15 more at that point.
No USB Type-C, and as an H370-based board, it's unknown at this time if this would support overclocking K-series CPUs (initial announcements indicate 'no'). It also lacks support for faster system memory. That's two, potentially three major downgrades over my Z270 stinger.

 

On top of that, it looks like EVGA's engineers are still struggling with:

 

* getting the USB headers around the edges, where they should be, and where everyone else places them (Kudos for at least moving the 3.0/3.1 gen 1 header to the edge this time)
* pre-installing the wifi card and antenna connections, like literally everyone else

Pricing has never been the strong suit of the Stinger series. You paid for a product with fewer features than the competition, but better warranty and customer service. But potentially lacking overclocking and support for > DDR4-2667, it's going to be VERY hard for EVGA to sell this at the typical Stinger launch price of $179-$199 when you can get the Gigabyte Z370N for ~$150 and have been able to for awhile. So I'll be curious to see what the MSRP is for this board. At $99 it has a market, but the Asrock Z370m is only $15 more at that point.
No USB Type-C, and as an H370-based board, it's unknown at this time if this would support overclocking K-series CPUs (initial announcements indicate 'no'). It also lacks support for faster system memory. That's two, potentially three major downgrades over my Z270 stinger.

 

On top of that, it looks like EVGA's engineers are still struggling with:

 

* getting the USB headers around the edges, where they should be, and where everyone else places them (Kudos for at least moving the 3.0/3.1 gen 1 header to the edge this time)
* pre-installing the wifi card and antenna connections, like literally everyone else

Pricing has never been the strong suit of the Stinger series. You paid for a product with fewer features than the competition, but better warranty and customer service. But potentially lacking overclocking and support for > DDR4-2667, it's going to be VERY hard for EVGA to sell this at the typical Stinger launch price of $179-$199 when you can get the Gigabyte Z370N for ~$150 and have been able to for awhile. So I'll be curious to see what the MSRP is for this board. At $99 it has a market, but the Asrock Z370m is only $15 more at that point.
A 1080ti is an absolute beast. In some games you are going to be CPU limited with a 1600x, even @ 4ghz. That could be one culprit.

You have G-Sync enabled. What about V-Sync? IE, what happens when you go above your refresh rate and G-Sync turns off? If you're capped via V-Sync or other artificial means, then you aren't maxing your GPU.

Note, there is absolutely nothing wrong what what you're doing, btw. I just want to be clear that if you're running at 75°, you've got some additional headroom :)
@ /u/Satzlefraz and /u/baldmannbob

Techpowerup agrees with you, 84° under load - https://www.techpowerup.com/reviews/NVIDIA/GeForce_GTX_1080_Ti/34.html


@ /u/spETHtacular

A blower style cooler isn't as subject to a case's cooling as a dual-fan circulation style cooler. If you're getting much lower temps than EVERYONE else, including the press (who typically get sampled a unit that was tested prior), you may have an issue. Consider the following:

* Are you power limiting the card in any way?
* Are you capping FPS artificially or via V-Sync? (doing this keeps your card below 100% load)
* Are you CPU limited?

If you push a GTX 1080ti FE to full load for extended periods, it's going to run at 84°. If you have meaningfully lower temps, then you're not running your GPU under full load.
I'm OK with that considering that he's coming in at half what we paid Torrey.
The sources at the bottom of his page are outdated, meaning the links redirect to the wrong place, or are dead links entirely. As a result you have to search for them to verify their accuracy.

His 2nd source, "Gamma FAQ by Charles Poynton -  Definitive and detailed explanation of gamma.," can be found here - https://poynton.ca/notes/brightness_and_contrast/index.html

Note is in italics following the 2nd paragraph.

Additionally, your comment here:

> To back up your claim, that the page I linked to was referring to CRT displays, you'll need to provide a valid source. As the page I linked to, states clearly at the top it was last updated on June 8, 2005

That should have been a red flag to you. In early 2006 I was returning from Iraq and building my first 2 PCs. While we did use LCDs for each system, the majority of pre-builts we looked at still used CRTs, with the more expensive units coming with LCDs. We hadn't fully transferred away from CRTs at the time, nor had the television space.

I would never take an article last updated in 2005 on display technology to apply to 2018's display technology.
It basically works the same as the Steam Link, but it supports 4k. It also does support the Xbox One controllers (Bluetooth models).
Nvidia Shield.

But no HL3.
> Your reply was completely asinine, what does it have to do with anything I said?

As a third party here, it's pretty easy to see what he was talking about. You said:

> Cannon lake was just a die shrink, not a new architecture.

And he stated:

> Well there were changes on the architectural level

He corrected your statement. It was more than just a die shrink. No need to get upset over that.
AOC warranty is non-transferable. It covers the original owner only.

-----

http://us.aoc.com/warranty.php

> Who is covered?

> **Original consumer** who has purchased the AOC monitor from an AOC authorized distributor or reseller within Canada or the Continental United States, Alaska, and Hawaii. Must have a valid USA or Canadian address which can be verified by FedEx, UPS, USPS, or any transportation carrier.
> I don't know how you got my removed post. 

Posts aren't actually removed, they are simply hidden. If a moderator 'removes' it from their subreddit, it's still in YOUR post history.

> What does it matter what word you use?

The explanation to that was in the AutoMod response that they gave you. Apparently they're sick of those kinds of posts.

That subreddit has over 17 million subscribers, making it one of the largest subreddits on the website. They likely get a new submission every few seconds, so they need Automod to filter and limit this.
> How in the hell can freedom of speech be impeded like that? 

A subreddit isn't the US government. You don't have freedom of speech here.

Now, your removed post which can be viewed [HERE] (https://www.reddit.com/r/movies/comments/88u8r3/i_love_eyes_wide_shut_its_extremely_underrated/) was removed by AutoModerator, which is a bot. They apparently get so many of those posts that they feel the need to auto-remove them.

It concludes with - - reword your submission and resubmit.

So, it's encouraging you to use a different word. Hence, I can see why they suggested a thesaurus.
The beauty of G-Sync is that you don't need to maintain the max refresh rate. I have a GTX 1060, which is slightly faster than your 970. I run at 3440x1440 on an Alienware AW3418DW. I target ~60fps.
Wait, I thought Republicans said that celebrities should keep their mouths shut about politics?
Then it's Karma fraud. Take him to /r/KarmaCourt
Depends on the return policy. Some places let you return for ANY reason. So, this would be covered. And if the boxes weren't opened, even better for the retailer.
> Could the Wifi card also be powered through that second connector?

My understanding is that the 4/8-pin is exclusively for the CPU, and the 24-pin is for the rest of the devices connected to the motherboard.

Don't take this as gospel though. I'm just guessing and haven't even looked into that.
And this is why the MGTOW/"Niceguy" type can't get women. These are abusive men who don't realize that they are abusers, so they fail at hiding it.

"Chad" isn't their enemy but rather, their ideal - someone who can remain as ignorant and abusive as they are, but still get the girl.

It's basically Dunning-Kruger for social interaction.
> I have a fx8350 at 4.9ghz and an msi 290x with a 10% overclock so mine definitely isn't over estimated but even then I'm probably only using close to 650 watts on intensive games.

I'd be shocked if your system used close to 650W. I'd be shocked if it eclipsed 500W. But yes, that is a high power draw combo.

I have an i7-7700k and a GTX 1060. I'm running the CPU stock for now, will re-evaluate when I get an 1180/2080. Anyway, using a Kill-A-Watt P3-4400, I can see less than 200W (typically ~180-190W) at the wall when gaming, and ~215-225W when stressing CPU and GPU. Again, that's at the wall. A PSU draws more than the system needs due to no energy transfer ever being 100% efficient, so my system draw is actually lower.

You wouldn't believe how many people insist that CPU/GPU combo needs AT LEAST a 500W PSU. Don't get me wrong, I'm all about getting a quality PSU, and you're not going to get a 300W PSU with the quality and warranty length of a 500+W PSU today. But the actual wattage needs aren't what a lot of gamers think they are.

The people who fall for the "I need high wattage" meme are the same people that get racing chairs and gamer-branded headsets.

And to a degree, we've all been there. I still have my racing chair.
More than enough. Most people here GROSSLY overestimate their power needs (and internet PSU calculators don't help).

Your PSU can handle 456W over the 12V rail, which is where most of your system draw comes from. Unless your CPU is an overclocked AMD FX-9590, you're good.
It's the same.

The point of a 6+2 pin is for it to be able to work in a 6-pin OR an 8-pin.

You have a 6+2 and a 6-pin, your PSU was designed to work on a 6+6 GPU, or an 8+6 GPU.

You're good.
Advantages to the Alienware

* Warranty - while both are 3 years, Dell does advance replacement (sends you replacement first), and pay shipping both ways. Acer has you send yours in first, and you pay shipping. Shipping one of these isn't cheap.
* Potentially build qualty (debatable) - Acer's build quality has been poor enough in their gaming monitors that backlight bleed (not an IPS issue, it's a build quality issue) has been a known problem on prior monitors. That said, the AW3418DW's build quality isn't as good as my prior Ultrasharps, so I'm not convinced it's actually better than Acer.

Advantages to the Acer

* Speakers (if you need them)
* 8-bit + FRC (if you have a use for 10-bit)
* OSD control for gamma (this is the biggest deal for me, on the Acer side)

In the end, the Dell warranty wins out for me. And I was lucky enough to get a unit with acceptable contrast (2.46 out of the box per DisplayCal, measured with an i1 Display Pro).
> Turning the monitor brightness setting up, turns up the backlight, which increases the power draw, and increases the black level

And the white point, and the overall brightness. Black level, white point, and measured brightness all increase when you raise the brightness. That is the core function of this setting.

Is it possible that some monitors/TVS have it reversed? Sure. But for the monitor we're discussing, one I have in my possession, and have calibrated myself, I can confirm that everything that I've stated is correct.

And referring to your prior post:

>  If you have the brightness set to 100%, then you're going to have a terrible contrast ratio

Nope. The AW3418DW's contrast is close to uniform across all birghtness levels, as seen [HERE.] (http://www.tftcentral.co.uk/reviews/dell_alienware_aw3418dw.htm#contrast_stability)

In fact, it was at 0% brightness, the lowest black level, where the monitor showed the lowest contrast ratio. That's because, again, luminance also increased with brightness, not just black level.

-----

But there's a reason for your confusion. A subtle note you may have missed, from his primary source on contrast vs. brightness.

> This note dates from 1999. It applies to CRT displays. At 2010-06-03, I am working to update it to reflect modern displays, particularly LCDs

Yup...you're applying CRT logic to LCDs.
I have. They are falling, and have been.

Miners (most of them) stopped buying GPUs over a month ago. It wasn't and won't be an immediate change.
The problem now isn't pricing held up by miners, but by gamers. There's so much pent up demand that gamers are the ones buying them at inflated prices. It's going to take awhile still for pricing to come down.

VERY few miners are buying GPUs right now.
Right now it's a rebrand. Basically, Google WAS updating the Android Wear "version" via the Android Wear companion app via the Play Store (Starting with 2.5 I believe, up through 2.9). The WearOS rebranding of that app caused | AndroidWear 2.9 --> WearOS 2.10.

But once you get the OTA, WearOS becomes 1.0 (full rebrand), and below that is a version called "Home App" which displays 2.10, IE, the version of the companion app.

Confused yet?
The 24" version, despite its size is a higher quality monitor. It has fewer display issues (no monitor is perfect, so this is nitpicking), and support for OC up to 165hz.

Also, making smaller pixels (both are 1440p) comes with a cost.
>  The cooler in the Armor is 100% inadequate as it runs much hotter than a FE card.

I'd be really surprised at this. Granted, I couldn't find a quick review of a 1080 Armor, but I did find the 1060 variant.

* [The Armor was generally 2-3% faster than FE] (https://www.techpowerup.com/reviews/MSI/GTX_1060_Armor/30.html)
* [7° cooler under load while being quieter] (https://www.techpowerup.com/reviews/MSI/GTX_1060_Armor/34.html)

Gamers Nexus did do a review of the 1080ti armor:

https://www.gamersnexus.net/hwreviews/2927-msi-1080ti-armor-review-high-temps?showall=1

And they found that the 1080ti Armor was 6.8° cooler than the 1080ti FE, though the Armor was 1dB louder in this state.

So, much hotter? For the 1060 and 1080ti, it's found to be noticeably cooler, not hotter. I would argue that it's not a good overclocker, since it was at its limits on the 1080ti. But value cards aren't meant to be for overclocking. That's why he had a separate recommendation for overclocking.

The Armor is generally the cheapest option, and offers slightly better than FE performance and cooling. It's a value-oriented offering.
Did they pay shipping both ways (as EVGA does), or did you pay to ship it to them?

Did they cross ship (as EVGA does), or did you ship first, and wait an period of time for a repair/replacement?

My point was, they aren't similar. When you call EVGA For an RMA, you can request a cross ship. They'll put CC Hold on the replacement cost (though they waived this for me during my 2015 RMA). They ship (it took 2 days to get to me), and they give you a return label.

I paid nothing and had a replacement in 2 days.
The GTX 1060  uses 8Gbps GDDR5 on a 192-bit memory interface. That's 192 GB/s. Per Nvidia, the effective is 230.4 GB/s (best case, due to their DCC).

The GTX 1060 9Gbps GDDR5 model has a real memory bandwidth of 216 GB/s, and an effective of 259.2GB/s with DCC. Despite this, benchmarks show no real world performance gain outside of margin of error, because the GTX 1060 isn't as memory starved as the RX 480/580. SOURCE - http://www.guru3d.com/articles_pages/msi_geforce_gtx_1060_gaming_x_plus_review,13.html (each game as a difference of 1-2fps, a few 3fps results, and numerous ties).

A GTX 1160 like the one I'm describing would feature 224-256GB/s of bandwidth, and Nvidia will likely eek out more gains from DCC for this generation. This isn't unheard of for Nvidia. Look at their modern history with the x60 lineup.

* GTX 460 = 86.4/115.2/96.2GB/s (multiple versions)
* GTX 560 = 128GB/s
* GTX 660 = 144.2GB/s
* GTX 760 = 192GB/s
* GTX 960 = 112GB/s
* GTX 1060 = 192/216GB/s

They peaked with the GTX 760, then had a massive drop. Small gains aren't out of character for Nvidia, especially with the refinements they've had in DCC and other areas.

You're arguing my speculation, but you're not using history to back up your points. Heck, I could  see them doing 8GB 14gbps (224GB/s) and 4GB 12gbps (192GB/s) to differentiate the two.
> so I was wondering if I can use the FreeSync feature and get the 75hz on the monitor (Nvidia card users are able to)

No, you cannot. Neither Nvidia nor Intel currently support AMD FreeSync. You need an AMD GPU of some kind to support FreeSync.

There was a report awhile back of Intel eventually supporting the open adaptive sync standard that is the underpinning of FreeSync, but they're not doing that yet.

Many 75hz FreeSync monitors only run at 75hz with FreeSync enabled, otherwise cap at 60hz. To go beyond this you would need to set a custom resolution. You can read up on that here:

https://www.intel.com/content/www/us/en/support/articles/000005540/graphics-drivers.html

Next, you'll want to test for frame skipping here:

https://www.testufo.com/frameskipping
> Looked up “numerous.” It means infinite amounts of, not just two. So they did lie.

http://www.dictionary.com/browse/numerous

> very many; being or existing in great quantity:

By your definition, you just lied, because it doesn't mean infinite.

Again, you're playing the victim, and I can't help someone who clearly doesn't want help. Private messaging me doesn't change that. Best of luck to you.
That gets taken as spam sometimes, unfortunately.

Not all downvotes are due to rudeness.
Start by not swearing at people who show a willingness to help.
Let's start here. You said:

> Then they lied to me claiming I posted my Discord "numerous times" when I reposted it once.

They claimed 'numerous times,' which means "more than once." You claimed to have reposted. A post is one time. A repost is a second time, meaning more than once.

They didn't lie here, as you claimed. You're being manipulative.

Once you understand and acknowledge that, we can discuss more of your post.
> I would link my evidence, but people will just be rude to me

Then you are literally just wasting everyone's time, and people will probably just be rude to you.

Rather than break down your post point by point as I normally would, I won't bother. It wouldn't be productive. Let's just say that you have a "play the victim" mentality so no one here is going to convince you otherwise.

Until you look inward and seek self improvement, no one else is going to be able to help you. You have to want help first.
Two possibilities here.

1. The entire world is involved in a conspiracy to pick on you
2. It's you, seek self improvement.
\>  WQHD is 2560x1440 

* 2560x1440 = QHD \(Quad High definition, which is 4x 1280x720\)
* 3440x1440 = WQHD \(Wide QHD\), I don't know why those monitors are mislabeled as WQHD
They wouldn't. They'd use 4 chips. And since GDDR6 comes in 1GB/2GB sizes, that would mean a 4GB and an 8GB variant, which is a logical evolution from the 3/6GB of Pascal and the 2/4GB of Maxwell.
Good to hear. I've been hearing this more and more on their newer monitors. If they had supported this on the 34UC98-W, I would have kept that and my RX 480 and enjoyed FreeSync (though the range on it was laughable, I would have adjusted).
> To be fair it's normal to claim adult children on your taxes when they're still young and not making much money. You get more from the dependent than the small refund you might get from filing taxes.

It's not normal to disown your child, sign emancipation papers, watch them leave for the military, and still try to claim them on your taxes.
The best option is the Nixeus EDG 27, and it's not even close. That will run you $400-$500. If you want those specs for $300, you'll be buying used or refurbished.
Everyone always cites Sapphire, but their RMA experience is nothing like EVGA.

* 2 year warranty
* Handled by 3rd party (Althon Micro)
* You pay to ship it
* No cross ship option

Sapphire is not in the same league as EVGA. And save one exception (MSI RX480), all of my Radeon cards have been Sapphire from my 9800 Pro to my 6850.
They also do home repairs for less than a handy man.
I found out awhile back that my father died by falling in his back yard and hitting his head on something while drunk off his ass.

And the phrase 'while drunk off his ass' described nearly everything that he did. Last I heard, I had a small inheritance, but my sister and father's ex-wife found a way to cut me out and split it. All that's left is a $50 savings bond from the early 80s that they can't cash. She DM'd my wife on Facebook to try to get it to me, which I believe to be nothing more than an attempt to get my address or phone number.

I told her to frame it and put it on the wall as something to remember me by.
I'm doing VERY well. I became the polar opposite of my mother. Thanks though :)
Can confirm, she inherited the crazy.
The sister is following in her mother's foot steps. We don't talk. I've disowned my entire family. Aunts, uncles, cousins, you name it. They all lined up on my mother's side (against a 14 year old). It wasn't until years later that my mother proved me right by taking out credit cards in my step-father's name without his knowledge and losing the house to her gambling. And when they realized that I was right? Not one apology.

Fuck them.

They're all like my mother to variable degrees, and pursue money from family. If they knew how I was doing, they'd try to use me as a source of revenue. It's best that I keep them at a distance. We're literally on different coasts now.
https://www.asus.com/us/Motherboards/ROG-STRIX-B350-F-GAMING/HelpDesk_QVL/

Check the memory QVL (qualified vendor list). If the memory is on there, it can run at the avertised speed. If it's not on there, then YMMV.
I suspect that was a typo in the spec sheet. There aren't many 5k IPS 27 panels out there, and all of the ones LG makes (that are publicly known) are 8-bit or better.

EDIT: Nope, seems pretty deliberate in their spec sheet and is being reported elsewhere as well.
No. GS/PS are made by SeaSonic. The G2/G3/P2/T2 (and a few more) are made by Super Flower. Their cables are mutually exclusive.
Nah, more of a joke. He wasn't tech savy, and our only internet device was the Dreamcast in the living room. So I figured what the hell.
Of course.
The Nixeus EDG 27 is the best current 27" 1440p FreeSync display

* It has an AHVA (IPS-like) panel with no ghosting/overshoot ghosting issues
* It has a 30-144hz Freesync range with LFC
* It is the ONLY FreeSync monitor with Adaptive Overdrive (a feature required by Nvidia on all G-Sync displays)

It is essentially a G-Sync monitor using FreeSync instead. If I were still using an AMD GPU, this would be the monitor that I would get. It's not even close.
My mother had a gambling addiction. We lived in Northeast PA, and she'd go to Atlantic City every other weekend. At 11 she made me start working under the table, and she took the money. When she found out that I had been hiding tips for myself, oh man I was in for it.

I moved out at 14, and was legally emancipated by 17. That happened because my mother and father (who were divorced and not on speaking terms) thought that they could sue me for child support (at the time, he was paying her for both me and my sister). He sued her to cut off my allotment since she wasn't caring for me. So, she thought that she could sue me to recoup that (don't ask, she never was smart). Why he teamed up with her, I don't know. But anyway, they lost and the judge told them that they both had to pay me child support directly (or take me in, but neither wanted that). I traded that for them to both sign my emancipation papers and to leave me the hell alone.

Of course, the story doesn't end there. When I was 19, and in my first year in the Marine Corps, she again tried to sue me. She had kept claiming me as dependent on her taxes, and since I was filing, she got audited. Somehow, she thought that this was my fault.

There's so much more that's wrong with this woman, but this isn't the right venue for it. I haven't seen her since 2002, don't know where she is, or even if she's alive.
My grandfather's first experience with internet porn was through the web browser on my Dreamcast.
Funny thing about that. At 14 my parents disowned me, so I moved out on my own and then in with my grandparents because I couldn't quite handle it.

My 'PC' and internet access was provided through a Sega Saturn with a Netlink (28.8kbps) modem. For homework that had to be typed up, I'd type it up via email, email it to myself, go to the library, and move it over to Word and format it.

My 1999 upgrade to a Dreamcast with a 56kbps modem and 16MB of system RAM was HUGE. Back then, I could not get a serviceable PC anywhere near the price of a Dreamcast ($199 at launch). And yes, I did own the DC keyboard and mouse for UT and Q3A. I was a console user, but not a peasant.

I'm a PC gamer now, but certain consoles will always be fondly remembered for getting me through a difficult part of my life.

EDIT: I always remembered the name of the guy who created the browser for Saturn and DC, Ken Soohoo. Just found out that he passed away 2 years ago :(

http://www.dreamcastlive.net/blogs/post/CEO-Founder-of-PlanetWeb-Kenneth-Soohoo-Passes-Away
What's your budget in your local currency, from that website?

Also, I'm calling it a night, so forgive me if my response doesn't come for a day or two. Wednesday into Friday I generally go semi-dark, or mobile-only on Reddit due to work obligations.
All displays under each designation are matte.

If you ever find a glossy FreeSync monitor, let me know. They don't exist and a lot of us have been looking for one.
That's a poor understanding or poor translation on their part. I wouldn't give it any credibility.

If you could have provided the link, I could have seen which monitors came up under each option to weed out what the differences were and what the retailer meant.
Just to add to this.

The additional speed might be offset by smaller memory bus, which is then partially offset by better delta color compression.

Gone are the days when the x60 product had a 256-bit memory interface, like the GTX 560. The 660/660ti/760/760ti had a 192-bit interface. The 960 was actually reduced to a 128-bit interface, with the 1060 returning to 192-bit.

Don't be shocked if the new x60 is 128-bit. Going from 192-bit 8gbps GDDR5 to 128-bit 12gbps GDDR6 is a wash in overall bandwidth. A higher speed (like 14- or 16-gbps) combined with improved DCC gives better overall/effective memory bandwidth, with some respectable cost savings for nVidia.

I would expect the 16gbps memory to be reserved for the x80 and even x70 class products, with 14gbps  at best for the x60, and 12gbps (or even GDDR5) for the x50 lineup.
This has routinely been one of the cheapest actual (not knock-off) Cherry MX-based boards, and it's a good one. It's basically a poor man's Ducky One at about half the price. And there's no shame in that.
If that's the case, then they did a poor job of listing it. Can you please post a link to where you're seeing that?


> Can anyone explain what anti-glare on a monitor means, and how is it different from backlit and matte?

> Is anti-glare matte or have I got the whole thing wron

Backlit means that a light shines behind the display so that you can see it. All LCD displays are backlit. OLED is not backlit as it is self-emissive (the pixels are their own light source, no backlighting needed).

Anti-glare (AG) means a treatment to reduce glare and reflection. Matte is one such coating. Semi-gloss is another. HP sometimes uses a chemical AG to retain most of the glossy look.
> In comparison to expensive office chairs, it provides the same level of support.

No, and you didn't read any of the articles that I linked to if you have a full reply 6 minutes after my post.

> And for sources...you want me to dig up the scientific papers on my chair that don't exist?

There's a reason why I'm able to show citations on why your chair is bad for you, and you're not able to find sources that contradict it. It's because that chair is bad for you.

We get it. You aren't going to be swayed by facts, even if they are backed up by a medical source. Your opinions matter more to you than actual evidence. I can't argue with faith.

But if you want to sway me, and change my mind, you're going to need to use logic and facts backed up by medical sources.
> that was my first post, as i was visiting my parents in wenzhou.

So, linking to an article about something in China proves that you're from China? Interesting, let me try.

[I'm an astronaut!] (http://www.sciencemag.org/news/2018/03/nasa-s-new-satellite-brings-search-earthlike-exoplanets-closer-home)

> but hey instead of talking about why i got banned, we're talking about the proof of my ethnicity.

I also pointed out why you got banned. You weren't able to refute it, so I didn't discuss it further. You followed it up with:

> I didn't do anything to warant a ban/mute.

Which doesn't refute what I said (you don't explain how your post wasn't slanderous, or a violation of the rules cited), and just re-states what you want to be true.

Some subreddits are echo chambers where your feelings are more important than facts. This isn't one of them. You were banned properly. Also, I'm not one to keep an argument going. If you can't address my points on why you were banned, I won't respond further.
> My chair has proper lumbar support and a great balance of firmness and comfort.

Is that you personal opinion that your chair has proper lumbar support? Or, do you have any citations to back up your claim?

This is an example of a good source, as they go over the specifics of ergonomics needed for a proper office chair. It's also written by a medical professional.

https://www.spine-health.com/wellness/ergonomics/office-chair-back-support

-----

This is an example of a bad source, because they are targeting gamers and providing Amazon referral links to profit off of gullibility. It's written by an individual with no medical background.

https://www.bestforbackpain.com/best-gaming-chairs-for-back-pain-reviews-top-picks-buyers-guide.html

They also hide their domain registration through Privacy Guardian, as it may even be written by an employee of one of the gaming chair companies. Something that has been [DONE BEFORE] (https://usa.clutchchairz.com/health-benefits-using-gaming-chair/). (It should go without saying, but that's also a bad source).

-----

> You're commenting on a chair that you don't know and are assuming it's "bad" like some others.

Same shape, features, and parts as the others.

You're arguing off of emotion. You spent money on it, so you WANT it to be good, so you argue that it is good. But the facts don't line up with your preference.

I also spent money on mine. I want it to be good. But the facts don't line up with my preference, so I choose to learn from my mistake rather than double down on it.

But hey, you do you :)
Nothing yet. But EVGA had previously stated an intent to update Z97, and other chipsets. This was, however, prior to the January updates being pulled.

Here's the quotes from an EVGA employee:

https://forums.evga.com/FindPost/2749683

> This is our current schedule for BIOS updates related to these vulnerabilities:
 
> By end of this week*:  Z170, Z270, Z370, X99, X299, Laptops.
 
> Within 1-2 weeks*:  X79, W888, Z87, Z97.
 
> Please note that this date is projected, contingent on receiving source code from Intel, and final testing may push the dates back temporarily.
 
> At this point, we do not have source code from Intel to update X58, P67, or Z77 motherboards.  When/if that becomes available, I'll provide an update.

And then more recently, from the same person:

https://forums.evga.com/FindPost/2755711

> Unfortunately, some of the BIOS updates have been delayed at Intel's request.  The list of boards is still accurate, although the release dates will be extended somewhat.
In 5 years, you have 11 posts in /r/Austria and zero posts in /r/China. You do, however, have one post in /r/Hongkong

A search of your most frequently used words shows that, as of the post above, you've used the word "China" 4 times over your entire 5-year posting history. The word "Chinese" is also used only 4 times, mostly from the rant that got you banned.

FYI, scraping a user's history is insanely easy. So it's best not to make claims that are easily disproven, such as you having posted in /r/China.
>  I know because i am chinese

* Your only post on a subreddit about Chinese-related news and issues
* You post in /r/Austria, not exactly a hotbed of Chinese nationals
* Your German is outstanding based on your post history (I'm not saying it's impossible to have a German-speaking Chinese Austrian, it's just EXTREMELY uncommon, 0.375% of their population, to be close to exact)
* Your first post in a Chinese-centric subreddit is to claim you're Chinese (doubtful, based on the above) and slander the Chinese

Not only did you warrant a ban (potentially violation of rules # 1, 2, 4, and 5), this is potentially solid /r/quityourbullshit material right here.

EDIT: [Just in case...] (https://web.archive.org/web/20180328184049/https://www.reddit.com/r/ReportTheBadModerator/comments/87rg78/rsino/)
> I wasn't the original guy who said his was very nice for posture

My apologies for confusing you with another poster.

> I really only have a vague idea of what my lumbar is.

Same.
How heavy was it? Did it hurt your back? Did you pull a Linus and drop it? Where did you set it down?

(just ignore me, I couldn't resist, I'll see myself out)
I'm not a subject matter expert, but my understanding is that it needs to be mostly firm, and properly contoured to your back. However, some softness is allowed to adjust to various backs (not everyone is the same).

Most racing chairs tend to be too firm or too soft. Even the ones that get it right in that department, the shape is still wrong.

You've got the right idea in replacing the stock pillow. Basically, you're doing something like [THIS] (https://www.amazon.com/Back-Cushions-Seat-Cushions/b?ie=UTF8&node=1069194).

Replacing the stock pillow is a far cry from your original post of "it's very good for posture." By replacing the pillow, you're acknowledging how bad it is in its stock configuration, and you're actively correcting for it.
> Wouldn't having to sit upright with lumbar support be good for you're back and neck?

It's good for your back, but these chairs do not have proper lumbar support.

I could place a rock behind my back and claim it's lumbar support. Doesn't mean it's proper lumbar support.
She may need to be reinflated.
Marketing is a helluva drug.

Gamer chairs are up there with gaming headsets and 1ms monitors. Gamers as a group are gullible and fall for some pretty absurd things.
And then she gets the chair in the divorce.
I have one of their base models, the SL-2000 or something like that. Their Trigger lineup looks interesting, but I suspect that the Office Master OM5 and/or Eurotech i00 would be better, and those options are generally less expensive.
Sit in it daily for a few more years. You'll start to feel the problem.

If you mostly sit upright, they are horrible for your back. They're only adequate if you recline most of the time.
The popular choice at that price point is the Ikea Markus. Try their website.
The same as gaming headsets.

Marketing - "If we make a cheap piece of crap, make it look cool, and markup the price, gullible gamers will buy it!"

And it works. I'd like to think that I'm intelligent, and I fell for it too. So my made up quote isn't making fun of you, or anyone in particular any more than I'm making fun of myself.
I'm currently choosing between the Eurotech i00 and the Office Master OM5. This will allow me a nice office-style chair that matches my setup's aesthetics in my preferred price range ($500-$750).

For my secondary setup, I'll get a used Steelcase Leap. They go for $95 at our local supplier.

I've sat in the Markus, and I'm not a fan, but I can see why it's highly recommended in its segment.
> and it's very good for posture.

It's not. You may personally like it for reasons your own, but these chairs are not good for for your back and neck.
Good question, and I'm not a subject matter expert to here's my best layman's explanation.

A chair needs to be contoured to support your back and neck properly. These chairs are not. It's as simple as that. Instead of being contoured to support you where it's needed, racing chairs are recessed in those areas, relying on pillows to act as support.

If the pillow is too soft, it doesn't give you adequate support. If it's too hard (as in the case of Vertagear), it doesn't allow you to sit comfortably and therefore, acts as inadequate support. In the latter case, the pillows sometimes go unused.

Racing chairs are designed to look good to appeal to gamers so that they can jack the price for higher profit margins. But they aren't healthy.
They probably use the same ODM. These days there are so many turn key solutions, almost anyone can make PC peripherals.
In my case, my wife doesn't care for mine either.
> But it does look comfy.

Generally, they're not. Thjey are bad for your back and overall posture for standard upright kb+m use. I do like them for when I'm reclining with a gamepad.

But if you want comfortable and healthy, a proper office/task chair is better for you.

I have a Vertagear and I'm going to unload it soon. Saving up for a new chair right now.
Most people who get one of these end up not having a girlfriend to make fun of them in the first place. So, you're already ahead of the game.

/s
I hate these chairs, they are horrible for your back in most typical desktop uses. Currently saving up for a nice office chair, then dumping mine.

Seriously hope yours works out better though. There are a LOT of people who claim to enjoy these, and I hope you end up being one of them.
His problem is that GFE is doing it. NVCP doesn't do it by default like GFE sometimes does.

Dummy :)
The article that you linked answers your question.
Just keep in mind that the Moto 360 v1 and v2 are two completely different animals. The v1 is on Android Wear 1.4 and the apps work differently. At any time Google can remove automatic app installation from apps, thus partially negating their AW 1.x compatibility.

If you have the v2 or Sport, you're fine, even if it's not getting the full wearOS rebranding package.
> If I pair my watch with iOS, how does Google Fit work?

Right now, not well at all. It's more dependent on a companion Google Fit app on a smartphone, and it's not out on iOS yet.

[More info here] (http://www.mobihealthnews.com/content/reports-google-fit-coming-ios)

> Does it send data to the apple health app?

Apple would never allow this.
If you use GFE to manage you're games settings, it will do this periodically.

I'm in the camp that doesn't install GFE, so I don't have this issue.
If you can stand the performance hit, use 2x or 4x DSR with 0% DSR Smoothness. That should net you a cleaner look with minimal to no impact on text and fine UI elements.
And NOW it's gone.
It's not expired. The link takes it to a 3rd party seller, but Amazon still sells it for $69.99.
That's the first I've heard that, but it makes sense. Because your watch isn't slated for the WearOS OTA, they may have decided to keep the Versions screen as Android Wear.

To be clear, your watch is end-of-life, Motorola has bowed out of smartwatches, and there is no planned WearOS OTA for your watch. At least for now, there's no loss in functionality, just a loss in branding.

I'm actually wearing my Moto 360 v1 right now, which is stuck on Android Wear 1.4.
Correct. So, you'll get the WearOS rebranded app upgrade for your phone and your watch. The OS version on your watch will report WearOS 2.10. You won't get the OTA that changes it to WearOS 1.0, nor the bootup animation.
I currently game at 3440x1440 on a GTX 1060 6GB. It's fine for medium/high settings, 45-60fps for newer titles. For the games I'm primarily playing now, I'm getting:

* World of Warcraft - Maxed out, 70-90fps in environment, 100fps (V-Sync on) in simpler areas, haven't seen a drop below 60 yet though I'm sure it's happened.
* Tomb Raider (2013) - Maxed out it's 45-55fps throughout. Disabling TresFX brings it to 65-75fps. Moving down another tier to High makes it 90-100fps.
* Doom - Maxed out I got 45-60fps throughout.

There are more demanding games and you'll need to drop to high or even medium. You won't need low. You got a decent monitor that can last you a long time. Just use it and, if you find your GPU lacking, get an x70 next gen for a nice boost.
Just wanted to explain what's going on here.

Most games separate their 3D rendered layer from the 2D UI overlay. If you use in-game FXAA or SSAA, it ignores the UI and only applies to the game itself.

But if you force FXAA or DSR via the NVCP or GFE, it applies it to the entire screen, causing loss of detail and blurriness in text and fine UI elements.
You're not going to like this, but you were actually in the wrong per that sub's rules.

On their sidebar, the first rule states:

> Refrain from posting jokes, memes, puns, reaction gifs and wordplays. Comments should contribute to thoughtful and serious discussion only.


That sub has almost half a million subscribers. That means that their moderators have to go through a LOT of posts, and a lot of users. It's more efficient for them to remove and move on than to try to rehabilitate.

From their perspective, a user like you who makes one drive by post that immediately breaks rule #1 is no loss to their community, so they won't waste much time on you. You and their community aren't a good match, so they've removed that option from you.

Again, I know this isn't what you want to hear. Every community has its own rules.
I've never been to an NFL game because I'd never want to subject myself or my family to that kind of environment.

And I'm an Eagles fan.
> I made a pun joke that I don't mean to

You didn't mean to? Was your account hacked? Who made you make that post?

> The Moderator is bias and not performing well filtering the most important stuff to ban.

Sounds like the moderator did his/her job very well.
So... In theory of we as a group we're to upvote numerous TD posts daily, they all occupy numerous pages of r/all, thus reducing ads and Reddit's revenue?

We can use this against them.
A Steam Link is going to have a subpar experience for this. Geforce now introduces lag because you're playing a stream (game is run on remote server, video recorded, compressed, sent to you). This entire process plays out again on your PC so that it can stream to your Link. The lag is going to be extremely noticeable.

Your best bet would probably be to cut out the middle-man and use an Nvidia Shield. However, I don't know how the Now beta on PC differs from the version available on the Shield.
It's not water proof but is listed as being dust/splash resistant, so likely the same IP67/68 as most other Android Wear/WearOS devices.

One thing that people tend to confuse, but IP certification is NOT water proof.
Wife and I play World of Warcraft on and off together. We've been doing it for years and taking our time to enjoy each expansion at our own pace. Some days we only play 15-20 minutes.

You can enjoy a game for any amount of time, so long as your goal isn't too max everything day one.
>  I have all settings the same on both. 

Every monitor varies, even within the same product range. Making the settings match won't give you the same output. That's why you don't use ICC profiles provided by third parties.

If you want them to match exactly, you'll need a colorimeter. If you want to go that route, your best bet on a budget is the Spyder 5 Express (~$120), and download the DisplayCal software (free). It will make them match, easily.

If you want to avoid that, you can use Wndows' display calibration. Just hit Start in Windows and start typing the word 'calibrate' to bring it up. Alternatively, you can use the [Lagom website] (http://www.lagom.nl/lcd-test/) to eyeball it up.
ViewSonic XG 2401/2402 for a TN option at a good price. Basically, a great compromise between performance and value.

Samsung C24FG73 for better picture quality at a slightly higher price (VA panel, higher contrast, limited HDR). Downsides are some ghosting at higher framerates/refresh rates, and a more limited FreeSync range.

Both monitors support AMD FreeSync.
24" or 27"?
It doesn't, unfortunately. OP already answered this when someone else suggested it a couple of hours before you did, but I'll answer your suggestion since I have the same issue.

While it will pin/lock the app, it won't block input. It doesn't stop my daughter from pausing, fast forwarding, or even exiting the video but staying within the app.
Fingers crossed :)

Not the best comparison, but Userbenchmark shows the 8400 as 10% faster in single thread than the 1600x. A boost of 200mhz and ANY IPC gain would put that within margin of error.
Which is why it's so hard for them to know if a baby had shit on the floor.
Moto 360 v1 or v2?

The actual WearOS update is an OTA. The WearOS app on the watch only updates the home version. If you don't get the OTA, the WearOS app will update you from Android Wear 2.9 to WearOS 2.10. If you do get the OTA, you'll be at WearOS 1.0, with a Home App version of 2.10.

That said, you're not getting the OTA. If you have a Moto 360 v1, you're stuck on Android Wear 1.4.

If you have the Moto 360 v2, you'll get the WearOS app update on the watch, which will tell you that you've upgraded to WearOS 2.10. But you'll still have Android Wear branding on the bootup animation. Motorola has discontinued support so there won't be a WearOS OTA.
In games where you are GPU bottlenecked, you will notice little to no improvement. In games where you are currently CPU bottlenecked, you'll notice improvement.

I would, however, consider waiting for the Ryzen 5 2600/X CPUs. They should offer comparable out of box performance at a comparable price, with the benefits of being overclockable and having multi-threating (6c/12t instead of 6c/6t).

But if you can't wait, the 8400 is an excellent option.
It uses an 8-bit panel with no FRC support and targets the sRGB color space (narrow gamut). I'd be shocked if it used anything other than W-LED. RGB LED and GB-r LED are used for wide-gamut displays.
> It's not an incorrect comparison at all.

It's an incorrect comparison. It compares a partial spec listing, not actual performance. If you continually go back to this for comparing GPUs, you're going to continually have bad information.

>  It's comparing the 3 gig version of the 1060, the one you linked was comparing the 6 gig version.

Fair enough, but most of us are talking about the 6GB version. The 3GB version is a cut down card.

> I'd agree the 1060 6 gig is better than the 970.

So is the 3GB verison.

https://www.techpowerup.com/reviews/MSI/GTX_1060_Gaming_X_3_GB/26.html
That's an incorrect comparison. It's just charts showing raw specs, but only part of the truth.

For example, the 980 has higher memory bandwidth, but the 10-series has better DCC (Delta Color Compression) which negates the need for higher memory bandwidth.

Look at actual gaming results. Here's TPUs composite of 16 games:

https://www.techpowerup.com/reviews/NVIDIA/GeForce_GTX_1060/26.html

The 1060 comes out 1-2% faster at all resolutions. Do note, however that this is reference vs. reference. The 1060 boosts higher, meaning that the 980 benefits more from an actual overclock.
> How long does the battery last on the skagen?

Haven't used one, so unsure. But the LG is known for having a smaller battery than almost all watches.

> Also what is a scrollable bezel? The LG watch style had a scrollable dial, is it the same thing?

No. You can run your finger round the edges of the display in a clockwise or counterclockwise direction to scroll.
> how are the XG2402 colors for a TN panel

Typical for a 1080p, 6-bit+FRC, TN panel.

> would it be noticeably worse compared to a VA panel? Thanks!

Side by side, definitely. But for general use, you likely wouldn't notice or care too much.
For me, I'd try to get the Pixel 2. I'd want the phone to last as long as possible, but this isn't the typical 1-year gap.

The Pixel 1 was guaranteed 2 years of OS updates. It will get Android P (likely 9.0), and possibly any 9.1 upgrade as well based on prior generations. But then it's 1 year of security patches only.

The Pixel 2 is guaranteed at least 3 years of Android software updates, and is a year newer. That means Android P, Q, and R (presumably 9, 10, and 11).

You're talking about an additional 2 years of software updates. For some, that's a big deal, and it was a deciding factor for me.
They have the same feature set. The main differences are cost, styling, and battery life. And in those cases, no one here can tell you which to prefer in terms of styling, or what your personal budget should be.

But I think we can all agree that if it's otherwise a tie for you, the Skagen should have longer battery life :)

EDIT: And I believe that the Skagen has that scrollable bezel.
Possibly, but even if you're wrong, the 980 benefits more from an OC anyway. Lower base clock and more CUDA cores.

Reference to reference (the link that I provided), shows the 1060 being 1-2% ahead. But aftermarket models only gain 2-3% over reference for the 1060. The 980 sees gains as high as 10% from aftermarket models. And that's before additional overclocking from the consumer.

So, stock vs. stock, it's a tie in performance, except in memory limited scenarios (1060 wins there), and that the 1060 consumes significantly less power. But the 980 does get to push slightly ahead, I'd estimate 10% at the furthest extreme (not enough to push it into 980ti/1070 territory though).
> Even with GPU's it's good for just a quick idea of GPU tier, especially within the same manufacturer / generation where the synthetic test comparisons scale fairly well to real use performance.

I agree. But I do give their CPU results more credibility due to CPU testing being (generally) less complex AND their GPU results use weighing based on unknown criteria as well as self-reported numbers. It's too subjective.

> Of course, before spending that much money on a card, you should lookup benchmarks for the type of titles you will actually use it for.

It is amazing how many people miss this simple idea...
Looks like the the mayor from 'Cloudy With a Chance of Meatballs"
CL 19 out of the box, CL15 when you enable XMP.

I would hope that gamers buying faster memory would know how to do that by now, but then I keep getting surprised. Friend bought DDR4-3200 and it was running at 2133 the whole time because he didn't enable XMP.
Somehow I botched the link (fixed above).

Problem with your link is that he compared the 980 Classified (one of the highest OC'd models) to the Gigabyte G1 Gaming 1060 (one of the least OC'd models, which has near-reference performance). The (now corrected) link that I provided is apples to apples.
> (incl. The horrible Herman Miller Aeron)

> It's so refreshing to find someone else who hates that terrible chair.

The problem with ergonomic chairs is the human body itself. We have so much in common, but there's still a lot different about us. The Aeron is designed to fit certain people like a glove, and for those people, it's like sex in chair form. But for everyone else, it's pure pain. These chairs are meant to be a love it or hate it ordeal, and I would never buy one without trying one.

It's like the top sports teams in each league. The teams with the highest number of fans also tend to have the highest number of haters.

To be clear, you're not wrong at all for disliking that chair, so long as you also are aware that it fits a lot of people. Hopefully, you find the chair that fits you (and hopefully I find mine as well!).
>  As for FG70 vs FG73, any big differences?

Limited HSR (FG73) vs. no HDR (FG70). Also, the FG73 has the improved scaler. It was the GF70 I was talking about when I said:

> but it is improved over their prior generation monitors, which was 120-144hz on the standard engine.

So, it's 80-144hz vs. 120-144hz range in favor of the FG73. Both are 70-144hz with the expanded range, but that's less of a stretch over the default setting for the FG73. I'd trust is more in terms of potential flickering issues.

They also appear to be close enough in price that I'd go with the newer model anyway.
> It's a bit better than the GTX 1060

https://www.techpowerup.com/reviews/NVIDIA/GeForce_GTX_1060/26.html

They're on par. The 980 gains more from an OC, but has less VRAM and more power draw.
The rough rule thumb is good for ballparking, and one that I use frequently. But Maxwell to Pascal didn't work out that way across the board.

https://www.techpowerup.com/reviews/NVIDIA/GeForce_GTX_1060/26.html

Here, you can see the 1060 being 1-2% ahead of the 980 in all resolutions (this is an average of 16 games tested). The 1070 is actually ahead of the 980ti.
> 980 is definitely better than a 1060

So long as we ignore actual composite benchmark results, sure.

https://www.techpowerup.com/reviews/NVIDIA/GeForce_GTX_1060/26.html

That said, that's stock reference vs. stock reference. The 980 gains more from overclocking than the 1060. AIB to AIB with a typical overclock, I could see the 980 being 3-5% ahead. Still same ballpark though.
I wouldn't recommend using Userbenchmark for GPU comparisons. They're solid for CPU, but their GPU comparisons are a mix of weighted synthetics (not a real world scenario) and self-reported framerates for games.

Here's a real world test.

https://www.techpowerup.com/reviews/NVIDIA/GeForce_GTX_1060/26.html
I wouldn't recommend using Userbenchmark for GPU comparisons. They're solid for CPU, but their GPU comparisons are a mix of weighted synthetics (not a real world scenario) and self-reported framerates for games.
> Also I think a 980 is better than a 106

They are within a few percent of each other at stock. The 980 gets more from an overclock (lower base clock combined with more CUDA cores), while the 1060 has more VRAM.

There are maybe a handful of games with maxed texture settings that can give the 1060 a decent lead over the 980. Otherwise, they're about equal.

https://www.techpowerup.com/reviews/NVIDIA/GeForce_GTX_1060/26.html
> are you sure it has the LFC thing to save me from tearing? 

It has two settings for FreeSync range; Standard Engine (the default option), and Ultimate Engine (sort of like a monitor overlock, your results will vary).

* Standard Engine | FreeSync Range of 80-144hz, LFC Off
* Ultimate Engine | FreeSync Range of 70-144hz, LFC On

Because the Ultimate Engine is not a guaranteed result, YMMV. The range and LFC will work as advertised, but you may experience issues with flickering depending on the GPU, the game, etc. This is because Samsung seems to be using a cheaper scaler than other companies, but it is improved over their prior generation monitors, which was 120-144hz on the standard engine.

If you use the Ultimate Engine, LFC will prevent tearing when you fall below the range. However, the monitor's manual notes a potential glitch. If you change certain settings (resolution, v-sync, fullscreen vs. windowed), it may cause LFC to glitch out and tearing to occur until you reload the game. The manual recommends you turn freeSync off, then adjust the setting that you want, then turn freeSync back on.

That's what I've gathered from the manual and a few reviews. Hope that helps.
> Why the downvotes?

There are a lot of people who have fallen for the standard racing chair hype. And when people point out how bad these chairs are for your back, some get defensive over it. An attack on these chairs is perceived as an attack on the intelligence of those who bought one.

Not everyone is like this, mind you. I've been using a racing chair for a little over a year and I'm looking to go away from it. Like many, I fell for the hype. Unlike the people that I'm describing, I realize my error and have learned from it.

Anyway, I'll add this to my list of contenders. It looks interesting. Their website could use some better design though. I like being able to see what the chair looks like as I select the options, and they don't allow for that. It is very customizable though.
You need multiple updates. See This post - https://www.reddit.com/r/WearOS/comments/86i9fz/from_android_wear_29_to_wearos_210_to_wearos_10/
It's just the way that they number it now.

On Android Wear? Your WearOS app from the Play store determines your version number for "wearOS."

On WearOS 1.0? The same version number that YOU are seeing for WearOS (2.10.x), WE are seeing as the Home App.
Not OP and not the same arm, but I can tell you what I'm using for my AW3418DW.

I'm using an Ergotech Freedom Arm, and the Alienware is right at the upper weight limit. It hasn't given me any issues other than making me run it near max tension, but I'd recommend the Freedom Arm HD just in case. Because some subs frown upon referral linking (and I use Amazon Smile), I won't link, but just look up the following part numbers on Amazon and you'll get the right item.

* Freedom Arm (grey) = FDM-PC-G01
* Freedom Arm (silver) = FDM-PC-S01
* Freedom Arm HD (silver) = FDM-HD-S01

>  But what would you recommend me to get? The 24CFG73 or the XG2402, considering what I said above.. also, I heard there were ghosting/purple artefacts on the Samsung (maybe the 70 version), are those still there? Thank you!

Here's the thing, everyone is unique in what becomes a deal breaker. I don't notice ghosting that much. Ghosting wasn't an issue for me on the C34F791 despite numerous reports of it. But viewing angles on a TN really trigger me. So generally I avoid TN and go for IPS/VA, and I've come to appreciate IPS more than VA lately.

If you really can't decide, consider this. Wait until your credit card billing cycle is up. At the beginning of the new billing cycle, buy both from different stores with solid return policies. Return the one you like the least.
> which in some games would be under the Samsung's Freesync range (70-144Hz), would that be a problem or is it still good

LFC addresses this.

Let's say you're at 75fps. That's within range, so the monitor runs at 75hz. But what if you drop to 60fps? LFC takes over, and it runs at 120hz (it displays the frame twice). This works all the way down to 1fps. Where things begin to look less than fluid is subjective and up to you (I start to notice below 40fps).

> there are no reviews on the XG2402 so I'm not sure whether to get it or not.

https://pcmonitors.info/reviews/viewsonic-xg2402/

They had issues with static contrast with their unit, but this is typical variance and we had some uncommon reports here on this sub with XG2401 users (and other monitors as well). YMMV.

> Ok but what are the chances that rev a05 comes with little to zero ips glow?

I'd put it at 0.01%. Off-axis panel glow is something that impacts all panel types. The more light they let through, the better their viewing angles, the more they glow off axis. So, IPS/PLS/AHVA will always have this issue. It can be mitigated by proper viewing distance, posture, proper brightness level, and bias lighting, but it won't fully go away.

Now, backlight bleed is a different issue. That's caused by poor craftsmanship, poor build materials, poor quality control, and/or physical damage (IE, dropped too hard by courier). And the AW3418DW is definitely not up to the same build quality as the Ultrasharps that I've handled.

And the chances of Dell/AW upping their build quality with the new revision? Given that they never did with the S2716DG, I don't expect them to here either.
I used to be against gay marriage. But then I stopped watching Fox News (I was in the military at the time and it was the only channel on at most office locations), and did my own research.

I began to see it less as a religious institution, and more of something co-opted by the government for the sake of benefits (taxes, healthcare, estate planning, etc.). And to deny homosexual couples access to the same benefits, the same love, and heck even the same misery, would be completely discriminatory. And besides, what a homosexual couple does has zero bearing on what my family does.

At the time I was also transitioning from Christian though Agnostic, to ultimately Atheist. But even before I completed the transition, I realized that being against gay marriage was also against Christianity. The Old Testament has one verse on the subject. Jesus Christ nullified that. Christians go off the New Testament, which says nothing on the subject.

People using Christianity to justify their feelings towards gay marriage and the LGBTQ+ community in general are just using a misinterpretation to justify their stance. And Christians ignoring the New Testament in favor of the Old are literally saying, "No, God, I think that I have a better plan than you do."

I think the point that really made me change was when my office (a radio repair shop in Camp Lejeune) had a debate on the subject. When I expressed a "live and let live" attitude that was more neutral at the time. I was attacked, verbally, by the majority of the people present. People that outranked me. I was called a faggot, a traitor to the country, and was subjected to further harassment and retaliation. I decided that I didn't want to be associated with that level of hatred.
The manufacturer actually has no control over it. It's all in AMD's driver.

If range = 2x+1 over minimum, the driver enables LFC.

My only concern with these monitors, and I should have mentioned it in my first post, is that they have lousy scalers. That's why they have a standard range and an extended range. The extended range is similar to an overlock, and results are not guaranteed. Many users will experience flickering depending on the GPU, game, phase of the moon, or scaler lottery. I'll edit this into my OP.

But LFC won't be the issue :)
> My only regret is that you're assuming this by comparing some other similar model.

I'm not assuming anything. I've done in-depth analysis on this before, even to the point of getting direct feedback from AMD leadership and monitor manufacturers.

AMD's driver automatically enables LFC once it detects a FreeSync range of 2x+1. You can even force it to enable by using CRU to adjust the range on a non-LFC monitor. The driver does this automatically.
> 1440p looks nowhere near the same as 4K, even on "traditional monitors".

> I have a 1440p and 4K monitor side by side on my desk.

As have I, in my testing. At 27", 1440p and 4k look very similar. Yes, 4k is sharper. But not mind-blowingly. The jump from 1080p to 1440p at that size is more noticeable.
It's much sharper. However, it's more demanding on your GPU. I was gaming at 4k on a GTX 1060 and an RX 480, but those were older, simpler titles. For modern titles, you won't be able to do that. You'll have to run at a lower resolution which leads to interpolation (blurriness).

Also, 4k makes the desktop UI too small for most conventional monitor sizes, which means you'll need to enable scaling, which leads to mixed results.

If you want a sharp picture, a 1440p display is the ideal compromise. It looks nearly as sharp as 4k for conventional monitor sizes, requires far less horsepower to run, and won't give you scaling issues since it's ideal to run at native resolution in those sizes.
Just to add to this - it was ANNOUNCED as a 120hz display, but when it was finally released, was only 60hz. Basically, Dell had given up on it due to unreported issues, delayed it for awile, led us to believe that it was cancelled, then released the watered down version out of nowhere.
Nor should you.

1. Most subreddits have rules about messaging the mods directly. That's what modmail is for.
2. That user hasn't appeared active for about 5 days, so I wouldn't expect a response.

But if you continue down this path, you're in trouble. You see, moderators can ban you from a single subreddit, and they can mute you (for 72 hours) from their modmail. But if you circumvent both of those by direct messaging them?

That's a form of harassment and the Reddit admins (the people who run the entire website, not some subreddit) can ban you from the entire website. And when you make an alt account? That will quickly get banned as well (mainly because people like you fall into old habits quite easily).
Based on what I'm seeing:

* you were harassing the moderator
* that mod made an attempt to explain to you what you were doing, and you either failed or refused to grasp it
* You continued to harass the moderator
* Mod removed a cancer from the community

You see yourself as the victim when you were actually the aggressor. And while you pretend to not be aware of it, there's undeniable evidence that you are aware of it.

You see, when a moderator "removes" your post, it's not actually removed. It's just hidden there, but viewable from your profile...unless YOU deleted it after the fact. And you've deleted all of your comments from that sub from the last 2 and a half weeks. I checked both your history and that mod's history. His posts that you took a screenshot of are still there after 1 day. Yours are gone, deleted by you. You also deliberately didn't show all of your comments.

You knew that you were in the wrong when you made this post.
> Are you sure about LFC applicable for Samsung? AFAIK it should have max 70-144hz v.range and that 70 is a lot higher than my taste accepts. Since It seems it's not reaching the required 2.5x ratio LFC calls for I'd be in doubt it can achieve that.

Yes, I'm 100% positive.

While AMD recommends 2.5x for an optimal experience, in actuality LFC automatically enables when its 2x + 1. So at 144hz, a range of 71-144 would get the job done. I suspect there COULD be issues, however, as it switches between standard range and LFC.

I've had one monitor on my desk like this, the C34F791. The stock range was 80-100hz, but the extended range was 48-100hz with LFC. I did test for issues in the 40-60fps range and had no problems. The display did flicker when using the expanded range in some games, but that was a known problem with this monitor.
Best picture quality - Dell SE2717H

Best value - ViewSonic XG2401/2402/2701/2702

Best overall - Samsung 24CFG73/27CFG73

The Dell offers a 27", 1080p IPS panel, giving you the best overall image quality. It's limited to 75hz, with a range of 48-75hz and no LFC. It's also generally around $150, give or take.

The ViewSonic models are 24"/27" 1080p, 144hz, with extended FreeSync ranges and LFC support. They use TN panels and are generally ~$250 for the 24" and ~$350 for the 27", giving them great value and performance at the expense of image quality.

The Samsung options are similar to the ViewSonic, in that they are high refresh with LFC support. They also offer limited HDR support, and use VA panels for better contrast, though motion handling is weaker. They're typically priced in the same ballpark as the ViewSonic models these days.

These are the preferred compromise by many, as you get near-IPS picture quality, better contrast, and limited HDR support, but you are likely to experience ghosting. But they do have a major issue. They have two FreeSync ranges. The default option is extremely narrow to the point of being laughable. The expanded option is sort of like an overclock in that your results are not guaranteed. While the expanded option will enable LFC, it can cause flickering depending on the GPU used, game played, your luck (a lottery), and other factors.
> The only difference is this mouse is white and can be used on the charging mousepad (I have no idea what it's called off the top of my head). 

The G703 also uses new switches for the L/R mouse buttons with higher endurance (there's a slightly different feel to them, but you get used to them quicker). Also, the 703's wireless connection is supposedly lower latency. I can't confirm as I went from a wired 403 to the 703.
I'm really impressed by the (rumored) improvements that AMD has made here. It's going to be really hard for me to not upgrade to Zen 2 in 2019 if it's a similar improvement over Zen+. Because I'm on a 7700k, my goal was to hold out until 2020/2021 for a larger upgrade and DDR5. But, part of me wants to further support AMD (and hope that some of that CPU money trickles down into GPU R&D).

If the Zen 2 '3600X' offers similar or better IPC/clocks as my 7700k, I'm going to be seriously tempted. AMD is doing the mid-range right.
Look for the label, like this: http://images.hardwarecanucks.com/image/akg/Monitor/Dell_S2716DG/ports.jpg

In this case, they have an A00 (the last digits on the bottom number). A00 is the original version, or possibly a review sample, A01 is the first revision.
In some cases, yes. But most of the time companies find a way to slightly reduce costs by switching to a different supplier for one component, without harming the product's capabilities.

For example, Nvidia typically launches their GPU's with more expensive Samsung memory, because Samsung generally does a better job with wider availability. They then start mixing SK Hynix RAM in to reduce overall costs. The product will still meet the same advertised spec, and most users won't notice a difference, though Samsung's memory typically overclocks better.

So, they won't cheap out and weaken the product below its advertised spec. Normally.
It's a very common clause. If they don't give justification, it's harder for the customer to allege discrimination.

Taken to it's obvious conclusion, if a certain cake baker hadn't disclosed a specific reason for declining service and had used the more vague, "we reserve the right to refuse service to anyone," he wouldn't be in court (NOTE: I am not justifying the behavior, just using an extreme example to show how these clauses are applied and the reasoning behind them).
Fixed, thanks.
There is no "best" monitor as every display is a series of compromises. It's all about finding the right fit for you. Let's start with the basics.

* What's your ideal budget (amount you'd like to spend)?
* What's your stretch budget (absolute max)?
* Preferred aspect ratio (16:9, 21:9)?
* Screen size?
* Resolution?
* Pre-calibrated?
* Refresh rate?
* G-Sync?
* Panel type?

I'll give you some examples to show you how much some of this costs.

* Standard 24"/27" 1080p IPS/VA panel will run you $150-$200
* A 24" 1080p 144hz display can run you ~$250 for a decent one, $300+ if G-Sync is added, and these are TN panels
* A 1440p 60hz pre-calibrated display is $400 (most gaming monitors are not pre-calibrated, so this is an either/or thing most of the time)
* A 4k 60hz pre-calibrated IPS can run you $400-$800 depending on features, no G-Sync
* A 1440p high refresh IPS panel can run you $450-$500 for a quality one, $600-$700 with G-Sync, or $400-$450 for TN
* A 4k 60hz G-Sync is about $800-$1,000
* 2560x1080 34" ultrawide IPS 144hz with G-Sync is $600-$700, $100-$200 less without G-Sync
* 3440x1440 34" ultrawide 100-120hz G-Sync IPS is $1,000 or more.

So there you go. That should give you an idea of what's out there, and the costs associated. Give us an idea of what you want, and we can talk specific monitors.
If you can get a 1080 at the listed MSRP of $549 from Nvidia's site tomorrow, I think it's worth considering. Here's my rationale:

* We don't know when the new x70/x80 will launch, bur rumors span from April to September
* Pricing is unlikely to be cheaper than Pascal, which launched at $449/$699 (FE), respectively. They'll likely cost more. Imagine $499 for the new x70, and $749 for the new x80 as possibilities.
* And that assumes you can get one in the launch window for MSRP, not a guarantee in any launch window, but even harder if miners make a push.
* And finally, Pascal over Maxwell was a larger upgrade than usual. I wouldn't expect the new x70 to hit 1080ti performance, as the 1070 roughly matched the 980ti.

Overall I'd prefer to wait, but if you really can't stand gaming on your current setup, a 1070, 1070ti, or 1080 at MSRP are acceptable alternatives.
For the most part, a different build date but same revision number should indicate a similar overall product.

A new revision could mean minor tweaks to the firmware, hardware, or calibration to the display. These are usually smaller tweaks not worth releasing an entirely new model for. And while tweaks to earlier revisions will generally improve on quality, latter revisions tend to focus on cost cutting to improve margins.

The S2716DG, for example, just hit revision A09. But the biggest noticeable confirmed change happened around A03 or A04 when they switched to a lighter matte coating.

CC for /u/allamerican37
Any plans to allow flair like on our sister sub?

EDIT: I'll take the downvote as a no.
Depends on the game, really.

Right now I'm playing World of Warcraft with G-Sync and V-Sync on, and staying at 100hz rather than the 120hz OC. Settings maxed, 65-80fps most areas, rarely dips below 65, confined or simpler areas locked to 100fps.

I tried out Tomb Raider (2013) with everything maxed the heck out, which is very demanding, and was 45-60fps throughout. That was really surprising. Turned off the hair and jumped into the 75-85 range. Moved down one tier to High and was at 90-100fps. Aside from her ponytail, the game looks visually similar (most settings above High in most games give little visual boost for heavy GPU demand).

I mean, 90-100fps at High on a fairly demanding title is good to go in my book. I would suspect that in more modern games, a playable 45-60fps should be achievable at medium/high settings. Also, two more (off memory, no longer installed).

Doom Demo, maxed out, Vulkan, 45-60fps.

FF XV demo, maxed out, no gameworks, didn't measure the fps but was very playable. I can feel stutter under 40fps, so I'd estimate 45+. But that was only the tutorial (small, enclosed room), so don't take that as gospel. I didn't have time to really get into this.

Bottom line is that the 1060 will be fine in most modern titles at that res at medium+ graphics.
> Yes you can run that and any lower resolution

Not out of the box on all displays. My Alienware AW3418DW, for example, doesn't support 2560x1080. You'd have to set a custom resolution.

https://www.reddit.com/r/Monitors/comments/86hrju/can_a_34_inch_that_is_3440_x_1440_be_used_as_a/dw59rqu/
Not all 3440x1440 displays support 2560x1080 out of the box, so you may need to set a custom resolution. Here are the resolutions supported out of the box on my Alienware AW3418DW:

* 3440x1440
* 2560x1440
* 1920x1440
* 1920x1200
* 1920x1080
* 1680x1050
* 1600x1200
* 1600x1024
* 1600x900

Anything not on that list needs a custom resolution. Also, FYI, I run it at native res on a GTX 1060 :)
> I appreciate that, but is 10% of the cost of the monitor going to get me a 10% improvement in image quality? That’s what I’m not sure about.

It's a fair question. Out of the box, the gamma on this monitor varies WILDLY. Ideal is 2.2. Anything lower will be brighter or even washed out. Anything higher will be darker or even crushed. I got 2.46 out of the box on mine, Lim's Cave got 2.4, and TFTCentral had 2.6 on theirs, which is just too dark.

There's no gamma control on the monitor, so if you get a bad one and don't like the image, there's not a lot you can do outside of adjusting your GPU's settings.

If you like Lagom, but want something simpler, try Windows Calibrator. Just click Start and begin typing "calibr" and Calibrate Display should come up.
Probably cheaper to just replace the fans with ones you like, to include Noctua Chromax.
No USB3 header next to the socket!? Your mother is a better motherboard engineer than the guys at EVGA!
Typical. Ignores a nice guy like me and goes for an older Chad. Gold digger.

/s
Need for Speed: Most Wanted (2005)

I recently played through it again. In order for it to be enjoyable, you NEED the cracked version, with the widescreen and controller fixes. It's still buggy at times.

If they ever release a proper remaster, I'd actually consider an Origin account.
Here's the beauty of a G-Sync display. Are you ready for this?

**You don't need to maintain a frame rate equal to your refresh rate!**
It amazes me how impervious you are to fact, logic, and reasoning.
> I'm not arguing over whether it's significant, or a "real world" benefit, but it is a fact that 50% load is more optimal for a power supply.

In what way?

Service life? Nope.

Power consumption? Yes, but to an insignificant amount. I've already shown that it makes no financial sense to do that.
There isn't. EVGA, for example, rates their power supplies as MTBF of 100,000 hours at full load at 50°C. That's 11.42 years before expected failure, if run at full load 24/7 in a hot environment. Some newer PSUs are rated for more than this.

Regardless of what myth(s) you've heard, there are zero real world benefits to running your PSU the way that you've suggested.
So long as you're running at or below 100% load, that's optimal. There's really no benefit, contrary to popular myth, to running at a lower load.
> I mean to be fair, a power supply actually runs at its peak efficiency at about 50% load.

Sure it does. But the difference in efficiency from half load to 80% load or 20% load is typically in the 1-2% range. Heck, let's look at the most recent review on JG's front page.

Cold Testing - http://www.jonnyguru.com/modules.php?name=NDReviews&op=Story3&reid=545

Load|Efficiency
:--|:-- 
10%|90.4%
20%|92.8%
50%|93.0%
80%|91.4%
100%|89.2%

And hot box testing - http://www.jonnyguru.com/modules.php?name=NDReviews&op=Story4&reid=545

Load|Efficiency
:--|:-- 
10%|90.5%
20%|92.9%
50%|93.0%
80%|90.6%
100%|88.0%

That's actually a more extreme difference then I'm used to seeing from their reviews, but it's still not bad. So, let's look at their 50% load (roughly 750W). In hot testing, it's 93.0% efficient, drawing 802.7W from the wall. For comparison, let's see their EVGA G3-750 review - http://www.jonnyguru.com/modules.php?name=NDReviews&op=Story4&reid=500

In a hot box, at full load (roughly 750W), it was 87.7% efficient, drawing 858.3W from the wall. Even though the loads weren't exactly the same, we'll use the numbers since they favor your theory more than mine.

So, using a 750W PSU instead of a 1500W PSU would cause you to use 55.6W more under a roughly 750W load.


At Newegg, the G3-750 currently costs $120. The Silverstone Strider from that review is $520 after conversion. So, $400 more to get that extra ~6% efficiency. Let's see how that stacks up.

https://www.npr.org/sections/money/2011/10/27/141766341/the-price-of-electricity-in-your-state

Per the above source, the average cost of electricity in the US is $0.12 per KWH. The low is $0.08 (Idaho), and the high is $0.332 (Hawaii). Assuming 3 hours per day at that 750W load, that's 60.882 KWH per year. Let's see the annual cost of that additional power lost to inefficiency.

* Idaho - $4.87
* US AVG - $7.31
* Hawaii - $20.21

Now, let's see how many years you'd need to run that more efficient PSU to cover the additional up front cost.

* Idaho - 82.14 years
* US AVG - 54.72 years
* Hawaii - 19.79 years

So yea...If you REALLY think that it's important to own a PSU that you run at half load for a few percentage points of efficiency...you had better plan to keep that PSU for a LONG time.
For that size and price range, my recommendations would be the Acer G257HU (~$260), or the Dell U2518D (~$300). They are both 25" (no 24" option that I could see), and IPS. The Dell is on sale at Newegg for that price through today only, but shop around, of course.

The dell has an ergonomic stand, and the Acer is tilt only.
I have an LG Watch Style with Oreo, WearOS 2.10, and the Android Wear bootup animation.

It's incredibly inconsistent at this point.
It really depends. The 1050 and 1050ti do use the same chip (GP107). The 1050 has a cut down chip with fewer cores, hence the performance disparity.

The 1060 6GB and 3GB use the GP106 chip, with the 3GB model having a cut down chip with fewer cores.

The 1070 and 1080 both use GP104 (and the 1070ti now as well). The 1080 is the full chip, with the 1070 variants having a cut down chip.

The 1080ti uses a cut down GP102, while the Titan X Pascal uses a closer to full (but still cut down) chip, and the newer Titan Xp uses the full chip.

Basically, Nvidia uses full and cut down chips. The branding isn't consistent. We've seen the 1050/ti use the same chip, multiple versions of the 1060, the 1070/1080 share a chip, and then the 1080ti/Titan lineup use the same chip.

Clear as mud? Here's my attempt at a table to show how it works:

Cuts|GP108|GP107|GP106|GP104|GP102
:--|:--|:--|:--|:--|:--
Full|GT 1030|GTX 1050ti|GTX 1060 6GB|GTX 1080|Titan Xp
Cut 1|N/A|GTX 1050|GTX 1060 5GB|GTX 1070ti|Titan X (Pascal)
Cut 2|N/A|N/A|GTX 1060 3GB|GTX 1070|GTX 1080ti

Basically, don't see the 1080ti as a 'full 1080,' but instead, as a gaming-focused Titan.
Let's look at it this way. 

1 core @ 10mhz = 10 points (we're being totally fictitious here, using small, easy to follow numbers)

4 cores @ 5mhz = 20 points

If all else is equal, the 2nd configuration will be faster. Now, let's move to your 980ti/1070 example.

980ti = 2816 cores @ 1000mhz = 2,816,000 whatchamacallits (the new official performance spec!)

1070 = 1920 cores @ 1506mhz = 2,891,520 whatchamacallits

Which is about right, given how similarly these cards perform. Of course, this doesn't take boost clocks and OC into effect. And because the 980ti has more cores, a 100mhz OC will have a larger impact on performance than a 100mhz OC would have on the 1070.

Finally, different architectures can have different performance even if their core counts and clock speeds match up. Our example only worked so well with Maxwell and Pascal simply because the two architectures are so similar. 
> I'm confused. I didn't advocate anything. I'm not even advocating my own logic. **I just said that it's not supporting a company to only buy their products when they are the better products.** At that point they don't need support and you're practicing standard consumerism, nothing different than anyone who doesn't care either way. Which, for the record, is not a problem and I am not issuing any orders here. You do you.

Now this I can partly get behind. I mean, you're still (admittedly) confused. In regards to the bold part, I never said that. Not once.

As for my recent posts, I was deliberately taking your text out of context. I wanted to see how you'd respond when your tactics were used against you. I'm actually disappointed that you didn't pick up on it.

I really hate pulling the "I'm probably smarter than you" card, but if you couldn't pick up on the shift in tone, and you STILL can't grasp that the bold part above is false, I don't think there's any point in having this conversation.
We'll see. Right now there's a lot of confusion, and none of the transparency that they promised.

Give it time to shake out.
Then you're being hypocritical, because you CLEARLY stated the criteria which **I** should follow, and by applying the same criteria to **YOU**, you should be buying Intel.

I think you're just upset that I won't follow your orders.
They are both advertised as IPS or IPS type panels. The BenQ is an AHVA (IPS variant). They'll suit your needs for IPS on a budget.

Don't confuse AHVA with VA. Poor labeling, IMO.
So have the Cleveland Browns, and their season hasn't even started yet.
The primary difference is the size. I have a G2-650 in one system, and a G3-550 on the other, and the G3 series is smaller. For a clear example, here's a like for like comparison via JG reviews:

* [EVGA G2-750] (http://www.jonnyguru.com/modules.php?name=NDReviews&op=Story&reid=380)
* [EVGA G3-750] (http://www.jonnyguru.com/modules.php?name=NDReviews&op=Story&reid=500)
Only time I've seen this happen is when someone went into the NVCP and changed the global Vertical Sync option from the default. Go into NVCP > Manage Settings, scroll to the bottom, and ensure that Vertical Sync is on 'Use the 3D application setting.'

If that's not the issue, I'd suggest reinstalling the driver with the clean install option.
EVGA G2-550 (or G3) and Corsair RM550x.

* [G2-550 Review at JG] (http://www.jonnyguru.com/modules.php?name=NDReviews&op=Story&reid=440)

While JG didn't test the 550 G3 or the 550 RMx, he did test other PSUs within each series and found them all to be excellent, with reviews on par with the G2 above. (within a point or so, obviously, since the G2 got a perfect score).
> no offense, but why do you need 2x 144hz panels.

For some, the mouse cursor movement alone is quite jarring when going from one display to the next.
And to add to that, MOST monitors with strobing won't use the feature at 60hz. A PS4 will request 60hz from the display, negating this feature.

Any VA panel you see advertised as '1ms' means backlight strobing, and this typically won't help you with a console.
Perhaps  the ViewSonic VA2719 (~$260) will suit you. However, if you want a comparable display with an ergonomic stand (the ViewSonic is tilt only), go for the BenQ GW2765HT (currently on sale at Newegg for $299, typically $330-$350).

That sale ends Wednesday evening.
> No, I shouldn't, because I buy based on business ethics and support for open source technology.

But that's not what you said. You clearly stated that when one catches up, they're not the little guy. By your logic, Intel is now the little guy in CPUs.

So, you're clearly in Intel's corner now. That's obvious based on what you posted.
> fast response rate

Ignore the advertised response times. They are always misleading and often outright fabricated.

> need a monitor that supports 120-144hz over HDMI...(G-Sync is a plus)

If you are looking for G-Sync, then your GPU should support DisplayPort. There should be no need to use HDMI for THIS purpose.

May I ask what HDMI device you're going to connect that would support 144hz?
> I'm paying attention to the scientific tests. I also have experience with IPS gaming monitors. Generally there is more ghosting on these monitors even though the overall input latency is sometimes lower.

Ghosting occurs when the response time exceeds the frame interval. This doesn't happen often, if at all, on modern IPS (AHVA) gaming displays. Here's some examples using TFTCentral's reviews. For this comparison, I'll be citing their reviews of the [Dell S2716DG (TN)] (http://www.tftcentral.co.uk/reviews/dell_s2716dg.htm), and the [Asus PG279Q (AHVA).] (http://www.tftcentral.co.uk/reviews/asus_rog_swift_pg279q.htm)

At 144hz, and at their optimal overdrive setting, the Dell has a 2.8ms average response time. That's great because the frame interval is 6.94ms. For transition times, the worst measured was 4.5ms, also comfortably under the threshhold. But when it came to overshoot, two measurements were over 6.94ms, measuring 13.2 and 14.5ms, respectively. This means dark trails over 2-3 additional frames.

By comparison, the Asus measured 5.2ms average, with a max of 6.5ms, just beating that 6.94ms interval. No overshoot measurement exceeded 3.3ms. In this case, the Asus had noticeably less overshoot (0.4% vs. 4.0% on the Dell), and no overshoot ghosting at all.

And then here's a direct comparison of the motion blur (open in separate tabs and alternate to compare):

[Dell S2716DG] (http://www.tftcentral.co.uk/images/pixperan/dell_s2716dg.jpg) | [Asus PG279Q] (http://www.tftcentral.co.uk/images/pixperan/asus_rog_swift_pg279q.jpg)

In both cases (best case on left, worst case on right), the AHVA panel of the Asus does in fact have slightly more blur. But it's minor.

Bottom line is that in terms of today's gaming panels,

* AHVA (IPS-like) has less inverse ghosting
* TN has slightly less standard motion blur

Which one you prefer will be entirely up to you. But the monitor that you cited in your original post fared FAR worse in TFTCentral's testing (32" version used). - http://www.tftcentral.co.uk/reviews/samsung_c32hg70.htm

This display had a 13.3ms AVERAGE, with massive ghosting across multiple transitions. It's not exactly a poster child of what you're going for.
They both use a similar panel. They both have similar specs. Where they differ is:

* ViewSonic has a better reputation when it comes to customer service and warranty than Asus
* ViewSonic has thicker, reinforced bezels, reducing the instances of edge bleeding. It's still not a guarantee, you're still playing a lottery, but the ViewSonic has better odds.

If I had to choose between the two, I'd personally take the ViewSonic. But in most cases, it should come down to any price difference and your personal tastes in terms of aesthetics, because they're really that close.
The Dell U2715H/U2717D are pretty much the ideal standard gamut 1440p 60hz displays right now, and can typically be found for ~$400. They have higher quality panels, great build quality and quality control, and come factory calibrated. They're the total package.

Let me know if that works for you. If you need wide gamut for specific projects, let me know and I'll delve into those options (if you don't know what wide gamut is, you don't need it). If you want something cheaper, and don't mind dropping some features or a little in terms of quality, let me know and I'll look at the budget recommendations.
> but the CF591 (VA) I originally ordered has an aggravating blur effect during fast gameplay/camera panning, even with game mode activated (despite many glowing online reviews from console gamers), something I never noticed on my Sony TV. I'm worried IPS will sacrifice performance in-motion for viewing angles in a similar fashion.

Ghosting occurs when the response time exceeds the frame interval. The frame interval at 60hz is 16.67ms (that's how long it takes to move from one frame to the next). So long as the real (not advertised) response time stays below 16.67ms, you won't see ghosting, though blurring to some degree will always occur as it's the nature of LCD tech.

VA panels, like the CF591, tend to have really long response times for darker transitions, in the neighborhood of 40-60ms, causing them to smear over multiple frames. This is what you're seeing.

IPS doesn't do this in modern monitors. Most consumer IPS monitors these days have response times in the 7-10ms range (average), or 12-14ms (worst), keeping them below the frame interval.

My advice to you is as follows - identify one each (IPS and TN) that you feel would otherwise meet your criteria. Buy both. Return the one that least meets your needs.
He's likely falling for the advertised '1ms' response time, and confusing it with input lag.
> AMD wouldn't be the little guy if they are "tied".

By your logic, AMD is no longer the little guy in the fight against Intel, and no longer needs your support. You should, by your own 'logic,' buy Intel from now on. Support them as they try to catch up to Ryzen.
Further reading, for those interested.

OP asked the question here, and was told no, don't bother, PS4 uses 60hz only.

https://www.reddit.com/r/Monitors/comments/85fafk/my_first_monitor/

And here as well, same question, same answer:

https://www.reddit.com/r/pcmasterrace/comments/85ffe3/my_first_monitor/

**It appears that the OP, while knowing that a 144hz monitor won't give any benefit to a PS4, and while fully understanding that '1ms' means nothing in reality, really REALLY wants a 144hz, 1ms advertised monitor, and just wants somebody, ANYBODY, to tell him it's a good fit. And, he's willing to open as many threads as he can to get that answer.**

I'm not a good liar, so someone please lie to him and tell him it's a good idea.
What you are doing is called 'shopping for answers.' You didn't like the prior answer, so you figured you'd ask again.

https://www.reddit.com/r/Monitors/comments/85ohth/looking_for_a_monitor/

You:

> So, I need help in buying my first monitor and I want something fast, indeed I want something that has 1ms response time, I dont really care about the Hz (I play on console) also I want it to be 27inch. Any thoughts? Oh and also I want it to be curved, any help would be appreciated, thank you.

First answer (abbreviated):

> Ignore this spec. It's falsely advertised and there are precisely zero PC monitors on the planet with a real world average response time of 1ms (outside of professional OLED displays).

> So, you're looking for 27", curved, $390 or less, and you play on a console (meaning I should target something that is 1080p, and has some form of audio output).

> My recommendation is to start with the Samsung C27F591.

I then went on to list the specs and how they match what you were asking, but also the drawbacks.

You then asked:

> Hey, what do you think about the MSI Optix G27C2?

I replied (again, abbreviated):

> Without strobing, high refresh, and FreeSync, the monitor simply becomes similar to the one that I recommended. I don't think it's a good fit for your stated need.

You:

> How can I make the fast response time work properly in console?

And the answer was, you can't.

----------------

So, to answer your new question:

> I wonder if this monitor would be good for console gaming (ps4) I usually play games like Rainbow Six, cod and Fortnite. Any help would be appreciated.

It's a waste for your stated need. And that won't change no matter how many times you ask it.
It's an app update. Got mine today on my LG Watch Style. Typical Google staged rollout. Updates the base app and version info, but no actual changes on the watch in my case. Still have the Android Wear bootup animation.

To fully transition to WearOS, I believe that you need:

* App update on watch
* App update on phone
* Firmware OTA for watch

I could be mistaken, but this hasn't been the most transparent process. Also, their idea to update the version number.
Good ol' Anna, giving false hope to neckbeards everywhere.
Just an FYI, the death penalty is already an (not THE, so there are options) authorized punishment in the USA if found guilty of treason.

##[18 U.S. Code § 2381 - Treason] (https://www.law.cornell.edu/uscode/text/18/2381)

> Whoever, owing allegiance to the United States, levies war against them or adheres to their enemies, giving them aid and comfort within the United States or elsewhere, **is guilty of treason and shall suffer death**, or shall be imprisoned not less than five years and fined under this title but not less than $10,000; and shall be incapable of holding any office under the United States.
Nvidia: "There's nothing to see here, and we aren't enforcing terms of some GPP anywhere near what HOCP reported."

/gamer branding disappears from Gigbayte, Asus, and MSI's AMD cards simultaneously around the globe.

Nope, nothing to see here folks.
You can't. Unless the console is patched to support a refresh rate that would work with such a monitor. While FreeSync support is being patched in to the Xbox One S and X, they haven't announced or even discussed higher refresh rate support.

If you want to avoid the ghosting associated with VA panels, your best bet is IPS or TN.

All of the curved 16:9 1080p monitors that I know of are VA panels.
I haven't used one so my opinion on it is a bit limited. Bear with me.

It's a 144hz FreeSync monitor, and your stated use was for console gaming, so you won't reap any benefit from this. It's another one of those monitors that falsely advertises a 1ms response time. It's still a VA panel. The '1ms' is enabled through flickering the backlight on and off (backlight strobing). Most monitors with a feature like this require you to run at a fixed, higher refresh rate. Your console will request to run at 60hz. Most likely, you won't be able to use this feature.

Without strobing, high refresh, and FreeSync, the monitor simply becomes similar to the one that I recommended. I don't think it's a  good fit for your stated need.
> I'm not twisting your words. You're repeating over and over that your only motivation to buy a product is performance per price. Every paragraph is a reworded summary of this.

And I stated that if AMD was in the same ballpark, I'd go with them. IE, tie goes to the little guy.

They've done this with Ryzen, so while my primary build ended up with an i7-7700k (at the time, the fastest CPU for games like WoW that are poorly optimized), my secondary build was an R5 1600 + RX 580 for a friend (who promptly sold the GPU for ~$400 and bought a 1060 for $~240 because it was right when the current mining boom began).

I'm all about the best build for someone's money. AMD didn't make the cut for my build in that situation. But you're still wrong when you say:

> So you don't want to support AMD.

Yes, I do want to support them. I just ask them to meet me half way.
> Interesting you say not max. Is this on all monitors?

It's typical, but all monitors vary, even different units within the same product line.

> Looking at a guide for mine I see they say 100% 

The author there is targeting 200 nits. They explain this 2 pages later:

> Since we consider 200 cd/m2 to be an ideal point for peak output, we calibrate all of our test monitors to that value.

They consider this an ideal max value, not a target value. And they do this to 'benchmark' monitors all at a similar level of brightnes.
 There are monitors that even at zero brightness can't get as low as 120 nits. By targeting as point that pretty much all monitors can hit, they are able to remove a variable. They then add:

> In a dark room, many professionals prefer a 120 cd/m2 calibration

I would partially disagree though. In a dark room with no bias lighting, 120 nits is too bright, IMO, which is why you sometimes see some gamers target 80-100. I prefer bias lighting though, so 120 in is. Even during the brighter parts of the day, 120 is fine in my office, but YMMV.

Set it to what you prefer, but for most people, max brightness is too much. Max brightness also makes off-axis glow and backlight bleed look worse.
> Dont forget crappy gaming keyboards with cherry mx style key switches instead of good stuff like matias, unicomp, or topre.

There is nothing wrong with subjectively preferring a certain key switch over another. I actually prefer scissor switch! The Microsoft Modern Keyboard with Fingerprint ID is my dream keyboard, except that it uses Bluetooth which causes some serious issues. If I were ok with remaining on a wired keyboard, I'd buy it again.

The problem with gaming keyboards from my perspective was the desire for them to look like futuristic weapons. Even Ducky got in on it, with their Shine 6 having an edge taken from military rifles.

In reality, the only thing that a gaming keyboard needs over a non-gaming keyboard is anti-ghosting to prevent certain key combos from locking out. After that, features, aesthetics, and even switch type are purely subjective.
> So you don't want to support AMD

I like how you're twisting my words :)

> What you want is for AMD to become better than Nvidia so that you you can have a faster computer with red stickers instead of green.

I want AMD to be on par, or close to it. I went through 2x GTX 1060s and 2x RX 480s in my experimentation. At the time, I was playing a lot of UE-based games (going through the Arkham games in my backlog, and playing a lot of Redout). The 1060s drew a max of 120W. The 2 480s (Sapphire Nitro+ OC, MSI Gaming X) topped out near 225W, typically running just shy of 200W. That was more heat and noise, and they performed worse in these games. Redout is a particularly bad offender, running nearly 2x the speed on a GTX 1060 as it does on an RX 480.

AMD's GPUs are competitive in games they've been optimized for (IE, games that make it into most reviewer test benches). Outside of that, it's ugly. Why the heck would I pay a similar price for something that runs hotter and louder, but is significantly slower in the games that I play?

If AMD was relatively close (IE, their ~5% performance deficit shown in reviews translated to most other games as well), I would have stayed with them. But the Doom Vulkan issues combined with the poor performance in the games I played meant that I was better off going elsewhere.

 > Just don't pretend to care when you would sooner see the market burn and monopolies rise than sacrifice a few fps or deal with a driver issue.

Give me an adequate product and I'd buy it. Given the rumors surrounding Zen+, I'd likely take a 2600x over an 8600k despite being slightly slower in the games that I play. AMD is competitive in the CPU space again and I love it.

But I'm not doing charity. I am not paying the same price for a GPU that consumes nearly twice the power and runs barely over half the speed in some games. That was absurd (and yes, that's a worst case scenario).
It wasn't clearly a joke, sorry. It's hard to tell when something is a joke when so many people actually think the way that your post implies. Yes, there are people who are legitimately offended over me not calling Hollins a sure thing.

Now, your user name perfectly describing me? That's funny.
I'd agree with comfortable. I'd disagree with any degree of accuracy. I'm still going to side with, "better than nothing, not a viable substitute for a colorimeter if accuracy is desired."
> I don't get this. For gaming isn't that exactly what you want? Video production/editing etc would have a problem with this (and you technically wouldn't be using a curved monitor in any case) but for gaming isn't that the very purpose of calibration settings - so that you can adjust it to your liking?

Adjusting a picture of a crosshair or other object in the calibration wizard is not the same as adjusting a game to look the way that you like.

Adjusting for accuracy gives you what the artists intended you to see.

Don't get me wrong, there's nothing wrong with subjectively liking the color to be off in certain ways. I was just trying to highlight that Windows' built in calibration will not get you an accurate result, and is not a viable substitute to a cheap colorimeter for those who want an accurate picture.
If you're willing to give up the curve for less ghosting, there are some viable options. My two favorites are:

* [HP 27ER] (http://store.hp.com/us/en/pdp/hp-27er-27-inch-display) - 27". 1080p, IPS, glossy-ish coating, and it does have dual-HDMI. Downsides are no speakers OR audio output, so you'll need to adjust for that.
* [Dell SE2717H] (http://www.dell.com/en-us/shop/dell-27-monitor-se2717h/apd/210-aizr/monitor-accessories) - 27, 1080p, IPS, matte coating, FreeSync support (48-75hz, IIRC). However, 1xHDMI and 1xVGA, no speakers, no audio out.

Basically, finding a good 1080p with at least 2xHDMI and some form of audio support for console use is difficult. Doing a quick Newegg search listed a few possibilities though that you can take a look at (I removed TN due picture quality, and VA due to ghosting).

* AOC I2757FH - 27", 1080p, IPS, 2x HDMI, speakers, audio out
* Asus VZ279H - Same as above
For pure gaming, I like the LG 34UC89G, which offers a 2560x1080 144hz (166hz OC) panel with G-Sync, and is factory calibrated. However, many feel that 1080p is too low for that size.

So, you can go with a Dell Ultrasharp, which often falls below $700. They offer 3440x1440 60hz IPS panels and no adaptive sync, but picture quality is outstanding.

If you want the best of both worlds (3440x1440 with G-Sync and a higher refresh rate), you're looking at closer to $1,000.
It's a poor substitute for actual calibration. You don't get accuracy. What you get is what you subjectively like. It's basically a more interactive version of what Lagom offers via their website.

Don't get me wrong, it's better than nothing. It's just not comparable to a proper calibration tool.
That's fine. If you think he's a guaranteed stud, good for you. As for me, he's an unproven commodity, and it's good to have camp competition and let the winner take the spot.

Or maybe reading just makes your head hurt :)
> I checked the thread (thread ID 637yyd) you posted. The AMD guy said he created a ticket for it to look into it on the AMD side.
Top comment right now says it has a fix and that the problem is actually with the Vulkan libraries and not AMD driver.

> So that's a bit of historic revisionism on your part there.

There were multiple threads. And the ticket went nowhere, with no solution from them. And the problem ended up being on AMD's side (Vulkan worked just fine on Nvidia, and there's no conspiracy about Vulkan favoring Nvidia that I'm aware of). The solution was to revert to an older AMD driver. But sure, we can call me a liar instead :)

> Also, what about your 7700k? What about an R7 1700 instead?

I was coming from an i5-4590. I primarily game, and I was having issues with single-threaded performance more so than multi-core performance. World of Warcraft, especially, due to its poor multi-core optimization. I couldn't even maintain 60fps in many areas and a single thread would be pegged at 100%, leaving the GPU under utilized. Ryzen wouldn't improve this for me, as its IPC was on par with Haswell, and it ran at similar clock speeds. The 7700k, however, had a 10-15% IPC boost (best case, IMO), and I'd be running 4.4hz all cores vs. 3.5ghz, giving me a substantial uplift. It worked. I rarely dip below 80fps now, much less 60. And where I do, it's my GPU that has become the limitation.

I didn't just jump on the 7700k though. I waited for Ryzen to come out, then I waited on memory fixes and other optimization. I REALLY wanted to go AMD, but my loyalty is to my wallet, and the best performance I can extract for my stated needs.

> See, this is part of the problem. While I get what you're saying and I never bought Bulldozer myself because of how far behind it was, people need to support AMD even if they're not the best choice. If they're close that must be good enough to support them. Otherwise they'll never reach and we'll never have proper competition. 

Part of AMD's problem is that they know that they have a loyal base, and they can continue to release garbage and eek out a small profit because these people will continue to buy from them to 'support the little guy.' I think AMD is happy with their little guy role in GPUs. In fact, prior to Raja's hiring, they implied this is where they wanted to be, hence the constant rebranding (where Nvidia would release an entire new lineup, AMD would release only a new top-end card, and rebrand existing cards down the stack).

This loyal devotion from fans is what encouraged their stagnation. But when CPU buyers said "no thanks," what happened? They caved and developed Ryzen. They actually strived to be competitive again. And I hope they eventually do that with Radeon as well.

But until then, no charity from me., I buy what gives me the best performance for the dollar. That said, I do wish that I knew then what I know now. Because while the 1060 is a better buy for me than the 480, for this generation, there were prior generations where I actually should have gone Radeon over GeForce.
I've seen worse.

"Hey, I cracked my monitor's screen, when when I share a screenshot, no one else can see the crack. Why?"
> Stop buying hardware from the dominant producers. Encourage competition. It's easy to talk about the underdog and how shitty they're treated by the dominant one but most people become hypocrites when it's time to choose their own hardware.

I tried this, I really did. I bought an RX 480. Then it started having issues with Doom. OGL worked, but Vulkan would cause the game to not load. I posted a thread over at /r/AMD and numerous others had the issue. We got an AMD Community member involved, and one by one, he'd tells us, "Never heard of this problem, must be your configuration." He shut it down.

So I swapped my 480 for a GTX 1060.

I want to support the little guy, but it can't be as an act of charity. They need to earn it. And their support over the Doom issue soured me on them, so I left.
>  It's also disingenuous to switch the goalpost to "it's good value for the price" when you're recommending it to someone who asked for a good chair,

I moved the goalposts? You're actually taking the goalposts off the field!

He asked for a good chair without specifying a budget, so I made NUMEROUS recommendations. You're hounding me over one.

> and recommended it as a better alternative to "gaming" chairs which are most often actually better.

Gaming chairs are horrible for your back. They are not "actually better" as you stated. They are measurably worse.
Those investigations take years and are often subjected to partisan swings.

AMD has been fighting Intel for about a decade and I've heard they still haven't been paid.
> So I didn’t really fall for marketing =P

In two out of three you did :p (and again, I did as well, see the bottom)

* You think/thought you need 150-200W overhead for your PSU
* You thought there were good gaming headsets (I showed that even in the 2 best cases for them, they are beaten)
* You thought that there were no viable options for wireless (at the very least, I named one that is better in audio quality, and really in other areas, but you sacrifice the mic).

> And yea I’ve seen the V-MODAs but they’re $326 and I got my G933s for $100. They’re far from the best quality but I only use them for gaming anyways and can’t go back to wired.

Yea, it wasn't the best example. I don't normally deal with wireless headphones/microphones and I didn't have time to do the research Mea culpa. If you need a wireless headset, you're basically still going to be using a headset for gaming, and separate headphones for everything else.

In my case, I use my AKG headphones for virtually everything, and add the Boom Pro for gaming. I have cheap wireless headphones for my phone, and I'll probably consolidate all of this when the Crossfade 3 comes out.

So, how did I fall for marketing?

* Purchased a G2-650, which was overkill
* Purchased a Hyper X Cloud II
* Purchased a gaming chair (my back aches!)
* Other examples, I'm sure

I'm learning. And hopefully, my posts come across as more helpful than insulting (I sincerely mean it when I say I'm not trying to be an ass).
> If you can find me a non gaming wireless headset with a mic that’s better quality than my G933 for a similar price I’d be all for it. If I was okay with wired I’d probably do like a Sennheiser with a modmic. Not too many gaming headsets I like.

Wireless is probably the best exception. For wired, the best ones are the Hyper X series (not all of them, but the Cloud 2's are outstanding) and Sennheriser's headsets are good. HOWEVER, even in those best case examples, you can do better for the price.

The Sennheiser PC373D ($250), for example is basically just the HD 569 ($150) plus a microphone that is beat by a V-MODA Boom Pro ($30). So again, even the good one is beat by buying two separate parts at a lower price.

Heck, my AKG M220 ($60 when purchased) and Boom Pro ($30) is $10 cheaper than the Cloud II, and offers better quality on the headphones, and a much better microphone.

But yes, wireless is the main exception. I'd personally go with the V-MODA Crossfade Wireless 2 and use via BT, as it does have a built-in mic. You'd get better sound quality than ANY wireless Logitech gaming headset, but the mic would be of inferior quality. So, balancing your priorities at that point.
> For fuck's sake, stop recommending the Markus. It's a flimsy piece of shit

It's a proven commodity at its price point and will continue to be recommended as an option.

> Especially if you're a heavy person.

The people who most likely have issues with it are people who are above the weight limit. The chair is tested for 110kg (~242 lbs). The chair that I have is rated for 220 lbs max. If you consistently apply more than the rated weight to a chair, it degrades quickly.

Larger people should consider larger chairs. And no, the chair isn't fat-shaming you.
Some guy a few weeks back was bragging about getting an Aeron for $2 (yes, two!) at Goodwill. That's amazing!
> ASAIK if you get a PSU that has near 0 headroom as the capacitors degrade with age it will become a problem. I also haven’t seen gaming PSUs but maybe I haven’t been looking... My 850w Corsair Gold is 7 years old and going strong. I’d say over budget by 150 to 200w. 1000w psu for a GTX 1060? Sure that’s overkill. 650w/750w for a 1080ti? I’d say that’s fine.

And I'm sorry, but you're mostly wrong.

First, you don't need the overhead. My EVGA G2-650 can handle a 650W load on day one (it's actually tested over 700W). And in year 7, as it nears the end of its warranty, it can still handle a 650W load. If it can't, it gets RMA'd. Quality PSUs are overbuilt to account for degradation.

Then there's the efficiency meme. Many people think efficiency falls off a cliff if you go away from 50% load. Also bunk. The difference between 50% load and the extremes (15-20% load and full load) is typically 1-2%.

As for my system, I have an i7-7700k and a GTX 1060. At the wall, it draws ~210W under a dual stress test, and ~180-200W during gaming. That's measured by a Kill-A-Watt P3-4400 and that includes PSU inefficiency, meaning actual system draw is lower.

I also don't fully agree with your comment on headsets. To put it plainly, you're falling for the marketing. You're exactly the kind of person that I was describing.

And I sincerely apologize if this comes across as insulting, as I swear it's not meant to be. There's no easy way to say "you fell for marketing," because it always comes across as "you're dumb," no matter how it's worded. I admitted in my last post that I fell for it too.
Absolutely.
NP.

If you go that route, please read this for basic setup and installation.

https://www.ceos3c.com/2017/05/17/calibrate-your-screen-with-displaycal-and-a-spyder-5/
Can't really tell from a photo. I'll just give you my standard suggestions:

* Set brightness to something other than max. Recommended is ~120nits. This is typically 20-40 in a monitor's OSD, depending on model. (35-40 for AW3418DW). If it seems too dim, give it 2-3 days for you to adjust. You really shouldn't be cranking the brightness.
* Ensure you're sitting an appropriate distance from your monitor. 30-36" ideal, but no closer than 24".
* Ensure proper posture and head height. Most people try to have their eyes level with or above the top, but that's the old TN rule. You shouldn't be looking down at your display (bad for the neck, and encourages panel glow in the lower corners). If you could draw a line from between your eyes to your monitor, it should be centered horizontally, and between 1/2 to 1/3 from the top. My preferred spot is 1/3 from the top of the display, or just above centered.

After you do all that and adjust to it after a few days, follow [THIS] (http://puu.sh/lDz83/b31a64b5e3.png) guide from the sticky in this subreddit.
My primary beef with Dell's gaming monitors is the lack of gamma adjustment in the OSD (see flair, I'm afflicted as well).
Updated prior post with links, hope that helps. Not much in the >$200 to <$600 range, but everything else is covered.
> the colors on my Samsung are very muted, which makes editing hard/nearly impossible.

It's possible that the former is incorrect and overly vibrant, while the latter is more accurate. Many people tend to describe accurate colors as 'muted' and/or 'boring.'

Or, maybe the gamma is just too low :) No way to know for sure without measurement.

> If I need to buy two whole new monitors, what is the cheapest route I can go?

The cheapest that I would recommend would be to buy the Spyder 5 Express (~$120), and download the free DisplayCal suite. Eyeballing it up isn't a great idea, especially given the importance you're implying.
> To answer your question about the stuttering, yes, I pay attention to the slightest stutter and it annoys me immediately.

Then you want G-Sync, no doubt about it :)

> Also if it's not a burden for you, could you give me a recommendation for an TN panel as well ? Since you said there have been improvements in overall quality of the panels I'm interested in checking them out since the price difference is significant.

Not a burden at all. My recommendations are going to be the Dell S2417DG or S2716DG, depending on your size preference.

As for the S2716DG, there are better options out there now, like the new Omen and Asus' displays that are using updated panels capable of 165hz and fewer visual anomalies. The problem is that these newly launched displays cost as much as or more than the AHVA variants. So why bother?

Get the S2417DG or S2716DG if you're on a budget, and the XG2703-GS if you prefer better picture quality at a higher cost.

Or say "eff-it" and get an Ultrawide :p

> Thank you again for your time!

Just glad to be of help in any way that I can. Whatever you get, enjoy!

EDIT: Wow, thanks for the gold!
I had the 6GB version of this. The SC Gaming is noticeably cooler and quieter than the standard Gaming version due to the beefier heatsink and the heatpipe inclusion.

Running it in my dampened Lian-Li under my desk, it wasn't audible during gaming. Moving it on top of my desk inside of an NZXT was a different story, but it was still acceptable.
I can't even recommend myself a good chair at this point!

Really, it depends on your budget. The standard recommendation for a recovering racer-chair gamer like me is the Ikea Markus (~$200). Costco has a tolerable (good for the price) task chair made of mesh material for $99.99 (on sale for $69.99 at my local Costco this month) for those on an extreme budget.

If your budget goes beyond those, you can look into the higher-end offerings, with many suggesting Herman Miller ($1,000 and up) or Steel Case ($600-$900 range). I'm personally looking at the Eurotech i00 in white, for ~$600 w/headrest.

EDIT: Links for the items I discussed (these are Google'd links, no referral links AFAIK).

* [Costco Mesh Task Chair - $69.99] (https://www.costco.com/Metrex-II-Black-Mesh-Task-Chair.product.100177590.html) - Membership NOT required!
* [Ikea Markus - $199.99] (https://www.ikea.com/us/en/catalog/products/00103102/)
* [Eurotech i00 - $592.00] (https://www.thehumansolution.com/raynor-eurotech-ioo-chair-with-headrest.html)
* [Herman Miller Aeron - $820.00] (https://www.hermanmiller.com/products/seating/office-chairs/aeron-chairs/)
* [Steelcase Leap - $972.00] (https://store.steelcase.com/seating/office-chairs/leap)
* [Steelcase Gesture - $1,010.00] (https://store.steelcase.com/seating/office-chairs/gesture)
> For the panel type I would have to go for IPS mainly cause I currently have a TN one (Asus-VG245Q) and the viewing angles plus incorrect colors drive me crazy. I’d like the best color representation possible.

To be fair, these 1440p TN panels are FAR better than ye olde 1080p TN in terms of color and viewing angles.

> Lastly, FPS>refresh rate will not be the case for 90% of the games I play since the upgrade from 1080 to 1440p will cost some performance. (I don’t think my 1080/Ryzen 1600 combo would output more than 144fps constantly on games like PUBG for example).

And when you normally game without G-Sync, do you notice/are you annoyed by stuttering (V-Sync on) and/or tearing (V-Sync off), or are you ok with these as-is?

------

That said, unless the answers to those concerns above change your mind, my recommendation for you is the [ViewSonic XG2703-GS] (https://www.viewsonic.com/us/xg2703-gs.html).

* 27"
* 2560x1440
* 144hz native, 165hz OC supported
* AHVA (IPS-like)
* Subdued gamer aesthetic (the green isn't very noticeable in person)
* USB hub
* ergonomic stand
* G-Sync

Why take this over the other AHVA options? Unlike Asus/Acer, ViewSonic uses thicker, reinforced bezels, somewhat mitigating the additional backlight bleed. Make no mistake, it's still a lottery like the other brands, you're just buying a ticket with better odds. VS's warranty/customer service tends to be slightly better than the other brands, if/when needed. They're not Dell/EVGA quality though.
>  I know they both have “fake” HDR.

This is actually incorrect and partially overblown. I'll expand on this later, but let me address your other concerns first.

> I will be using this for primarily digital/print design (adobe cc), general productivity, XB1X, and PC gaming.

> Is the new LG worth picking up over the Dell

> TL;DR need new 4k monitor for my freelance work and some XB1X/PC gaming.

Both displays use a similar panel, with Dell using the slightly higher revision (1300:1 vs. 1000:1 contrast). Both are 10-bit (8+FRC), but limited to sRGB, so no actual wide-gamut support for your photo editing (that's ok, not a deal breaker at this price point).

**Where the Dell Wins**

* Warranty (3-years + free advance replacement vs. 1-year and you pay to ship)
* Customer service (I've used both for customer service, Dell was a joy, and LG is the Comcast of monitors)
* Integrated USB-hub (the 27UK850 is the comparable monitor in this case)
* Factory calibration - My experience and multiple reviews from trusted sources tell me that Dell's calibration is on point, whereas LG's is subject to some crazy variance. If out of box color matters, Dell wins in this regard.

**Where the LG Wins**

* FreeSync support (40-60hz) including over HDMI. Great for AMD GPUs, and SHOULD work when the Xbox One S/X is updated.
* Aesthetics - I know this is subjective, but I just love the look of these monitors.
* HDMI 2.0 w/HDCP 2.2, a big deal if you plan to use 4K Blu-Ray with that Xbox (I couldn't find confirmation that the Dell supports this, but it might)

## The truth about HDR

I noticed that you believe the #FakeHDR meme going around. And that's ok, as I admit to being partly responsible for starting that.

Early HDR monitors fell into one of two categories; Ones that did not support HDR10, [like this display] (https://www.reddit.com/r/Monitors/comments/676xts/dell_s2718d_impressions_ultrathin_fake_hdr_fake/), and ones that did.

The former category had no HDR10 support. What they did was had an "HDR" mode in their OSD. This was basically just re-branding Dynamic Contrast. It was ugly, and it had nothing to do with actual HDR. The monitor was not detected as an HDR-capable display, nor could it display an actual HDR signal. This is the very definition of fake HDR.

Monitors supporting HDR10 are not 'fake' HDR displays. However, I consider them to be 'limited' HDR displays.

The two main benefits to HDR are expanded color range, and expanded range of luminance, giving greater perceived details and contrast. Both of these monitors support 10-bit (8+FRC), so they can show the expanded range of color for the most part. As for luminance, this is where most people get it wrong.

Most people think that you need 1,000 nits. You don't, but it's nice to have. What you need is RANGE. A static backlight does nothing for you even if it can hit 1,000 nits. What you need is a dynamic backlight where one pixel can be at 1 nit, and another can be at 1,000 nits, and the rest somewhere in between. This combined with the panel's native contrast will give much better perceived contrast. 

A 400nit FALD (full-array local dimming) display will have better range of luminance than 1,000 nit edge-lit displays. Unfortunately, these two monitors are 350-450nits AND edge-lit. This means UP TO 8 perceptible lighting zones, at best, and they don't distinguish very well.

Don't get me wrong, you WILL notice a difference with HDR on vs. off in supported content. It's better than nothing, and it's far from being 'fake' HDR. But again, it's limited.
> Can you please explain why I would want to scale the UI size?

By default, the operating systems tries to run at a set DPI (I'm probably butchering this, I'm tired). So, as you go from 1080p, to 1440p, to 2160p at the same size, your Windows UI gets smaller. You have more work space, but elements are smaller so expect to squint.

If you run a 4k display at 200% scale, you're getting the same workspace as 1080p, just with sharper text. But, some programs don't like this and as a result, they tend to look blurrier (old control panel, Steam, and many others). In this case, you just lost the entire benefit of 4k (the work space) and gained additional scaling issues.

Most consider 1440p to be ideal at 27".

> How does the 5k displays of microsoft studio & imacs manage to look so nice with ~27" 5k displays?

They run at 200% scale, and Mac OS doesn't have the scaling issues of Windows due to Apple's tighter requirements for software developers combined with Apple's scaling implementation. However, due to the scale run, you get the effective work space of a 1440p display, with slightly sharper UI elements.

-----

Given your stated price and specs, here's my recommendation and my rationale:

Recommendation - Dell U2715H or U2717D, whichever you find cheaper. If found at the same price, flip a coin or choose based on aesthetics.

Why - 

They'll fit around your stated budget, and well within your stretch budget. They'll give you the ideal workspace with a native 1440p. They are 8-bit and sRGB (narrow gamut) displays, but wide gamut AND build quality at this segment jacks the price up. They come factory calibrated to a highly accurate degree (unlike LG/Samsung who tend to vary greatly between units even after their so called factory calibration), mitigating the need for a colorimeter, which you'd prefer to avoid.

Let me know if you have any questions/concerns over the recommendation, or if you'd REALLY prefer to go 4k, and I'll try to address that as best I can.

If you do go with my recommendation, enjoy.
> Do you guys have any suggestions ?

If you can fill in some blanks, I can give you a tailored response. At 2560x1440 and high-refresh, you have the following options:

* TN + G-Sync | Budget friendly options with great specs for the price, and picture quality is vastly improved over prior generation TN offerings. As a TN-hater myself, I'd still be OK with one of these. Downsides are that picture quality still isn't on par with IPS/VA of the same spec sheet, and there are some visual anomalies.
* IPS + G-Sync | The problem here is that LG (IPS) and Samsung (PLS) didn't enter this market, so you have an IPS clone called AHVA from AU Optronics (AUO). Is that enough initials for ya? :) AHVA will give you better picture quality than the TN counterparts. TFTCentral's in-depth measurements also reveal lower input lag than the TN options (by 1-2ms, nothing great), which is largely offset by the slightly worse response time. Basically, the AHVA panels will feel more responsive, will look slightly more sluggish, but both aspects are so close that most users won't notice. The primary downsides are cost ($600-$800), and the lottery (the panels have worse off-axis glow than the typical IPS/PLS panels, combined with the shoddy build quality from most gaming vendors making backlight bleed unacceptable in a lot of them).
* VA + FreeSync | VA gives you a nice compromise, picture quality approaching (but not quite matching) AHVA, but without the lottery issues. The downsides are the inferior response times (you WILL have ghosting and overshoot ghosting at higher refresh rates), and the lack of a G-Sync option at this spec.

So, weigh the importance of G-Sync (more on that below), and then choose the panel type. I'll give you a recommendation based off that.

> Also is G-sync worth the extra money ?

If you're going to be running FPS > refresh rate, then G-Sync is useless and can be ignored. If you expect framerates below max refresh, then ask yourself, "Self, am I annoyed by stuttering (v-sync on) or screen tearing (v-sync off)?" If the answer is 'no,' then you don't need G-Sync, but it's certainly nice to have.

In layman's terms (and I can get more technical later if you want), G-Sync is an alternative to V-Sync that mitigates tearing, stutter, and input lag. It's not the best in all areas, but it's a great all-arounder. G-Sync + V-Sync on = no stuttering, and reduced input lag relative to plain V-Sync (until FPS = max refresh). G-Sync on + V-Sync off means no screen tearing when within range (and tearing resumes when you go out of range).
B&H puts up a pre-order when they feel like it. They don't ever guarantee a release date (even ones they post are subject to moving). They were the first to post a pre-order for the LG 32UD99-W, famously sparking news articles about its 'pending' release. I think  it was close to 8 months after the pre-order went up that the first B&H user got one...
I'm not surprised. I've been wondering what they'd do with the position given that Smith didn't work out (though I do greatly appreciate his contribution). They seem to be building a trio along these lines:

* WR1 = Possession | Alshon has this locked up
* WR2 = Deep Threat | Smith was a relative bust, and Hollins isn't a sure thing
* Slot | Agholor appears to be a stud here

There were no viable options in FA, IMO. And for those clamoring for a return from former Eagles, Maclin (WR1-type) is older than and inferior to Jeffery. Jordan Matthews (Slot-type) is older than, interior to, and more expensive than Agholor. DeSean Jackson (WR2-type) could have been a good fit, but age and cost were major concerns.

The best course of action is a draft pick to compete with Hollins for the position. And as 2018 showed, we can give this person time to develop due to the sheer number of other quality targets that Wentz has available to him.
Latest is that it was delayed to Q3.

Article - https://www.techspot.com/news/73686-trio-4k-g-sync-144hz-desktop-monitors-reportedly.html

Source - https://twitter.com/TFTCentral/status/968799839143317504
> Man, this is really bad for AMD.

It really is. As much as I love PC gaming, PC gamers as a group are incredibly gullible when it comes to gamer-oriented marketing.

* Gaming headsets combine a microphone and a headphone of a quality generally inferior to non-gaming components in the same price range.
* Gaming chairs have notoriously bad ergonomics.
* Gaming monitors falsely advertise response times in both value (pixel transition times are much slower than advertised), and intent (many people confuse response times and input lag, largely due to the way it's marketed).
* Gamers tend to grossly overbuy in rated wattage for PSUs, passing on high-grade 400-550W PSUs in favor of mid-grade or lower 750W PSUs all due to gamer-centric marketing for these products. (You'd be shocked how little your PC actually draws when gaming).

Because we as a group are so susceptible to gaming-centric marketing (and I include myself, as I sit in my back-breaker racing chair), this program has the chance to do significant harm to AMD.
> indeed I want something that has 1ms response time

Ignore this spec. It's falsely advertised and there are precisely zero PC monitors on the planet with a real world average response time of 1ms (outside of professional OLED displays).

Also, most users confuse response time and input lag, so it's hard to tell what a user really wants when they say "1ms."

So, you're looking for 27", curved, $390 or less, and you play on a console (meaning I should target something that is 1080p, and has some form of audio output).

The good news is that the vast majority of budget non-gaming monitors have obscenely low input lag these days as their low budget for materials/software means that they don't do the image processing a TV does, meaning little additional lag.

My recommendation is to start with the Samsung C27F591. It has the following specs:

* 27"
* 1080p
* Curved
* VA panel
* Built-in speakers (useful for console gamers)
* Typically $250-$300

[PCM's review] (https://pcmonitors.info/reviews/samsung-c27f591fd/) showed adequate response times (minor ghosting/overshoot, typical for a VA panel), but strong vibrancy and contrast, and it uses a native 8-bit panel, so a strong showing in image quality for this segment. Input lag is typical for this segment, reasonably close to 0 (they estimate 6.7ms). To be clear, you're gaming on a 60hz console and you can't control V-Sync, so you want input lag to be < 16.67ms. Anything below that will put your movement data on the next frame, meaning 0.01ms and 16.66ms are essentially equal when v-sync is forced. You're good here.

So again, that's where I'd start. Let me know your concerns with that suggestion, and I'll try to adjust based on your needs.
> The U2717D looks good, but I don't understand the difference between that and the U2717Q. Does anyone know what the difference is?

U2717D is a 2560x1440 display using a Samsung PLS (IPS-like) panel. The U2717Q doesn't exist, but I'm guessing you meant either the P2715Q, U2718Q, or UP2718Q. If so, please clarify as these are vastly different displays.

> Also, based on the bullets below, what are some other models/brands that I should be considering? I will not be using this for gaming.

> Specs I care about

> * 4k
> * good colors
> * 27"-30"
> * prefer small bezels/nice looking

First, I want to make the pitch to avoid 4k at that size and go for 1440p. With 4k, you're going to want to scale to a UI size equivalent to 1440p anyway, and this will introduce some scaling issues without the benefit of the 4k workspace. If that is what you'd be doing, stick with 1440p and run native.

So, before I can give a decent recommendation, can you please answer the following questions?

* Preferred budget (how much you want to spend)
* Stretch budget (how much you're willing to spend, absolute max)
* 1440p (native workspace), or 4k (large workspace, tiny UI)
* Programs you plan to use for photo editing, including the version of each program (this tells me which gamut to target)
* Do you own a colorimeter, and if not, would you be willing to buy one if within your total budget? (if yes, this devalues factory calibration a little, but expands the potential lineup of monitors)

Sorry for the questions, but I really strive to tailor my recommendations to each individual user and their unique needs, rather than resort to blanket recommendations.
> D= 2k, Q = 4k resolution

I just want to clarify this for other readers and the OP (CC for /u/poopiehead46)

* 1280x720 = HD (high-definition)
* 1920x1080 = FHD (Full HD)
* 2048x1080 = Cinematic 2K standard, not used in consumer monitors
* 2560x1440 = QHD (Quad HD, 4x 1280x720)
* 3840x2160 = UHD (Ultra HD, 4x 1920x1080, typically marketed as 4k)
* 4096x2160 = Cinematic 4k

So, the U2717D is QHD, not 2K (though this is common mistake, and even Newegg lists 1440p as 2k).
Out of morbid curiosity, what's the end goal? You are posting multiple threads a day asking a different monitor question, and you rarely if ever interact with the people answering you.

Are you farming answers for a third party site?
You'd think so, but the few I've seen are just matte displays with a plastic or glass layer over them for touch interaction. You get the worst of both worlds.

I believe that HP has/had an actual glossy VA-based display. I'll try to dig it up later if I get time.
My trusted places NEVER worked for home, so I disabled it. I'd rather not use it at all, than leave it on and be disappointed.
> Oh I see you linking to displayspecifications.com as a primary source. Hmm they also list other monitors on there. Let's look one up.

As one of 5 sources, not THE PRIMARY source. Nice try.

> Let's look one up.

> https://www.displayspecifications.com/en/model/f16c61d

> Huh that's odd, it says it's an 8 bit panel.

I was hoping you'd find that :)

Now, as you can see, someone made a typo. So, I have 5 sources backing my claim, with one typo on a different page from one of those 5 sources. You have one source backing up your claim, an obvious typo. Do you now realize what I'm getting at?

You're providing one source to your claim, which was easily debunked. I'm providing 5 sources to my claim corroborating the same thing. You are attempting to debunk one of them.

Most importantly, you're still not providing one solid source to back up YOUR claim. You are flat out wrong and that is why you're resorting to ad hominem and strawman arguments. Your pride won't let you admit to your mistake, so you take this route instead.

> I'm done with your charade. Get blocked.

I'm sorry that you feel this upset and distraught over the minor mistake that you made. But, the good thing is that since you're supposedly blocking me I won't have to deal with you crying when I correct your misinformation anymore :)
> I already fucking did by linking you to Dell's official website where they list the 1440p 24" monitor as being 6 bit + FRC and you threw it back at me as "not good enough".

I already explained why that was inadequate. As previously posted, that monitor uses the M238DTN01.2 panel, which is a known 8-bit panel. [Source 1] (http://www.panelook.com/M238DTN01.2_AUO_23.8_LCM_parameter_27073.html) | [Source 2] (https://www.twscreen.com/en/lcdpanel/8998/auo/m238dtn01.2/23.8/2560x1440) | [Source 3] (https://www.elecok.com/m238dtn01-2-23-8-a-si-tft-lcd-panel-for-auo.html) | [Source 4] (https://pcmonitors.info/reviews/dell-s2417dg/) | [Source 5] (https://www.displayspecifications.com/en/model/3c3d64a)

I mean, the official spec sheet of the underlying panel states true 8-bit, but hey, what's my multiple sources against your one typo?

> Go fuck yourself and enjoy your shit 6 bit ugly banding monitors.

Just because I'm correcting your misinformation doesn't meant that I use those monitors. You can see the monitor that I'm currently using by looking at my flair.
https://www.sbnation.com/2017/1/7/14198726/michael-bennett-kicker-shoulder-pads-seahawks
> You mean this S2417DG? That's officially listed under the Dell website as being 6 bit + FRC?

Likely poor copy/paste text from Dell. They and LG are notorious for it. The M238DTN01.2 panel is known for being 8-bit.

> Or how about the PG248Q, which claims to do 16.7m colors just like all the marketing material from Dell for their gaming panels, but in actuality is a 6 bit + FRC.

All 1920x1080 IPS and TN monitors are 6-bit + FRC. This isn't a revelation. It's also not relevant to your original claim that DELL (this isn't Dell) 1440p (this isn't 1440p) displays aren't true 8-bit.

> Meanwhile, when we look at the spec sheet for the PG279Q, we notice a neat little appendage to the color depth listing: 16.7M (real 8 bit).

Oh, because one company advertised it as 'real' 8-bit, everyone else must be 'fake' 8-bit? I'm sorry, but that's some pretty sad logic on your part.

> The bottom line is

The bottom line is that you were mistaken. Again, show me the panel data on just one Dell (not Acer) 1440p (not 1080p) display that is 6-bit+FRC.

Support your original claim without going off topic (strawman arguments) with actual evidence. Because right now, you haven't proven your claim at all.
> Just wondering why do people build one when they can buy and Alienware? Or is building one better?

Personal preference. Sometimes it's cheaper. You have more flexibility with parts. There's a lot more flexibility with customization and appearance. You get to truly build your own. No risk of proprietary parts limiting future upgrades. Also, each component generally has a warranty that is longer/better than what a prebuilt comes with.

> What's the best choice

Depends on your needs and personal preferences. Do you like the way it looks out of the box? Does it seem to be a good value for the stated specs? Are you ok with potentially more limited upgradability? If so, go for it.

There's no 'wrong' answer. There's only what is best for you.
Firmware, and EDID profile, to be more precise.
Thanks for doing that. Prior releases with this monitor could not do 2560x1440, so it's good to hear that LG has worked out that issue.
>  Is there anyway to make sure my gamma is correct without buying additional hardware?

Measuring is the most accurate way. But you can use websites such as  [Lagom] (http://www.lagom.nl/lcd-test/) to eyeball it up and, at the very least, get it to a point where you are subjectively happy with it.

Also, we're talking about a $1,000+ monitor here. I think that when you have that much in the game, consider a budget calibration option. The Spyder 5 Express (~$120) combined with the DisplayCal software (FREE) will get you very good results.
Noctua Chromax

https://noctua.at/en/products/product-line-chromax
Just to be clear:

* Your out of box gamma can vary, no two monitors are the same. TFTCentral got 2.6 out of the box, I got 2.46, and Lim's Cave got 2.4.
* Don't apply someone else's fix. If I were to apply TFTCentral's ICC profile, my gamma would be just as off, but in the opposite direction (under the desired 2.2).

If gamma is a concern and you aren't able/willing to go through the calibration process,the X34P is a better option. Even after calibration, most games bypass the ICC profile anyway. So, I have 2.22 gamma in the desktop and windowed games, and 2.42 (best I could do with the OSD) when gaming exclusive full screen.
The correct answer is to use a calibration tool. And which tool you'll use is based entirely on your budget. Generally speaking, here are your best options in order from least to most expensive, up to $250. If you have a budget more than that, let me know and we can look at higher-end options.

* Spyder 5 Express + DisplayCal ~$120. All of the Spyder 5 units have the same hardware with a software lock limiting the cheaper options. DisplayCal is free software that is more functional than what's included, and doesn't enforce the software lock. Getting any member of the Spyder 5 family above the Express makes no sense.
* X-Rite ColorMunki ~$175, a more accurate tool than the Spyder, but I still recommend DisplayCal based on my own experience with X-Rite's software.
* X-Rite i1 Display Pro ~$230, reportedly of similar accuracy to the ColorMunki, but faster. This seems to be the go-to for people who don't want to spend t$1,000+ on more professional tools. This is the one that I have, and again, I prefer to use DisplayCal.

Many people will suggest using someone else's settings from the web. Please, don't do this. Each unit varies in out of box performance, and those settings are meant to correct for that unit's individual deficiencies. Using someone else's configuration or ICC profile is the same as using someone else's prescription glasses.
Just keep in mind that each unit varies, and your settings won't give the same output for his monitor as it does for yours.

I've I've said before, using someone else's settings on your monitor is similar to using someone else's prescription glasses.
You're not an imbecile.

Assuming you're running Windows 10, please try the following.

* Click the Windows Start Button
* Click the Settings Gear Icon
* Select System (should be the first option)
* Select Display (should be the first option and you're already there)
* Under resolution, do you have 2560x1440 and/or 1920x1080 as an option? Are both there? If either are missing, which one?

Regardless, thanks for your time spent looking into this.
> But often uncalibrated monitors have more issues than just temperature.

It was one example. I didn't say that "this is the only issue."

> Using settings from online sources can help fix brightness and contrast, and overall color balance.

Assuming there is zero variance between your unit and the reviewed sample, which is a faulty assumption. A software profile created via calibration is designed to correct for the faults of that unit.

Using someone else's profile is like wearing someone else's glasses.
> Next Google settings for your monitor. I found these:

You should not do this. Uncalibrated monitors vary from unit to unit. Settings that would improve one unit could make another look worse.

Example - Your target is 6500k for white point. TFTCentral gets a unit that has 6100k. They create an ICC profile that fixes this by making the display cooler.

Then you buy a unit, and your white point is 6900k. You load their profile getting an even cooler display.
With VERY few exceptions (so few, that I can't really name any, but I didn't want to claim 100% and be proven wrong by some outlier):

* 1920x1080 (FHD) = 6-bit + FRC for TN and IPS, 8-bit for VA
* 2560x1440 (QHD) = true 8-bit or better for all panel types
* 3840x2160 (UHD) = true 8-bit or better for all panel types
* 2560x1080 (WFHD) = true 8-bit or better for all panel types
* 3440x1440 (WQHD) = true 8-bit or better for all panel types
> Are these the A2’s?

AudioEngine A2+, yes.

> To be clear your saying they DONT have good sound for cost?

I like the sound, but I'm not an audiophile. These speakers are $250. If sound quality is your primary concern, you can do better for the price.

What you can't do better than is the size/aesthetics/sound (as a combo) for the price. The closest are the Kanto YU2, which are similar in size, appearance, and quality. I preferred the look of the A2s, and the only comparison review I could find claimed that the A2s sounded slightly better.

Also, the Kantos are rear-ported, while the A2s are front ported, meaning you can place them closer to the wall. The Kantos have that annoying blue front-facing LED, which is an automatic no-buy for me. At some point, electronics makers will learn that most of us don't want a blue laser pointer in our eyes.
Ok, and are you telling us that you can change your resolution, but 2560x1440 is not an option?
I'm curious if the current revisions of this address a flaw that the launch models had, if you wouldn't mind.

Can you try to set your desktop to 2560x1440 and let me know what happens?
They appear to be the same hardware, but the bundled software locks the Express into doing less for you. You can download DisplayCal for free, at which point, both tools have the same capability.

The Spyder 5 Express + DisplayCal is a great budget option. If you're considering paying the price of a Spyder 5 Elite, get something better, like what was recommended earlier in this thread.
> Don't fall into the trap of thinking you can do a software calibration, you can't. You need hardware to do it.

Also, don't fall for the trap of confusing the meaning of software and hardware calibration (no offense, I'm sure you know the difference, but your wording isn't correct).

* Software calibration = using a hardware tool (like the ones you recommended) combined with software to create a profile to color match your system to the monitor. Basically, the monitor output is flawed and you are using is a tool to make the system's representation of color equally flawed but in the opposite direction so that the end output actually looks correct.
* Hardware calibration - Using a supported hardware tool to calibrate the LUTs on a monitor, rather than calibrating a designated system to a monitor. Most monitors don't support this.

Software calibration means that the monitor appears calibrated only when connected to the designated PC, and any other connected device (Xbox, Blu-Ray Player, etc.) will NOT have calibrated output. Hardware calibration means that any connected device will benefit from the monitor's calibration.

What you described as "software calibration' is merely eyeballing. And you're correct, most people end up getting it wrong.
> Good advice, but skip the Spyder line of products. They really suck at proper calibration.

The Spyder 5 has gotten mostly positive reviews and is a step in the right direction. I agree that it's not as good as your recommendations. But since the Spyder 5 line all uses the same sensor and the features are just software locked, getting the Express (~$120) and using DisplayCal (free, doesn't enforce the software lock) yields the best results for those who want calibration on an extreme budget.

I actually use an i1 Display Pro and recommend it wholeheartedly, but the OP seemed to want to limit cost as much as possible.
You're likely correct. I was going off of this.

https://techreport.com/news/31954/agon-ag251fg-can-do-2560x1440-or-240hz

And it appears that they may have misread an unofficial spec sheet prior to launch.

Thanks for the heads up.
Calibration tends to drift over time. Some recommend that you calibrate annually, while others recommend monthly. If you're using your GPU's driver to assist in calibration, even a driver update can mess things up.

You could pay a professional to calibrate for you, but these costs border on a budget calibration tool, so why bother?

For solid, entry-level calibration, the Spyder 5 Express is ~$120 and the DisplayCal software is free. This will set you up at a reasonable cost without making you dependent on others.
>  not all 1440p panels are real 8 bit. The Dell ones are fake 6 bit + FRC.

Source?

The S2716DG uses the M270DTN01.5, which is a known 8-bit panel.

The S2417DG uses the M238DTN01.2 panel, which is also a known 8-bit panel without dithering.

Their 1440p Ultrasharp displays use a variety of LG AH-IPS and Samsung PLS panels, all 8-bit.

Which Dell 1440p are you talking about that uses a 6-bit panel with FRC?
Sort of.

The AOC Agon AG251FG is a unique monitor that can do 2560x1440 @ 144hz, or, 1920x1080 @ 240hz. As far as I know, it's the only monitor that has this dual mode (to be clear, it's the only one I'm aware of, there COULD be more).

While unique, I know this isn't what you're looking for. But figured I'd mention it just in case it interested anyone. And yes, I'm aware that interpolation will be an issue when run @ 1080p.
>  Maybe there's a combination of drivers/GPUs/inputs that causes it.

The EDID of the monitors didn't report it as a supported resolution, so it didn't work. Different users, drivers, GPUs, all had the same issue.

Also note that during the same week I also had the Samsung C34F791 on my desk for testing. So, same system, software, and drivers, but it didn't have this issue.

> I mean supporting it without custom resolutions would be great, but is there really an issue if it works with a forced resolution?

Yes, there were two main issues.

1. A layman wouldn't know to download third party software to force a resolution.
2. As outlined in my prior linked post, the 34UC98 exhibited issues when running 2560x1440. Specifically, the monitor ran at 75hz at 3440x1440 and 1920x1080, but was limited to 60hz at 2560x1440. Attempts to run higher resulted in frame skipping. It also limited the FreeSync range at this resolution to 55-60hz, which is laughable. Attempts to expand the range using CRU led to extreme flickering.

I have no such issues on my current AW3418DW.
> I know it supported it for a fact

I'm not calling you a liar, just asking you to provide a screenshot because I'd like to see it for myself. It would be educational.

> I googled it to see if it was specific to the 34UC99 out of curiosity, but the only person I've seen mention it is you...

> I can't think of any reason why it wouldn't work.

I'm sorry that you don't believe me. I'm merely relaying my experiences, and I have confirmed this with my own testing and with other users of the monitor.

-----------

You found one of my more recent posts, but I started digging for some older content related to it. Here's a few that I've found.

https://www.reddit.com/r/Amd/comments/5ld421/impressionspartial_review_lg_34uc98w_freesync/

This was when I did some testing on the LG 34UC98-W (I was incorrect on the model number earlier when I stated the 34UC99-W, sorry). My 16:9 experience is outlined clearly.

Here's an example of the first run Acer X34 having the same issue (CRU was needed to force the resolution) - https://www.reddit.com/r/ultrawidemasterrace/comments/3l3kmh/displaying_2560_x_1440_on_an_ultrawide/

Same issue, Acer XR341CK - https://www.reddit.com/r/ultrawidemasterrace/comments/3rh7yb/able_to_play_games_in_2560x1440_on_an_acer_xr341ck/

-----

I apologize for not having more links. Reddit's search sucks, and I'm basically looking for comments/conversations from 15-18 months ago (give or take). But it seemed like early generation Acer 1440p Ultrawides, and LG's first few entries at least (at the time a few of us confirmed LG's 98, 88, and 68 series as lacking 2560x1440 support without CRU).
> Are you sure that's not an issue with your GPU or a specific input you were using?

Yup. Testing with multiple GPUs and confirmed with other users of the same models.

> It'd be really strange one of their first ultrawides (34UM95) supported it and another one after it did (The 29" one I was using), but they suddenly stopped supporting it

To be clear, the 29" is 1080p, and the 34" 1440p models also support 16:9 1080p. The only issue I've seen so far is 2560x1440 support on 3440x1440 models.

So, just to confirm, you're not using CRU or any other program to set a custom resolution? Would you mind taking a screenshot of the driver showing 2560x1440 as a selectable desktop resolution?
> My old 34UM95 did, and the 29"

On a few of their models that I tested (the ~~34UC99-W being the most recent~~ 34UC98, mea culpa), you couldn't set a desktop resolution of 2560x1440, or select that for games that weren't running in windowed mode. 1920x1080 was the highest available resolution which led to interpolation.

I have yet to try an LG monitor with a 3440x1440 resolution that allows the selection of 2560x1440 without the use of custom resolutions.
I have two of these. Amazingly effective in both performance and acoustics.

I "downgraded" from a Kraken x61 AIO (280mm radiator) to this. Temps are lower, and there's less noise. It's pitiful just how good this is compared to an AIO that cost roughly 4x as much.

NOTE: I'm talking about temps after an hour or so. Yea, the AIO will have lower temps in the first 5 minutes due to the way that water absorbs and retains heat. That's not the proper way to measure.
Stop being wrong and I'll stop correcting you. Deal?

Here's a thing about the internet. If you post a reply to someone, they have the absolute right to reply to you. If you continually post incorrect information about monitors in this subreddit, I will continually correct your information.

Don't like it? You can stop replying to me, in which case I'd have nothing to reply to. Or, if you feel I've crossed a line, click the report button below this post and a moderator will review it (the mods here are really good when asked to look at something).
> Well, OP took the photo and said 'look at this backlight bleed I'm dealing with'. I think it's pretty obvious what's in the picture is at least similar to what he's dealing with

Just because OP confused BLB and off-axis glow in his description doesn't mean that we should assume it's BLB. I refuse to pretend that I'm ignorant on a topic just to make someone feel better about a minor mistake on their part.

And you shouldn't play along either. I mean, if I told you [THIS (WARNING POTENTIALLY NSFW)] (https://imgflip.com/meme/Unicorn-MAN) was a unicorn, would you play along and say, "Yup, that's a unicorn."?
Just repeating my other post here, for those not following the entire topic:

-----

We can't tell from his picture how much of that is off-axis glow and how much is actual BLB.

If he were to follow my advice, he'd be able to separate the two. You should never judge a monitor based on photos like the one taken by OP.
We can't tell from his picture how much of that is off-axis glow and how much is actual BLB.

If he were to follow my advice, he'd be able to separate the two. You should never judge a monitor based on photos like the one taken by OP.
LG pre-calibrates, which matters to some. However, LG's usually don't support 2560x1440 exclusive fullscreen for legacy (non-21:9) content.
> backlight bleed

Backlight bleed is a defect due to poor material quality, build quality, quality control, and/or physical damage. Panel doesn't matter. You can have backlight bleed on TN, VA, or IPS.

Off-axis glow is a side-effect of better viewing angles. The same physical feature that allows for better viewing angles (the way that light is allowed through the panel) is what causes the off-axis glow. In terms of off-axis panel glow, IPS is the worst, followed by VA, then TN.
Just as a follow-up, per his other post the reason that FreeSync isn't working for him is because he's using an Nvidia GPU.
You cannot use FreeSync with an Nvidia GPU.

* Nvidia = G-Sync
* AMD = FreeSync

There's your issue.
> Using the monitor to watch a movie isn't deliberately sabotaging the experience lol.

I didn't say that it was.

Cranking the brightness in a dark room would be the sabotage that I was referring to. That should have been obvious, given that I said,

> In a dark room there should be some form of bias lighting with the monitor set to a proper level of brightness.
That would be a poor choice. In a dark room there should be some form of bias lighting with the monitor set to a proper level of brightness.

What you're describing is deliberately sabotaging the experience. To better illustrate our conversation.

You - "I'm having a hard time getting this hockey puck into the basketball net."

Me - "Try using a basketball instead?"

You - "But I'd rather do it with the hockey puck. The puck must be broken. It's a piece of junk, so I'll send it back."
The picture doesn't represent what we'd actually see. Cranking the display brightness, with a black wallpaper, in a dark room, and taking the photo with a cell phone isn't a great way to shows off-axis glow or backlight bleed.
They use a similar panel and neither are calibrated, so they will be subject to the same variance. But the ViewSonic has two advantages:

* Better warranty/customer services (based on anecdotal evidence)
* Better reinforced edges, reducing the odds of additional backlight bleed

Basically, if you consider these monitors to be lottery tickets, the ViewSonic has better odds of being a winner.
Your brightness is too high and you have no bias lighting.
Additional context.

OP:

> Please, for your future "guides", put the conclusion AT THE TOP.

Mod:

> Pester someone else, troll.

OP (comment removed from thread, visible in OP's history)

> It's just a simple request. It's not like I have any power over you mods,so all I can do is ask. You already do what you want, there's no need to be mean.

Now, my opinion. Your first message came across too strong. The all caps does make the message appear as rude to some. A better post would have been, "As a suggestion, perhaps a TLDR/Conclusion at the top to give the reader an abbreviated guide up front. Regardless, great work. "This would come across as more complimentary than confrontational.

That said, the moderator's response was not proper. Also, your response to him was not out of line and in fact came across as polite.

But you pretty much have no recourse with that subreddit. The moderator who acted childish towards you is the most senior mod of that subreddit, so he's not getting overruled. Your shadow ban doesn't extend beyond that subreddit (or I wouldn't be able to see your post, as I'm not a mod here).

Basically, unless they begin to harass you outside that subreddit, there's nothing that the admins can do. But take solace in the fact that you did nothing (that I can see, at least) to warrant a ban.
Loving the MX518, my all-time favorite mouse. Since I couldn't find another after a near-decade of use, I had to go through a few mice before finding an acceptable replacement.
Try the following.

* Set brightness to 25% - Per TPU's review, this will get you close to the ideal 120 nits. If you're in a very bright environment, go as high as 50% (closer to 200 nits), but please don't go above that. If you're used to overly bright displays, this will look dull at first. Use it for at least 3 days at ~25% before judging it. Get used to it first.
* Make sure you're sitting at a good viewing distance. I'm probably 24" from my display, which is a bit close. Aim for 2 feet minimum, 2.5-3ft preferred.
* Sit with your head at a proper height relative to your display. Your eyes should be centered horizontally, and vertically one third to one half down from the top of the display (the top-third rule is for TN displays due to their vertical viewing angles). The higher that you sit, the worse that the bottom corners will glow.
* Get some form of bias lighting. A lamp with a 6500k bulb is preferred for most. I use a Luminoodle 6500k USB light strip adhered to the rear of my monitor.

Doing all of the above actions will make backlight bleed and off-axis glow virtually disappear except in the most extreme cases. So, give it a shot and please report back.
You get a more accurate picture, which lets you see what the artist intended. A few things you'll adjust through proper calibration are:

* White point - if this is off, you whites (and overall image) could look overly warm (red-ish) or cool (blue-ish).
* Gamma - Proper gray-scale tracking. If this is off, your image will look overly dark or blown out.
* Color accuracy (Delta E) - The more this is off, the more inaccurately your colors will display.

Note that accurate doesn't necessarily mean better. Accurate colors are OBJECTIVELY better, but not necessarily SUBJECTIVELY better. In the latter case, you could have a personal preference for inaccurate, overly vibrant images (many people do).

Also, most software calibration is done through the display driver (can be reset by a driver update), an ICC profile (can be bypassed by many full screen games), or a combination of the two. Basically, calibration for gaming has its pitfalls.
Is there any chance that you're using an Nvidia GPU? If you are, you won't have FreeSync as an option.

If you are using an AMD GPU, FreeSync should work with this display. The documentation isn't ideal here, so here's what I'm gathering:

* You MUST use the HDMI input in order for FreeSync to work
* There's no on/off for the OSD, FreeSync is ON if you use HDMI and a compatible AMD GPU
* FreeSync must then be enabled in the Adrenaline drivers
* You can verify it's working by going into the OSD, selecting OSD (below Gaming and above System), and turning on the "Refresh rate num" setting, which will display your refresh rate at all times. If you set refresh to 144hz, and you're gaming, this number should change to match your frame rate if FreeSync is in use.
Unless you want to go ultrawide, the ideal setup for your system would be a 2560x1440, 144-165hz G-Sync display. And right now I would recommend one of two options.

* [Dell S2716DG] (http://www.dell.com/en-us/shop/dell-27-gaming-monitor-s2716dg/apd/210-agjr/monitors-monitor-accessories)
* [ViewSonic XG2703-GS] (https://www.newegg.com/Product/Product.aspx?Item=N82E16824116827)

The key difference between the two is that the Dell uses a TN panel while the ViewSonic uses AHVA (IPS-like). Get the Dell if you would rather save money. Get the ViewSonic if you want better picture quality. Both are arguably the best of their respective panel options in this segment. Sure, there are better TN options than the Dell, but they cost as much as or more than the ViewSonic.
Slightly warmer on the left than the right. Noticeable on an all-white background when looking for it, or if I sway left to right (again, with an all white background). Not noticeable in regular usage.

If you have to look for a flaw to see a flaw, then it's not a flaw to be concerned with. My inability to see it could be due to a good panel (luck), an inability to notice (ignorance), or a combination of the two.
Hard to tell. Dark room, black image, monitor likely at a high brightness setting. What we're seeing isn't going to be what you see.

When it comes to off-axis glow and blacklight bleed, despite the poor drawing here, this is the definitive guide.

http://puu.sh/lDz83/b31a64b5e3.png
> 1440p is almost 50% more pixels for your GPU to render though. 

Just to add to your point, and correct your math, 1440p is 77.78% more pixels than 1080p.
Yup. This panel is LG's first foray into 3440x1440 high refresh (their first was 60hz native, with a spec allowing overclocking). They have two more planned revisions to this panel, a late 2018 revision that has 144hz native refresh (no announced monitors using it yet), and a 2019 revision that is 144hz + HDR support.

Perhaps the kinks will be ironed out then (and other issues will rear their heads). But I'm happy with this one.
> most people can't afford 13-GPU mining rigs, thats the whole damn point

Wow, it's almost as if I said:

> The people hurt the most are those who don't mine with more than one or two GPUs.

You're expending a lot of effort to argue a point that you actually agree with.

I'm not saying that I want ASICs to take over mining. I agree with you on that point. It would concentrate too much for those who are already wealthy (IE, what we're seeing with the US wealth gap).

But I would still like to see cheaper ASICs go mainstream. An ASIC that could mine like 6 GPUs, for the cost of 2 GPUs, with the power draw of 1 GPU would be:

* easier for miners (easier setup)
* better for the environment (mining isn't going away, but let's reduce the power draw at least)
* better for gamers (miners and gamers aren't going after the same part)

Hopefully, I've clarified my point a little better this time.
> you would need 100 of the BEST Cyptonight mining GPU's to match 1 of the Bitmain X3's.

And during the mining peak, 100 of those GPUs would cost over $100k. An X3 is currently $12k. That's more accessible.

If you can afford a 13-GPU mining rig, you can afford a more powerful ASIC. The people hurt the most are those who don't mine with more than one or two GPUs.
Using a macro lens worked for me.
>  1% more than Wear OS, and virtually no developer support?

1% more successful DESPITE being sold from one vendor, and having less developer support.
They get their Google payment account suspended too? :)

(But seriously, thanks for the heads up)
> Nowhere even close, btw, asics are impacting the gaming space right now, in a very direct manner. Bitmain is worth nearly as much as Nvidia and bids for a lot of wafer space.

Weird, it's almost as if I said this...

> Why don't you get your facts straight before blabbering further down the road?

Don't re-word my post to agree with me, then claim my facts aren't straight.
Nope, because again, a GPU farm can do the same damn thing.

I'd prefer that ASICs go mainstream. I could build a mining rig that consumes far less power while mining at a higher rate, and it wouldn't impact the gaming space as much (it would still have an indirect impact due to the need for wafers and other components).
Just one? How is this fully ascending!?

/s
Paging /u/PixelCommunity again (name tagging in an OP doesn't give them a notification).
You should probably drop the 1ms response time requirement. It's a misleading, poorly advertised spec and there is no monitor on the planet with an actual 1ms average response time.

Also, most users misunderstand what response time is, often confusing it with input lag. The truth is that at 1440p and 144hz, the difference between today's TN (advertised as 1ms) and AHVA (advertised as IPS with 4-5ms response time) is negligible. The AHVA options tend to be around 5-6ms real world average response times, and the TN options in the ~3ms range. But the TN options have more aggressive overdrive, leading to more overshoot ghosting and slightly higher input lag.

Basically, balance your needs for picture quality (AHVA) and price (TN) to isolate what you want. One of the most popular TN options, the [Dell S2716DG] (http://www.dell.com/en-us/shop/dell-27-gaming-monitor-s2716dg/apd/210-agjr/monitors-monitor-accessories), is routinely on sale for ~$400-$450 and will meet your desired specs. For the AHVA crowd, the [ViewSonic XG2703-GS] (https://www.newegg.com/Product/Product.aspx?Item=N82E16824116827) tends to be the most highly recommended, is finally available again from mainstream etailers, and is at the edge of your budget.
You're looking at a 60hz monitor. That means the interval between frames is 16.67ms. As long as your response time is below that number, ghosting will be minimal.

Despite the (usually falsely) advertised response times, most non-gaming monitors have a real-world average response time of ~5ms (TN) or ~10ms (VA, IPS) for their default settings.

You'll be fine.
I won't be touching another LG product. I was actually considering their upcoming gaming monitor (34GK950G), but ended up with Dell's equivalent, the Alienware AW3418DW.

It does use an LG IPS panel, but that's virtually unavoidable in this segment. But I will steer clear of their branded products for the foreseeable future.
He just posted this:

https://www.reddit.com/r/DrNCX/comments/84r3jt/wecravegamestoo_downno_best_reviewed_flicker_free/
Nope. While the contract in this case likely pays out the same amount, the conversion to signing bonus means he is paid now instead of in the future. But because of the way the accounting works, the bonus (which again, is paid up front) gets spread out for cap purposes. This actually makes it harder to cut the player due to dead money.

Bottom line, player gets paid up front, but team frees up immediate cap room to the detriment of future salary caps.
I'd say that politicians will never learn, but clearly they did learn. Last time Republicans did this, they succeeded in blaming Democrats for the fallout and they will again.
I have an extremely homophobic neighbor. Every year in spring I wear a bikini and really go at it for that first car wash.

I'm a straight white male.
> It's still significant because they're getting their money later instead of now.

In most cases the salary this year is converted to signing bonus. So, they actually get their money earlier (now, instead of via game checks during the season), but the cap hit is spread out over multiple years. Both parties benefit, because the bonus becomes guaranteed at signing whereas the game checks for a Veteran don't become guaranteed until week 1 (and for younger players, aren't guaranteed at all).
As a person who prefers IPS and hates TN, I'd be ok with this monitor. It's very good in person. I did have it next to my U2717D (Samsung PLS, factory calibrated) and the S2716DG wasn't as good in comparison. But if you're not running dual screens with an IPS next to it, it's a solid display.

For 1440p, 144hz, and G-Sync, you can't really do better at this price.
At the moment, I'm running Need for Speed Most Wanted (2005) with the widescreen and controller fixes. It's a new experience for me, and an absolute blast.

That said, the game still has warts and is showing its age. I'm almost done with it (again) and will move on to other games soon.
An Aeron is an ergonomic chair. Because our bodies all have subtle differences (or not so subtle, in some cases), one person's ergonomic is another person's torture.

An Aeron is one of many attempts at healthy seating for office use. It succeeds for some, fails for others. But a racing chair, which is what I was discussing, isn't meant for office use.

I do enjoy reclining in mine when gaming with a controller. But it's not so much fun when I'm working, or gaming with kb/m.
Correct. With the costs of 4k going down, it's now becoming more expensive for a 1440p panel than a 4k panel. As a result, more manufacturers are abandoning the resolution.

It's a damn shame too, as it's the best resolution for standard monitor sizes. I'm already seeing some games (mostly console ports) where you can run 1080p or 4k, but not 1440p. It pisses me off.
I agree that it will last US awhile. What I meant by dying is that fewer and fewer panels are being made with it. LG ceased production of 2560x1440 monitors a few years ago. Samsung left the market as well. AUO has exclusive control of the 2560x1440 'IPS' space, and that lack of competition has led to higher price and lower quality.
How would you have felt about spending $400-$500 for a GTX 970 in early to mid 2016?

if Nvidia's trend continues, the new x60 GPU should offer ~1070 performance (it tends to alternate, and is unlikely to match the prior x80 twice in a row). You'd spend $420 for a GPU that would be outclassed by a (hopefully) cheaper GPU in likely several months.

Unless what you have is an abomination, I'd wait. Also, maybe look into monitors as well. You're considering a 1070 for 1080p 60hz, and I'm running 3440x1440 100hz G-Sync on a GTX 1060. One of us has room to grow for our GPU upgrade, and one of us really doesn't.
They lack proper neck and lumbar support. They are ergonomically bad for your body. I got one based on looks and have regretted it ever since. I should have listened.
The link that I provided shows all grades and ranks them, giving further explanation on what they are.

1.  Full-Grain Leather
2.  Top-Grain Leather
3.  Suede
4.  Correct-Grain/Genuine Leather
5.  Bonded Leather
Correct. Not sure why someone downvoted you.

It's easy on a 1060 to keep temps low with a medium to high fan speed and a low power limit.
> I now prefer comforts to looks.

To be clear, the desire for mesh and a headrest for me are comfort items. I just want to get looks with it, if possible :)

After the Vertagear, comfort > looks for me, but if I can afford both I'll go for it.
Different strokes. I prefer mesh chairs over the more plush variety, but thanks just the same. I also want a headrest, so a tall back or detachable headrest.

Also, if possible, I do want it to color match my white/black build.

The Eurotech checks all blocks, and is in the same price segment as the Serta.
That's why I suggest a budget to work with.

The go-to for most in the under $200 space seems to be the [Ikea Markus.] (http://www.ikea.com/us/en/catalog/products/00103102/) I've sat in it and it didn't wow me, but I also didn't have any real complaints. It's a safe bet in the affordable office chair category.

I could see myself getting one for my upstairs telework office, since I'm currently using a $50 Ikea chair there and it's starting to fall apart.
If I had gotten a Maxnomic, I probably wouldn't hate racing chairs as much as I do. I went with a Vertagear. The included pillows are just bagged rocks, so you can't use them, leaving pits where pillows should be. Horrible ergonomics.
Not OP, but any recommendation would start with a budget.

As for me, I think I've settled on the [Eurotech iOO] (http://www.eurotechseating.com/products/premium/fabric-mesh/ioo-all-mesh-and-fabricmesh-combo) in white, with a headrest. Looks like it's going to be $592.00 before shipping and any sales tax. The price listed at the linked product page is just to make it look like the chair is always on some sort of mega sale. It's routinely $560 for chair, $50 for headrest, or $592 for the combo.
https://www.heddels.com/2014/06/overview-guide-leather-grades/

Bonded leather is just scrap leather put together. It is the particle board of leather, and the lowest known grade.

If it was rated for 6-8 hours of daily use, did they specify a number of days? Because this stuff isn't meant to last long with prolonged use.

FYI, "Genuine" leather is also just a grade, and not even the top grade. It's 2nd worst.

> Simply put, corrected-grain or “genuine” leather has had an artificial grain applied to its surface. For those of you who are looking for high-quality leather goods, this would be a negative. The leather-like pattern is impressed into the surface and then sprayed with stain or dyes to give the fake grain a more natural appearance.
~~I've often wondered how he feels about this. IIRC, he asked to be cut so he could pursue starting opportunities elsewhere. And because of the cap hit, it actually hurt us financially to cut him AND sign Foles. We did it at his request. It was the noble thing to do.~~

~~So yea, how's that working out for him?~~

**EDIT: Reading up on it, I had it wrong. Daniels requested his release AFTER we signed Foles. Mea Culpa**

http://profootballtalk.nbcsports.com/2017/03/13/chase-daniel-asks-eagles-to-release-him/
* $1,000 monitor? Yup, AW3418DW
* Cheap desk? Nope, ~$1,200 Ergo Depot adjustable height desk
* Racer chair? Unfortunately, yes, but I'm going to get a real chair sometime this year.
> The contrast is actually the brightness,

> The monitors brightness setting is actually the black level.

I don't know where you are getting this from. If this were true, DisplayCal would detect minimal to no brightness change when adjusting the monitor's brightness setting.

Additionally, when adjusting a backlight's brightness, you're also adjusting it's power draw. When adjusting a monitor's contrast, you are adjusting the representation of white/black. My Kill-A-Watt P3 4400 shows power draw going down as I reduce the monitor's brightness setting, but no impact outside of margin of error when adjusting the monitor's contrast setting.

TFTCentral's notes mirror my own.
I know that HDMI does support compression but I don't know its exact limits.
> doesn't hdmi 2.1 support full 4k 144hz without that compression causing input lag. Don't think any devices are using it yet though.

https://www.hdmi.org/manufacturer/hdmi_2_1/

> Higher video resolutions support a range of high resolutions and faster refresh rates including 8K60Hz and 4K120Hz for immersive viewing and smooth fast-action detail.
In a budget monitor, the difference between $200 and $300 is significant. But when you look at mid-range gaming displays, the difference between $600 and $700 isn't as drastic.

It's like this with power supplies. You don't NEED 550W to run most systems. My i7-7700k + GTX 1060 uses less than 200W when gaming (measured, not speculated). But try getting the features/specs of an EVGA G3-550 in a 300W PSU. You can't. There's no point, because such a PSU would only be $5-$10 cheaper due to the cost of other components, so at that point, who would buy the 300W version?

Hell, I bought my G2-650 because it was on sale for the same price as the G2-550.
> I’m not sure DSC is compatible with CTA 861.3 so HDR might be an issue it also going to add quite a bit of latency judging by the spec.

I've updated my post while you were posting this. DSC enables 4:2:0 or 4:2:2, if necessary. If DSC can't do it, then your suggestion won't work either, simply because they are essentially the same thing in this case.

Also, it's telling that VESA was demoing the latest DSC version at CES 2017 while this monitor was being shown off.

The bottom line is that you're right, 4:2:2 is likely going to be used, because that's a core component of DSC, meaning that I'm also right :)
> More likely they’ll do 4:2:2 since that mode is also supported on DP 1.3 and you can push 144hz at 4K.

Display Stream Compression (DSC) supports those modes, so they will be used if necessary. The monitors are confirmed to be using DisplayPort 1.4, so they will likely support the latest DSC version at release.
My guess hasn't changed. I expect them to hold onto Foles until Wentz's timeline is better known. Given the nature of the injury, best case is mid-August, and worst-case is mid-December. (actual worst case is something that I don't want to even think about)

## If Carson is ready by August

Once the FO becomes aware that Carson will be ready, they might ship off Nick to a QB-needy team. I don't see them accepting anything less than a 2nd under this scenario. But given the teams who would suddenly be interested that weren't before, Howie will likely aim higher, ala the Bradford trade.

## If Carson is ready by Mid-Season

I could see Philly trying to swing a trade by the trade deadline to a contender who just lost their starter (IE, letting Nick try for a repeat). This would benefit Nick as it would allow him to essentially start almost all season before entering free agency. I could see Howie accepting a 3rd at worst in this case, if they are REALLY confident in Nate Sudfeld and they just want to do right by Nick.

## If Carson is ready by December

I think in most scenarios here, they stick with Nick. Already out of the playoffs? Why risk Carson for that. Already secured HFA? Let Nick ride out what he's earned. On the fringe? That's where it's a tough call.

But if Nick has started all year, and he departs in FA, and we don't sign a FA QB to start for us (Why would we? We have Carson), then most guesses at the compensatory formula have us getting a 3rd.

This is the scenario that Howie is most worried about, IMO, and what's driving the rumored demand of 'at least a 1st rd pick." If the worst case is that Carson is out all season, do you let Nick go for a 2nd or 3rd and ride Nate? Or, do you ride Nick and get an automatic 3rd?

So that's why I believe that Nick is stuck here at least until mid-July when the team has a better feel for Carson's recovery window. But ultimately, Nick isn't getting traded until Carson is fully healthy, unless a team gives us a king's ransom for him.
Yes, it's fine. It's well within spec for the hardware. A constant, sustained temperature is better than a fluctuating temperature.

Other components on the board (typically fans) will fail long before the GPU itself, in most cases.

I have mine at 75-76° while mining, but it fluctuates from 72° to 79° while gaming.
The displays in the article, (the 4k 120/144hz ones) have 384 zone dimming, just FYI.
Displayport 1.4 is limited to 4k at 120hz, so this monitor (and the Acer equivalent) use display compression to achieve 144hz.

You'd be trading a small (likely unnoticeable) amount of picture quality for the extra 24hz.

The compression is listed as "virtually lossless," and is an official part of the DP 1.4 spec.
I felt dirty even linking them.
How much BTC are you earning?

For example, at the moment my GTX 1060 is earning:

* DaggerHashimoto (Dual) - 0.00010752
* Decred (dual) - 0.00000980
* Combined - 0.00011732 BTC per day

It would take 9 days at this rate to earn a payout, compared to the 4 days when I started in January.

Ignore the fiat measurement, we need to see your hashrate and BTC earnings. Also, what's the time frame? If this is an overnight drop, that's significant. If this is a drop from 4+ weeks ago, that's pretty typical or close to it.
> Just letting you know I think your response is trapped in a spam filter, though Im not sure exactly what about it would get it caught

Many of our comments were hidden by moderators for reasons their own, and they didn't notify us. You can see this by viewing this topic while logged out.

> *Really... a downvote for alerting you... alrighty then

I am just now seeing this response, which is saying it's 23 hours old. I didn't downvote you. But for what it's worth, it seems you have positive karma now.
In almost any article about HTC, BGR continually says that HTC copied the iPhone 6 design with their HTC One series. It has to be added in any article about HTC for some reason.

BGR got it right in this piece - http://bgr.com/2015/10/22/iphone-6s-copy-htc-one-a9-statement/

But ever since, they add trollish comments to their articles about how even the HTC One was a stolen design from Apple. In most articles about Android phones, they always have to mention how it in some way copies Apple. Here are some examples:

* http://bgr.com/2017/03/07/google-pixel-2-rumors-vs-iphone-7/
* http://bgr.com/2017/02/22/galaxy-s8-tricks-for-windows-10/
* http://bgr.com/2017/02/20/pixel-xl-vs-iphone-7-plus-design-comparison-feedback/
* http://bgr.com/2016/09/27/iphone-8-rumors-fingerprint-scanner-mi-5s/

In the last article, they are talking about a company announcing a new phone, before the iPhone 8 was even announced or confirmed, saying that it is clearly copying Apple's new iPhone 8 based on rumored specs.
> 4ms input lag

It's going to be extremely difficult to confirm that. The number advertised by monitor manufacturers is in terms of response time, not input lag (and it's often falsely marketed anyway).

Very few review outlets have the capability to measure a monitor's input lag, with most measuring a combined value of system lag + monitor input lag + monitor response time. TFTCentral has the most accurate measurement currently, so you'd have to limit yourself to only monitors reviewed by them if you care about this number.

Thankfully, most modern budget and gaming monitors have low input lag. Gaming monitors, for obvious reasons. Budget monitors, because they cheap out on the signal processing features that would otherwise introduce input lag. Even the Dell U2715H, a monitor aimed at prosumers for color critical work, has a ~7ms input lag (the successor, the U2717D, was at ~21ms).

Basically, if you focus on non-professional monitors, you should be good. But if you REALLY care about this spec, then limit yourself to monitors reviewed by TFTCentral.

>  I got a Samsung CFG70 for a second monitor (which I just use for videos not gaming) and the colors are just so much better! I casual speed run old emulated games so input lag is insanely important to me.

Samsung's CHG70 lineup of monitors has absurdly low input lag, with the C32HG70 measured at 0.5ms per TFTCentral - http://www.tftcentral.co.uk/reviews/samsung_c32hg70.htm#lag

While TFTCentral never measured the VG248QE, they did measure the VG278HE, the 27" version (almost) of this monitor, and found it to have a signal delay if 8.45ms at 144hz - http://www.tftcentral.co.uk/reviews/asus_vg278he.htm#lag
It's HTC. Per BGR, HTC used the time machine to copy the design of the iPhone 6 (Sep 19, 2014) for their HTC One M7 (Mar 22, 2013).
You heard incorrectly. The brightness setting on the AW3418DW does in fact alter the brightness level of the backlight.
> A lot of people say both are normal within reason, but I've seen people mention they have gotten flawless screens

The vast majority of people who feel their screen is flawless either don't know what to look for (in this case, ignorance is NOT an insult), or, they know what to expect and what they have is very good or at least acceptable for their needs.

Mine has typical off-axis panel glow (AKA, IPS glow). It also has a few spots with some BLB (this is not the same as glow). All of this is mitigated by:

* proper distance and posture
* running backlight at 120cd/m2 instead of max brightness (if you can't measure, most units I've seen tested hit 120cd/m2 at 35-40% brightness, mine is calibrated with the setting at 37%)
* proper lighting in the room or, when dark, using proper bias lighting

My unit has both glow and BLB. But it's not noticeable TO ME in standard use case scenarios because of the above. But if you take a picture of your monitor with your cell phone, in a totally dark room, with an all black image, at maximum brightness, it's going to look bad. It also won't be anything close to a real world representation of what to expect in a proper use case scenario.
Just an FYI:

Quantum Dot has no real benefit for gamers (in most cases). Its purpose is to cheaply expand color gamut. Your games are mastered for sRGB, and these VA panels hit at or near 100% sRGB without Quantum Dot. So you're either going to be properly calibrated and not getting the benefit of Quantum Dot, or, you're going to have overly vibrant and wildly inaccurate colors.

EDIT: Additional clarification. Based on the down votes, some of you probably feel that my information isn't accurate (though I'm sure at least one of you was just upset). Hopefully this additional information better illustrates my point.

The entire purpose of Samsung's Quantum Dot (and LG's comparable Nanocell) is to extend color gamut. When we made the change from CCFL to LED, color gamut was lost. W-LED (what's used in most consumer monitors) makes it more difficult to show wide gamut. For professional monitors, colored LEDs are used for backlighting.

Quantum Dot and Nanocell are a cheaper way to accomplish this (though not as effective, which is why professional monitors still prefer RGB LED). So, your gaming monitor, in theory, can run a wider gamut (Adobe RGB or DCI-P3), though not with 100% accuracy.

The problem is the same as it's always been. Your games are mastered for narrow gamut (sRGB). If you run a game on a wide gamut display, you're going to get overly vibrant, over saturated, incorrect colors. Now, these gaming monitors are tuned for sRGB use. Meaning, they are designed to self-negate the presence of Quantum Dot.

This makes QD a marketing point. Like '1ms response times,' QD is a ploy to make you think, "this must be better!"

EDIT 2: Where is there potential for Quantum Dot to help? If they can expand the colors within the same gamut, IE, 8-bit (16.7M) vs. 10-bit (1.07B) within the sRGB gamut, they can make HDR more accessible. Range of color is one of the primary benefits to HDR (extended range of luminance being the other). But as we've seen with PC monitors purporting HDR support so far, having one without the other often results in a subpar experience.
That's a 60hz native panel supporting an OC up to 100hz (results not guaranteed). The AW3418DW and X34P ($1,000-$1,200) use 100hz native IPS panels supporting an OC up to 120hz.
Neither LG, Samsung, nor AUO have had any interest in making a 1080p high refresh IPS/PLS/AHVA panel. If they don't make them, no  monitors can use them.

The price difference between a 27" 1080p and 27" 4k has gotten smaller. The price difference between 24" and 27" 1080p is tiny. If you can get a 27" 1440p 144-165hz AHVA (IPS) G-Sync monitor for $699, a 1080p 24" would probably be $599. People would complain about the price and go 27" anyway.
For today, not really. Most of today's HDR monitors, even those with HDR10 certification, are still using edge-lit LEDs, so they can't control their backlight well enough to impact the luminance. That means their effective contrast is typically no better in real world use than a non-HDR display.

They do typically support the wider color range, and this can make a difference. The overall picture quality with a proper HDR signal will be better, just not mind blowing better.
UHD is describing the resolution. A UHD monitor has a resolution of 3840x2160, also often marketed as 4k.

An HDR monitor is a high-dynamic range display. These are capable of displaying a higher range of color and luminance giving you greater detail and contrast.

A display can be both UHD and HDR. In fact, MOST HDR displays I've seen so far are UHD (there are plenty of exceptions though).

For PC Monitors, look for "HDR10" support. Any monitor claiming HDR support but lacks HDR10 certification cannot even accept an HDR input and is purely fake HDR. Even monitors with HDR10 certification aren't anywhere near as good as an OLED HDR capable television, but they can at least accept the HDR signal and do something with it (not much in most cases).
## TN

* TN (Twisted Nematic) panels made by various companies are all simply called TN

## VA

* AU Optronics (AUO) makes Multi-Domain Vertical Alignment (MVA) panels. These include but are not limited to MVA, AMVA, AMVA+, etc.
* Samsung makes Patterned Vertical Alignment (PVA) panels. These include, but are not limited to, PVA, SPVA, CPVA, etc.

## IPS

* LG is the primary rights holder to In-Plane Switching (IPS)
* Samsung makes Plane to Line Switching (PLS), often branded as IPS (example, Dell U2717D)
* AU Optronics (AUO) makes Advanced Hyper Viewing Angle (AHVA) panels, which are an IPS clone despite ending in 'VA'. These are also often branded as IPS (see any 27" 1440p 144/165hz gaming 'IPS' display).

There are more manufacturers but those are the three key LCD panel types and their main derivatives/manufacturers.
I'd advise trolling the kid.

"We bought you this laptop with a GTX 9600! It's 9 times faster than a 1060!"

However, unlike my other idea, that won't meet the minimum specs for UE4 development.
All joking aside, it's better for your back and neck than racing/gaming chairs.

I'm hoping to pawn my Vertagear off on someone locally, and move to a proper office chair.
> hes definitely not a smart one

>  but he somehow bypassed that by changing his mac address or something like that

If he can figure out the latter, then you're not 100% correct about the former.

> Is there any 3rd part software or something that we can add to the laptop to stop him from gaming?

Not that I'm aware of, but even if there is, he'll eventually get around it if he has primary possession of the laptop.

You're going to have to baby him, get him a device with limited gaming potential (as suggested, or a cheaper, lower end Windows laptop), or deny him altogether and tell him to use his school's/library's machines.
> I've read on this subreddit that the minimum requirement for an 1440p 144hz monitor is the GTX 1070

Depends on what you're playing, and your expectations. I'm currently running a 3440x1440 display on a GTX 1060. It's fine, because I target ~60fps with G-Sync making that realistic. I haven't had to drop a game to medium yet.

> So the Monitor I picked has 165hz but the PC has a GTX 1080. Would you guys pick something else?

The beauty of a G-Sync display (which the PG279Q is) is that you can run a frame rate below refresh without stuttering or tearing. You don't need to lock at 144/165hz. A GTX 1080 is a fine GPU. You'll have the flexibility to max out graphics at playable frame rates, or lower your settings for extreme frame rates. Go nuts, and enjoy the flexibility.
'Unreal Engine' is incredibly vague. The engine is very flexible, allowing for results on mobile or high end desktop gaming.

However, the specs for DEVELOPMENT on UE are fairly light, thankfully.

https://wiki.unrealengine.com/Recommended_Hardware

* Win 7 or Mac OS 10.9.2 or later
* Quad Core Intel or AMD, 2.5ghz or faster
* GTX 470 or later, Radeon 6870 or later
* 8GB RAM

Taking those GPUs, even a GT 1030 or RX 550 will suffice, or even Intel's latest Iris graphics (which are noticeably better than Intel HD).

-----

If I were you, and could afford it, I'd get a Macbook Pro for him. It would meet or exceed all of the required specs for development, but would have a far more limited library of games that he could play.

"Well, since you said that you needed it for UE development, here's a kick-ass laptop that exceeds the UE4 development minimum specs, and should hopefully meet your goal of not being distracted by frivolous games while you work so hard."

It would be the most trollish way to give him what he says he wants, and what he actually needs. The downsides are - 1) Cost, and 2) If your brother is smart, he'd sell it, and use that money for a superior gaming laptop.
Same. On my older 60hz monitor, a game running below 60hz tears horrible for me. The higher the framerate goes, the less noticeable it becomes...for me.

Different strokes and all.
You violated rule #13. Clicking the link that they provide shows that your post falls under any (or a combination of) these:

> Please don't - Be (intentionally) rude at all.

> Please don't - Insult others.

Your comment was meant as a joke. But you were, based on context, making fun of drivers with religious bumper stickers. You knew what you were doing, so it was intentional. It is both rude and insulting.

There can be no doubt now that you know and fully understand why you were banned. Do you have to agree with it? Nope. To be honest, I don't agree either. I think it was a harmless joke, not aimed at any particular individual nor the poster that you replied to. But, the moderators are free to enforce their rules. You are free to comply, or not participate. That is your choice. Looks like the ban was temporary. That gives you time to decide if you want to participate in a community that strict.
> You explained your theory on how it will work

I explained how it does work. There's no indication that it will change. You are stating that it factually will change, because you want it to change.

> Hdmi 2.0 doesnt support 4k/60, thats true, theres not enough bandwidth,

Unless this is a typo, you're wrong. As I stated, HDMI 2.0 DOES support 4k/60. And that is why you cannot expect a higher refresh rate from the Xbox One S.

> but Freesync works over HDMI 2.0a.

I never disputed this.

> Of course though you would never try to pretend that isnt the case through omission. Just an accident?

I think you're trying to move the goalposts, because again, I never disputed this.

Because you are creating strawmen arguments, I'm going to bring this back on topic.

---------------

* The Xbox One S and Xbox One X are 60hz devices. This is indisputable fact. They report themselves as such to the display device, and they request this refresh rate.
* FreeSync requires maximum range to be at least 2x greater than minimum range for LFC to automatically enable (AMD advises 2.5x for best results). These are facts.
* Some high refresh displays support LFC despite a high minimum range, examples being 40-144hz, 55-144hz, etc. When capped to 60hz max, these devices are known and observed to lose LFC support.
* Because of this, running an Xbox on a FreeSync display capable of 40-144hz will limit it to 40-60hz, thus disabling LFC.
* As a result, my advice is to ensure that any FreeSync display used for an Xbox at this time support at least 30hz minimum to ensure LFC support.

Your stance has been that Microsoft WILL enable high-refresh support for the Xbox One devices to coincide with FreeSync support despite no confirmation, rumors, or even credible speculation, and that because you want this, I must be wrong.
> Whats ironic here is thats exactly what youre doing here, by declaring as facts, what you assume to be true because thats currently, as in right now, what the box supports, as opposed to the future, when its updated. Pretty funny.

Not at all. I was asked how it works, and I explained how it works. Going forward, it is more likely to continue working that way. It's a safe bet.

Also, the Xbox One S is limited to HDMI 2.0. That's 4k/60. It won't go above 60hz, no matter how much you REALLY REALLY REALLY want it to.

My advice remains unchanged. If you want to fully exploit Freesync tech on Xbox One, ensure that the selected monitor/TV supports a minimum variable refresh range of 30hz, so that you get the benefit of LFC. If you go with anything with a higher minimum, only 60fps titles will benefit, having their occasional dip addressed by FreeSync.

You can continue to downvote and pout over this, but you'll still be wrong.
I think your problem is that you want it to work a certain way, so you don't care how it really works if reality doesn't match your hopes.

Sorry you feel that way. But the Xbox One S (a device that I own) requests and forces 60hz on my AW3418DW, and my HDTV. My Shield ATV also requests 60hz, but can be adjusted for 24p mode (and some other refresh rates, I think).

Again, fact, not theory, no matter how inconvenient it is for you.
A monitor runs at the refresh rate requested by the device (so long as it's supported). When you plug an Xbox One S (and likely an X as well), it requests 60hz from the connected TV/monitor.

Not a theory. It's how it works.

EDIT: Poster above is outright lying. I was describing the way the current hardware works, and you can test this by plugging an Xbox One/S/X into a high-refresh monitor. Try setting it above 60hz. You can't. There's no theory here.

You can GUESS that this MAY change with future firmware, but I wasn't speculating on that. And the above poster is stating emphatically that it will happen despite no announcement/confirmation from Microsoft, no well grounded rumors, or no speculation from any industry insiders to support it.
I'm talking about mouse movement across the desktop with a static refresh rate.
Pops off easily by pressing the button below the insertion point. I used the stand for about 2 days before switching to an arm.
The issue that you are describing is an issue with this specific panel. The AW3418DW has it too.
This. They've already pledged 1440p support, and it already supports 4k. If it can run 4k, it can run WFHD and WQHD.
However, there's a loophole here. A monitor that has a range of 40-144hz will support LFC. However, the Xbox forces the display to run at 60hz, which will narrow the range to 40-60hz, disabling LFC.

It's not just FS 2 and LFC that Xbox owners need to be concerned about, it's a minimum 30hz supported to allow for LFC at the forced 60hz that the Xbox will run at.
> Tearing is barely noticable at 30fps

Tearing is noticeable at any framerate. Some notice it more or less than others. Higher refresh displays hide it better than 60hz displays. But it's still noticeable.

> Only a few FreeSync monitors actually support 30Hz.

A valid concern. And while LFC would address this, users need to understand that the Xbox will force a 60hz refresh rate. The first monitor on that list has a 40-144hz refresh rate. At 144hz it will support LFC and therefore, 30fps will be within range. But at a forced 60hz, it becomes 40-60hz and LFC is not supported.

> Regardless, there won't be any tearing in any games with vsync.

Correct, but I do recall there being some games that run with V-Sync off.
Though it was funny seeing her with blue skin if you played on a Voodoo 2.
> If I go with 4K then, what are the sizes I should look for? 24'' and 32''?

I kind of feel like you were against 4k, and that I was advising against 4k. So, to be fair, I don't think that my advice would help you. Hopefully, someone else can chime in to offer assistance that is better tailored to what you are looking for.
I suspect it.

Moving from 60hz to 100hz, I can feel the mouse being more responsive across the desktop. Moving from 100 to 120, I feel it even more.

What I think is happening is that, using high-refresh as my daily driver is new to me, so my mind is actually comparing 60/100 to 60/120, and I'm feeling the increased difference of the latter, and not comparing 100/120.

Does that make sense?
> A bit stuck on choosing the right monitor 

Aren't we all? :)

> 4K is basically pointless, 1440p is the way to go for better quality. I should pair that with a 27'' monitor for best results.

Yes. For people who prefer the height of a 27" monitor, you're limited to 27" 16:9 or 21:9 at 34". You've already ruled out the latter due to pillar-boxing content.

At 27", your options are 1080p, 1440p, and 4k. A 1080p is a fine budget choice, but is otherwise too low resolution for enjoyable viewing. 4K, by comparison, requires display scaling at that size for almost everyone, which defeats the purpose.

So yea, you're on the right track.

> So here's my question: What is my best option? I'd love a 1440p monitor but 27'' is a little too small, I'd prefer 30''+. How large can I go without running into the same problems?

Well, at 32", 4k becomes more feasible. A 32" 1440p has the same DPI as a 24" 1080p, so it won't be as sharp looking. Basically, you went from on the right track to slightly off the rails based on your prior criteria (unless I misunderstood you, which is certainly possible).

>  don't care too much about refresh rate since I don't game, at least not competitively or often, but the higher the better, I guess. What are your experiences? Any recommendations?

> I'd like to keep it under or around $500 for a single monitor, or $700 if I go with two.

At 32", I'm not well versed in the options. I don't like the size so I'm blind to what's out there, except for a few that interested me in the past (all with their own flaws).

At 27", your best bet for your stated needs would be the Dell U2717D/U2715H (~$399 apiece lately) for one. If you want two, the BenQ GW2765HT (~$330) will drop the USB hub, the factory calibration, and uses a slightly lower quality IPS-variant (AHVA) but you can get two in your price range.

That's where I'd start if you settle for 27" 1440p.
It's G-Sync, so the refresh rate adjusts with the frame. Your theory, while otherwise correct, won't apply in my case.

As for static refresh rate, I was talking about mouse movement across the desktop. And I've test for frame skipping, but it's stable at 120hz.
I can't explain why, but I noticed more of a jump from 100 to 120, than I did from 60 to 100. Statistically it doesn't make sense, so I'll chalk it up to placebo effect for now.
If your car uses USB 2.0 (most do, or older), the phone will use the cable as a USB 2.0 connection and it will work.

The problem occurs when using a USB 3.0/3.1 cable on a USB 3.0/3.1 port.
Nothing wrong with it, per se, but there's always going to be suggestions to suit the taste of each individual. Here's my suggestions, for what they're worth.

1.  See if you can get DDR-2666 (on the QVL for your mobo). Even if it's only $5-$10 more, it's worth it given Ryzen's love of faster memory.
2.  Swap the GPU to a Radeon RX 560. Performance will be in the same ballpark, but you'll see why when I get to the monitor recommendation...
3.  Monitor - Consider the Dell SE2717H. Cheapest I can find it now is $180, but I've seen it at $150. It's also 1080p/60hz, like yours. But, it's 27", IPS, and best of all FreeSync, which will give you smoother gaming performance with the recommended RX 560.

Those are my suggestions, but even if you stick with what you've got, you're in good shape for a solid budget gaming build.
According to the manual on my Dell U2717D, it supports a 24hz/24p mode over HDMI. If I get time this weekend, I'll pull it out of storage and test it with my Shield ATV to see if I can force 24p mode, and more importantly, to ensure that it actually works.
Thanks for the clarification, I'll see what I can dig up.
Just to be clear:

You want a 48 or 72hz display to ensure fluid playback of movies (24p content)...

But you want 1440p, a resolution that will lead to interpolation of both 4k and 1080p content?

Am I understanding you correctly?
"We slashed our engineering budget, so why do our phones suck?"

"We slashed out software dev budget, so why do fans hate our lack of software updates?"

"People loved the OG Moto X, X2, and Moto Maker. Let's make sure we don't ever do THAT again."

It's as if Motorola is being run by the idiots at Sega!
Have you verified 60hz when running 5k on dual DP? Some monitors like to default to 30hz for oddball resolutions, and it sounds like that may be what's happening here. You could be experience 5k/30 vs. 4k/60, which would make the desktop feel sluggish.
Depends.

Are you happy with 60+? If yes, then all is fine.

Do you desire a higher frame rate? If yes, lower setting or upgrade your GPU.
> So it's not an issue with my graphics card being weak when holding under the load of the game / not being able to handle the ingame gfx ?

Your GPU isn't weak. It just means that it takes your GPU's full power to run at the current frame rate. It would make no sense to run at lower power, unless you wanted a lower frame rate.

> Im running it with a i7 7700k and it's barely at 31%

Yup. It means your GPU is the limitation at maximum frame rate. This is a good thing. You have plenty of CPU headroom for future GPU upgrades down the road (meaning, you don't need to change out your CPU the next time you upgrade your GPU).
This is normal. If you are at or near 100% GPU utilization, it means that you are GPU limited. That means your maximum frame rate is dictated by your GPU, and lowering settings will give you a higher frame rate.

If GPU utilization is consistently below 100%, you are either V-Sync frame capped, or, you are CPU limited. That means you'd need a beefier CPU to catch up with your GPU for any games impacted like this.
This is normal. If you are at or near 100% GPU utilization, it means that you are GPU limited. That means your maximum frame rate is dictated by your GPU, and lowering settings will give you a higher frame rate.

If GPU utilization is consistently below 100%, you are either V-Sync frame capped, or, you are CPU limited. That means you'd need a beefier CPU to catch up with your GPU for any games impacted like this.
> that i just want more screen space for.

4k gives you more screen space if you don't scale it. But at 27", you'll likely scale it to make the UI and text readable, which defeats the purpose. So might as well get 1440p to avoid the scaling issues.

Unless you have eagle eyes, I would go 27" 1440p, or at least 32" for 4k.
I have a 7700k, stock, which won't be much faster than a 6700. I get ~200 H/s which, in the current market, is 0.0000275 BTC/day.
Quality office chairs go into the thousands. I work from home and I game. That's 10-12 hours a day in a seat (sometimes more).
FYI:

* QHD = 16:9 (2560x1440)
* WQHD = 21:9 (3440x1440)

And in your post, you're comparing 2 16:9 32" displays to a 21:9 34" display.

So I'm going to take a guess as what you want. You want a 32" IPS 1440p display, for ~$300, but you want Dell Ultrasharp picture quality.

Unfortunately, you're going to have to compromise. Either you're going to have to accept lower picture/build quality, or you're going to have to raise your budget. The Dell Ultrasharp monitors are TYPICALLY (not always) the best in terms of out of box picture quality for their price and performance segment. They haven't had much competition lately as others are abandoning the 1440p 16:9 performance segment. The LG 27MB85R was the last kick ass monitor, IMO, for that segment.
Answered your other post on this on this as you were typing this response, but I'll provide more detail here.

* i7-7700k stock (4.4ghz all cores)
* GTX 1060 6GB
* 16GB DDR4-3000 CL15

Our systems are in the same ballpark. My GPU is slightly faster, and your CPU is slightly faster but partially offset with slower RAM. Outside of benchmark graphs, you wouldn't be able to tell the difference in gameplay.

I run at 3440x1440. Yes, on a GTX 1060. CS:GO will run well on anything, and the few benchmarks that I could dig up on World of Tanks shows that it should run well at 1440p on that setup.

Your main problem will be running the newest AAA titles. You CAN run them, but if you aim for max settings, you'll be in the 30-40fps range. That will be relatively smooth thanks to G-Sync, but it depends on your tolerance. I prefer to not game under 45fps, usually targeting 60fps as my average. There are other who want to be above 100fps all the time.

For newer titles, you'll need to drop from Ultra to Very High or High to get 60+fps consistently. With modern games this is actually an easy trade off to make. Going High to Ultra usually gives very little visual quality for a massive performance hit.
> That's truly unfortunate. I always use GPU scaling in hopes of less input lag, assuming it will do a decent job.

It doesn't help there, at least not to a noticeable degree. Monitors do interpolation (minimal to no impact on input lag). TVs do upscaling (mucho input lag, game mode usually disables this to varying degrees). GPU scaling is a good option for TVs as it bypasses the television's additional input lag.

> Well. Turn it off then. I'm already used to 100% dpi scaling in windows on my 4k screen after all

> kidding. I never even noticed

The high DPI of 4k helps to largely offset interpolation. It's still there, just hard to notice. I cloned one of my 27" 4k monitors against a 27" 1080p and a 27" 1440p, and outside of small text and fine UI elements, they were indistinguishable. But 1080p on a 1440p display is noticeable.
Absolutely. I'm running a GTX 1060 (10-15% faster than the 970 most of the time). My monitor is a 3440x1440 Ultrawide. Counter-Strike runs well on just about anything. I can't vouch for World of Tanks as I don't play it, but the results at Userbenchmark (grain of salt, self reported numbers) seem to indicate 70-80fps, max settings, 1440p.

Also, you'd be using G-Sync, which would smooth it out due to variable refresh technology.
> Fabric

Nice, shame it doesn't come in white.

> I opted for a chair like this... even tho DX racers look infinitely better

I have a Vertagear. Killer looks, and killer damage to your back. I've learned my lesson and prefer a mesh task chair, even if they do cost more for quality chairs. The plan right now is to go for a white Eurotech iOO, unless I find a better option. I expect to pay around $700, give or take, for the chair, headrest, tax (9.5%!), and shipping.
Regarding the mesh material, is it fabric, or plastic?
Nope, unfortunately.

It theory it should work, since 1440p is just 4x 720p. However, that would require integer (nearest-neighbor) scaling, which zero monitors do. Peter from Nixeus has confirmed for us that all 3 suppliers of scalers for monitors do not support the feature, so no monitor can support it at this time.

There's also an ongoing thread at the GeFroce forums, started 3 years ago, asking NV for a driver-based solution. No response from NV yet.

What you're describing would be the holy grail of monitors. As 4k/8k ultimately become more affordable, you could have an 8k monitor that displays 4k, 1440p, 1080p, and 720p as native. Or, a budget 4k option showing 1080p/720p as native.

Unfortunately, we're not there yet :(
If you're looking for 27", 1440p, 144hz, and G-Sync, the Dell S2716DG is the absolute best budget option out there. And me calling it the 'budget' option isn't meant to be an insult, it's just literally the most affordable option.

You could step 'up' to an AHVA (IPS) display, but that's typically ~$250 more, and for what you gain in visual quality, you lose in quality control.

Basically, if you can afford to spend more AND are ok with playing the lottery, an AHVA option is for you. But if you wanted to keep it under $500, you made the best choice.
Running game at 1080p will lead to something called interpolation, which is the method used for scaling.

Let's look at it this way. Left to right there are 2,560 pixels in each row. At 1080p, there are 1,920.

The game will be drawing 1,920 colored dots left to right, but has to fit them into the 2,560 pixels of the actual display. How does it do this?

One way is to use 1:1, which means it colors in a center stretch of 1,920 dots, leaving black borders around the display (and your image is smaller since it doesn't fill the screen). This is sharper, but you lose screen real estate.

So typically, interpolation is used. It "stretches" those 1,920 dots to cover the actual 2,560 pixels of the monitor. This means mixing colors at each pixel to approximate what should be there. The end result is a blurry screen.

If you were to take a 27" 1080p display and place it next to the XB27HU, also running at 1080p, the native 1080p display would have a sharper picture.

That said, for MOST gamers this isn't a big deal. If you've ever used a console on a 1080p HDTV, you've seen worse interpolation (and at a lower frame rate too!). But if you're using to gaming at 1440p, then switching to 1080p on that same 1440p display would be a bit annoying.
> the GTX 1050 requires a 300 watt PSU

And to be clear, this is the conservative CYA estimate given by manufacturers just in case you bought a low quality PSU. You won't sniff 300W with this thing (unless paired with an AMD FX-9590).

For comparison, using a Kill-A-Watt P3 4400, my system draws ~210W from the wall during mining on CPU/GPU, and ~180W when gaming. That's an i7-7700k and a GTX 1060 6GB. At the wall is before PSU efficiency comes into play, so the actual power requirements of the system are lower.

A typical OEM 180-250W PSU will be more than enough for the GTX 1050.
Chair?

Looks like or similar to the Eurotech i00.
Do you have a Google Home in the same room?
The motion blur comparison was posted in my post, with direct links.
> This was one of the reasons why I returned the AUO panels. One side of the screen was clearly yellowish.

My Alienware AW3418DW has the color issue, but you have to:

* specifically look for it
* on a white backround only
* while swaying left and right

And even then, it's subtle. White is 6500k, so if we assume that's what the monitor is down the middle, the far edges might be 6300k and 6700k (warm and cool, respectively), IE, not VERY far off.

It's minor and overblown compared to the yellow/brown staining of AUO's AHVA panels (and their many other issues).

> I just noticed you have AW newest UW, damn, that must be one hell of a monitor. The 1080p version looks nice as well and is currently going for $760 at NewEgg but I read a review saying it had only 800:1 contrast.

All IPS (and PLS/AHVA) panels are ~1,000:1. Results will vary unit to unit. In the case of my AW3418DW, mine was ~850:1. TFTCentral's was 900:1. Lims Cave's was ~1,100:1. They vary. This is typical, so don't base your judgment on one review.
Right now the LG 34UC79 is your best bet. It's not quite the same lottery as AUO, but it's not as pristine as their 60hz options in overall quality. They also have a 1440p high refresh panel, but it's currently limited to G-Sync monitors, and it had issues (color temp varies across the screen, for example).

Basically, you can play the AUO lottery, or you can get a high quality 60hz no adaptive sync display, or you can compromise and get an LG gaming IPS that is an effective, if not perfect, compromise.
I closed my Origin account a few months back, which means losing access to my EA games. No loss, and won't be going back.
I keep boxes for storage, transport, and resale.

That said, you don't need the box for warranty if it's a Dell Premium Panel (AW3418DW qualifies). They ship your replacement first. When you get it, put the defective one in the box that they used for shipping.
It's now being reported that the roster bonus is due March 18th, and the Eagles will in fact be absorbing it.

Different outlets are reporting different numbers. Some are claiming $3M roster bonus, and others are reporting $4M. But all of them agree that the Eagles are on the hook and it comes off the Seahawks' cap.
* [$4M] (https://overthecap.com/player/michael-bennett/359)
* [$3M] (https://247sports.com/nfl/seattle-seahawks/Bolt/Michael-Bennett-trade-saves-22-million-off-cap-in-2018-115940757)
* [$4M] (http://www.spotrac.com/nfl/philadelphia-eagles/michael-bennett-7709/)
* [$3M] (https://www.seattletimes.com/sports/seahawks/seahawks-reportedly-want-to-trade-michael-bennett-to-make-team-a-little-quieter-michael-bennett-notices/)

Sportrac has $5M in the chart, but corrects to $4M in the notes, with the remaining $1M being per-game bonuses. If you have any other source of a $5M roster bonus, please let me know.

EDIT: Looks like instead of providing a source (you can't), you decided to downvote me.
Just wanted to chime in even though my specs aren't an exact match.

* GTX 1060 6GB
* 3440x1440 (1440p UW), 100hz, G-Sync

Prior to this I always ran at 60hz with V-Sync on because tearing drives me nuts. That means lowering settings to ensure never dropping below 60fps. That's not a concern now as anything down to 45-50fps is still smooth, and down to 40 is perfectly acceptable.

Also, performance has increased. Scenes in World of Warcraft that would drop below 60fps now maintain 65-75fps. The reason why is due to the way that V-Sync works. At 60hz, you have a 16.67ms interval between frames. If the first frame is ready in 16.2ms, it gets displayed. If the second frame takes 16.8ms, that's too long, so the prior frame gets displayed again, leading to a temporary 33.3ms interval and a 30fps reported average. Even with a rolling average, this hurts your reported FPS and results in a stutter.

Same scenario with G-Sync, first frame is 16.2ms, 2nd is 16.8ms. That's a rolling average of 60.6 (61) fps, whereas the same thing with V-Sync on would have temporarily reported as 30fps, or at last a dip into the 50s.

Basically, if you were targeting a 60fps minimum before just to satisfy V-Sync, you'll now be able to crank the settings to aim for a 60fps average rather than minimum.

I hope that made sense. My sleeping pill is kicking in.
It's recently released, but the NZXT H200i could stand some changes.

* Drop the integrated cam/fan controller and LED strip. Most users I think would avoid using these (I ripped them out to get the cables out of the way since space is at a premium in this), and it would lower costs.
* Front UCB-C and HDMI (for VR) ports, any new case should be including these today.
I know it's been a month, but you have the wrong panels listed for several displays:

* PG27UQ - This uses an AHVA (IPS like) panel. MVA is a vertical alignment panel.
* LG 34GK950G - It's just IPS. Nanocell is marketing (colored filters behind the IPS panel). If you're calling this Nano-IPS, then the PG27UQ above should be "Quantum Dot IPS" as they're calling it in their marketing materials.
* BFGD - AHVA panels
Miners learned from the last crash. You don't sell when the market is down. You accumulate and hold. Selling only locks in your loss.

They fully expect the market to recover. And while it's low, they're accumulating altcoins as much as they can.
Thank you for the link! And yes, it appears that you are correct.
> I don't believe that counts for compliance at the beginning of the league

It does. Once the player is cut/traded, the team chooses their designation and their cap it is altered.

> unless you are counting the top 51 rule 

Nope, as it's not relevant here.

> You have to hold that cap charge until June 2.

Source?
> okay and I believe Seattle will be covering his roster bonus of $4m.

NFL.com is reporting that Seattle is saving $5.2M by trading him. Over the cap has them saving only $2.2M if






































 they cut/trade him and absorbed the roster bonus.

That means they're not on the hook for the full roster bonus. We're taking at least part of it, but we'll know for sure when the terms are released.
If we designate Curry as a post June 1 cut, we save $9M on our cap this year.

Acquiring Bennett means that we absorb his 2018 base salary of $1.65M. So right there we're at a potential savings of $7.35M. However, Bennett is also due a roster bonus of $4M. If we're responsible for that, then our savings is cut to $3.35M.

Here's the thing. Seattle is already on the hook for Bennett's prorated signing bonus. It wouldn't make sense for them to trade him AFTER absorbing the roster bonus hit. Because at that point they only save $1.65M by cutting/trading a premier edge rusher. So until I hear confirmation otherwise, it's safer to assume that we pay the roster bonus and that we save $3.35M in this move.

His salaries for 2019 and 2020 are also smaller than Curry's, and he can be cut at anytime for no cap hit. This is a great move by Howie to reduce our cap hit while maintaining one of our best assets (the D-line as a whole). It also gives us greater flexibility going forward.
"We sold all 20 of them to one dedicated gamer who promised us that he would not use them for mining."
I have nothing constructive to add, just that you should scare the crap out of him.

"You've got nothing to worry about. All they do is a quick scan for kiddie porn. They don't care about anything else."

His reaction will determine if you should even be friends with him anymore.
All 1920x1080 IPS and TN panels are now 6-bit + FRC. Only VA panels in this range still use 8-bit subpixels.

Today's FRC implementations are outstanding, however, and the average consumer won't be able to tell the difference.

Of the ones that you cited, The Ultrasharp is probably your best bet as they generally come calibrated out of the box to a decent degree of accuracy. The LG may or may not be calibrated, but they don't have the same level of consistency out of the box as Dell does in this regard.
Over the cap.

Michael Bennet's prorated signing bonus is entirely on Seattle. This year he was due $1.65m base (this is now 100% on us), and $4m roster bonus (the timing of when that was due will determine which teams hold it on their caps).

For Vinny Curry, he counts $11m on our cap this year ($9m base and $2m prorated bonus). If he's a designed pre-June 1 cut or trade, he counts $$6m against our cap this year, with a cap relief of $5m.

If he's a designated post-June 1 cut, or he's cut/traded after June 1, he'll count $2m on our cap this year, giving us $9m in relief.

Those are all of the numbers direct from Over the Cap.
I'd like them to bring the Taichi theme back to the Taichi! (Take a look at their Z370 Taichi to see what I'm talking about)

MSI did similar with their Krait board.
> The IPS panel it uses is mediocre at best

It's not calibrated, which is unfortunate. But the panel itself is top notch.

You may be confusing it with the AHVA panels made by AUO, which have given IPS a pretty bad name.
Dangit.

I was joking. Was hoping for something aimed at a 5 year old, like, "Don't let the door hit you in the hiney!"

Then I could have asked for an EL1, getting mostly goo-goos and ga-gas, and maybe a messy diaper.

Oh well, dreams shattered.
Ok, now, ELI5?
Poor wording on my part. They can cut him today and designate him as a post June 1 cut (they can do this with up to 2 players). That would allow them to be in compliance by or before 3/14, while getting the post June 1 cap benefit.

Thanks for the correction!
> The whole selling point is the 200hz lol

No, the whole marketing point is to sell it to people who don't know better (just like 1ms monitors and other such shenanigans).

> Anyway how is that Alienware you have? Did you calibrate it or did you just open it out of the box and you're just like I love this.

* [Part 1: Initial Subjective Impressions] (https://www.reddit.com/r/Monitors/comments/812iuq/my_first_day_with_my_new_alienware_aw3418dw/)
* [Part 2: Calibration Analysis and Results] (https://www.reddit.com/r/Monitors/comments/82dcey/aw3418dw_calibration_measurements_results_and/)
* Part 3: Final Review (to come in about 2 weeks, give or take)

I'm still learning things about it, and I do want to try calibrating it at least one more time in a different manner to see if I can achieve different results. I'll post a final review in the coming weeks after I've played with it enough.
Depends on the panel. Those new Ultrawide 200hz VA panels should not be run over 120hz. At 100hz they are fine, some ghosting at 120, and a smearing mess at 200hz.
> I am currently using an AOC G2460PG (24" 1080p TN 144hz with g-sync).

> I have been considering upgrading to the Asus PG279Q and was wondering whether anyone else has done something similar to this or has any insight on how significant the difference between the two would be.

The PG279Q uses an AU Optronics-made AHVA panel advertised as IPS. It is similar to IPS in its underlying technology, but the execution is a little bit off. Whereas IPS/PLS have a blueish-white off-axis glow, the glow on AHVA is more pronounced, and yellowish bordering on a brown stain. They also appear to have a higher rate of dead/stuck pixels, both out of box and developing over time.

That said, they are still near identical to IPS, just more of a lottery.

> While I am not a crazily competitive gamer I was also curious to know whether people think the increase in response times between TN and IPS is noticeable.

There are two things here that competitive gamers confuse; input lag (what you feel) and response time (what you see). Let's compare two IPS/TN monitors that are otherwise identical; the PG279Q and the Dell S2717DG. I'll be using TFTCentral's numbers for this.

-----

## Response Time

In terms of response times, the Dell (TN) is advertised as being 1ms, while the Asus (AHVA-IPS) is advertised at 4ms. In order to prevent ghosting at 144hz, we need to keep all transitions below 6.94ms. Anything higher means trailing into the next frame (you'll still get general motion blur even without ghosting).

At its optimal overdrive setting, and at 144hz, the Dell S2716DG averages 2.8ms GtG, a max of 4.5ms (so all transitions below 6.94ms, no ghosting), and has 4% inverse ghosting. There are a pair of measured transitions that hit as high as 14.5ms, which means dark trailing across 3 frames.

By comparison, the Asus averages 5.2ms, with a max of 6.5ms, so no ghosting. It also has 0.4% overshoot, with no measured trails into the next frame.

Basically, the Asus (AHVA-IPS) will give you a more consistent, ghosting free image. But that 2.8ms vs. 5.2ms average does translate into some blurring, right? Here's the PixPerAn comparison courtesy of TFTCentral:

[Dell S2716DG] (http://www.tftcentral.co.uk/images/pixperan/dell_s2716dg.jpg) | [Asus PG279Q] (http://www.tftcentral.co.uk/images/pixperan/asus_rog_swift_pg279q.jpg)

Open them in two separate tabs, and compare them, coming to your own conclusions.

Bottom line is that, in terms of response time, AHVA-IPS gives a  more consistent look (no overshoot) at the expense of slightly more motion blur, and it's up to the individual to determine their tolerance.

-----

## Input Lag

http://www.tftcentral.co.uk/images/dell_s2716dg/input_lag.png

Basically, panels don't directly impact input lag contrary to popular myth. But TN requires more aggressive overdrive to compensate for its overshoot, and this introduces a small bit of input lag. The end result is that the PG279Q (0.75ms) beats out the S2716DG (2.7ms), but to be fair, these are both numbers so small as to be unnoticeable. At 300fps (CS:GO, for example), your frame interval is 3.33ms. Both are under that and therefore will be 0-1 frames apart depending on input lag from other sources (such as your mouse).

In this case, the AHVA-IPS is faster, but to a degree that few if any human beings would notice. In fact, most claim to notice that the TN is faster. We call this the placebo effect :)
If the trade is going after the point where the $4M roster bonus was due, we're off the hook.

If that's the case, I wonder why they'd trade him? They were already on the hook for his prorated bonus, and that doesn't change (if anything, it might accelerate, hitting them harder). And the cost to cut him next year (or prior to his roster bonus this year) is MUCH cheaper.

Basically, I'd be shocked if they traded him and are paying that roster bonus. It doesn't make sense to me. But then, I'm not an NFL GM and I'm likely just missing the other elements.
Reports were that they would ultimately just release him. From what I could gather from over the cap, Michael Bennet will carry a $1.65M base salary, and a $4m roster bonus. It is likely that we, not Seattle, are responsible for the roster bonus, giving us a $5.65M hit for this upcoming season on Bennet.

Curry has a cap hit this season of $11M. However, we don't save all of that if we cut/trade him. If the cut/trade is pre-June 1, we only save $5M, which doesn't fully cover Bennet. So cutting Curry prior to June 1 and getting Bennet costs us more money. Not a good deal on its own.

But as a post June 1 cut, we save $9M (same with a post June 1 trade).

Bottom line, expect Curry to remain on the roster now, unless we get an amazing deal, and then be designated as a post June-1 cut (EDIT: To be clear, they can cut him today and designate him as a post June 1 cut. They can do this with a max of 2 players). By effectively swapping Curry for Bennet, we'd be saving $3.35M for this season.
> And does the color shift impact significantly the color quality for say, a 27" sitting 2 or 3 feet away? I don't use color professionally, just like punchy colors.

There are a couple of factors at play here.

Can the pixel show the same colors (bit-depth)? If we're talking TN, VA, or IPS, all 8-bit, then they are all capable of showing the same exact colors (subject to below...)

Next is the color gamut. Unless you have a specialized use case, you want 100% sRGB or as close as you can get to it. No, numbers greater than 100% are not "moar betterer!!!111" Ideally, you get 100% coverage. Coming up short can lead to under saturation. Going above can lead to over saturation. Today's panels of all types (outside of the budget space) are hitting at or close to 100% sRGB, so again, all 3 panel types can, in theory, hit the same colors.

Also, we need to address those colored filters (Quantum Dot for Samsung, Nanocell for LG). They do extend the color gamut supported. If you're running properly calibrated sRGB, you get no benefit from it (unless it pulled a monitor from 96% coverage to 100% coverage). So this technology is great for marketing, but hasn't yet made a real impact in PC gaming.

That leads us to our next culprit, and your primary concern, viewing angles. It doesn't matter if a pixel is displaying a 100% accurate depiction of the color it's supposed to display if that pixel is at a large angle relative to your eyes. This is where off angle color/gamma shift rears its head. IPS and its derivatives handle this better. VA panels do well from top to bottom but do tend to shift at the lateral edges. TN panels shift slightly worse than VA laterally, but shift horribly from top to bottom.

It's the viewing angles that have the largest impact on perceived color accuracy. And you can't correct/calibrate for it.
Glad to be of help and if you have any further questions, don't hesitate to ask :)
> The year before was more frustrating. We could have beaten Tampa Bay, and we would have rolled over Oakland just like they did.

That was 2 years before. The year before our Pats loss, we lost to Carolina in the NFCCG, who then lost to the Pats.
> Everytime occasionally when I would Boot It Up it would not have the ICC profile loaded or it wouldn't show up an exclusive or full-screen mode. Then the GPU and make me edit the digital Vibrance in Nvidia settings.

I use DisplayCal to calibrate my monitors. It manages the ICC profile instead of Windows, so it's forced to load at boot. So thankfully, I don't have the problem that you experienced.

I also got lucky with my AW3418DW. Gamma was 2.42 after OSD adjustments, which is perfectly acceptable. The end result is 2.22 gamma in desktop use and most games, and 2.42 in the games that bypass the ICC profile. I'm good with this.

> Also I'm really a big fan of the ambient Lighting on the back to make IPS glow less, eye strain less and colors/contrast apprear better.

I use a Luminoodle bias lighting strip around the rear perimeter of my monitor. This is 6500k white and brighter than the rear accent lighting that LG uses. If I had LG's lighting, I'd turn it off and still use separate bias lighting, but that's just me.

I agree with you though, it makes a huge difference.

> The LG is going to come factory calibrated which is cool.

This right here is the clincher for most. However, not all factory calibrations are created equal. Dell does a great job hitting 6500k/2.2 in white point and gamma. Lg and Samsung? Not so much...

My C34F791 (Samsung) had a 1.8 gamma out of box in its "calibrated" state. That was horrible. TFTCentral's most recent LG reviews, the 34UC79G and 38UC99, were good but not great. The 34UC79G had a 2.2 gamma and 6092k white point, so a little warm. The 38UC99 fared better, being 2.2 and 6235k (ever so slightly warm). The 34UC79G also had a DE average and max, in its calibrated state, on par with the DE on my AW3418DW out of the box.

Basically, I'm not impressed with LG's factory calibration so I don't rely on it.

> I'm very interested in the Nano IPS technology along with you know the factory calibrated panel

Nano IPS is marketing. It's colored filters between the panel and lighting, same as Samsung's Quantum Dot. It cheaply expands color gamut (RGB LED is still superior, which is why professional displays use it). Your games are mastered in the sRGB color space, and the base panel used in these monitors already supports ~99% coverage.

With Nanocell, a proper calibration for sRGB means that you don't get the benefit of it. An improper calibration means an over saturated look (incorrect, though some may subjectively prefer this). Unless you have any professional use cases that specifically call for wide gamut support, you as a gamer will gain no benefit from this tech.

> The only major, I see right now is Dell RMA is fucking amazing. If you need a new monitor they got me a new Dell s2716dgr and 18 hours including phone call. That is fucking QUALITY CUSTOMER SERVICE. I am horrified of doing an RMA with Asus Acer LG because I don't think they have next day as an option.

Dell - They ship you a replacement, typically 1-2 day shipping, then you ship your defective unit back in their box with their prepaid label. You incur no cost, and reduced downtime.

Everyone else - You pay to ship your monitor back (guess how much it costs to ship a 34" ultrawide). You the wait for them to receive and inspect it, then decide on repair or replacement. Downtime can be 2-6 weeks depending on shipping methods used and if they decide to repair. Also, it's not free.

I'm not saying that the Alienware is the best option. Due to the lack of gamma adjustment in the OSD, it's a gamma lottery. I got an acceptable one. TFTCentral got a shitty one. But neither Acer nor LG offer anything with theirs that addresses a need of mine with this monitor. As you've stated though, they do seem to somewhat address your concerns. I just wanted to temper your expectations with regard to those, specifically, factory calibration and Nano IPS.
The U2715H uses an LG IPS panel, while the U2717D uses a Samsung PLS panel. The PLS panel has less off-axis glow, on average, but slightly more overshoot ghosting. In both cases, the difference is marginal, and mostly not detectable without specialized equipment.

The other main differences are appearance, USB port placement, and that the U2717D has 1 more frame of input lag at 60hz (as a casual gamer, you won't notice this).

I'd say go for whichever is cheaper. If it's a tie in price, decide if off-axis glow or input lag is your bigger concern and get the one that excels in that area (again, marginally).
The problem with your recommendation (the 144-165hz option) is:

* doesn't fit his pricing
* isn't great for his desired usage

The Dell U2715H/U2717D use higher quality panels, higher build quality, and comes factory calibrated. They are ideal for video editors who only casually game (what OP is doing). I'm coming from a U2717D myself (now using the G-Sync capable AW3418DW). Dell's Ultrasharps are outstanding displays for the OP's stated usage.

The PG279Q is for a competitive gamer who doesn't need the best picture quality.
The best bets at 1440p for office, semi-professional, and casual gaming are the Dell U2715H or the U2717D. Take a look at those. If either meets your pricing requirements, then you've found a winner. If they're too expensive, let me know and we can discuss cheaper alternatives.
Gamma will vary unit to unit. You could get a 1.8, a 2.6, a 2.2, etc.

There was never a gamma issue, just your typical per-unit variance combined with Dell's poor decision to not allow gamma adjustment. My monitor model has the same issue.
> I thought I'd ask: how good are the AudioEngine A2+ speakers?

I'm not an audiophile, but as I understand it, you can get similar or better sound for a much better price. You're paying for the size and aesthetics with these.

That said, for the size and appearance, you can't find better sounding speakers for the price.
The problem with 4k at 27" is that UI elements are just too tiny. You'll want to scale to close to 1440p anyway, giving you the same effective work space, sharper text, but scaling issues (blurry applications in some cases). It also requires more performance from your GPU in gaming.

Either is fine, so long as you know what you're getting into.
These models are uncalibrated, so the accuracy (including gamma) out of the box will vary from unit to unit. The only problem is that Dell still isn't including a gamma setting to correct if you get a bad one.

If they are otherwise the best monitors for your needs, play the lottery. Buy one and exchange until you get one you're happy with.
> I know dell ultrasharps were the go to models in the past but I feel they are a bit outdated now or above my budget...

The U2715H/U2717D are 1440p, 60hz, excellent build quality, and factory calibrated. They can usually be found for $399 in the US.

LG's 4k options, like the ones you're considering, are pretty much the best 4k alternatives.

It then comes down to 1440p vs 4k. Which would YOU rather use at 27"?
They're using Cryorig air coolers as their baseline offering via their BLD service. Also, the Cryorig H7 Lumi uses Cam for its lighting. They have a partnership.
100% means no TBW (terabytes written). The minute you write any significant data to it, it's going to report 99%. That first Windows install and game installs will do it.
>  Probably looking to run three 1080s at some point soon.

Because you don't need 1300W for 3 1080s. The FE peaks at well under 200W, and you should be lower power limit or undervolting.
A 34" 2560x1080 is literally a 27" 1920x1080, with more pixels on each side. Their DPI/PPI could be off by as much as 2 in either direction, as a 27" isn't exactly 27, and a 34" isn't exactly 34 (The AW3418DW and Predator X34P are 34.14", for example). But it's going to be very close to exact.
It's great, and I like it. It's a rip off for the MSRP, but for the price you paid is good. But compared to my still functional Moto 360 v1, it:

* lacks HR sensor
* looks like a cheap plastic watch (it's not plastic, but the matte finish makes it look plastic)
* has noticeably worse battery life

In addition to that, the lack of NFC is disappointing. It's basically the little brother to Fossil's 3rd gen, with a cheaper appearance and smaller battery.
I can't comment directly, but I will point out that shortly after our PM conversation, [THIS] (https://www.huffingtonpost.com/entry/gay-catholic-priests-vatican_us_5a9d79b2e4b0a0ba4ad6a1d4) happened.
I'm sure that'll buff right out.
They sold out last week. It's gone. Amazon discounted it to $599 right after (not sure if currently valid).
I love you!

My wife is scowling, however. I just bought the G810 and G703 less than a year ago.

EDIT: A white G613 would be a great replacement for this setup - https://imgur.com/LwkEdKO (yes, it's mine)
> Any opinions on curved vs flat?

I currently have a monitor with a 1900R curve (the lower the number, the more aggressive the curve, a 3800R is a flatter curve).

I appreciate the curve in gaming, but it's noticeably distorted for desktop use. For my needs, I think I'd prefer a flat display if I wasn't a gamer, 3800R for mixed use, and 1900R for exclusively gaming. I believe that the 38UC88/98 both have a 3800R curvature.
> As for the LG, it's good to know they're essentially the same model with minor variations, I was driving myself crazy trying to figure out what the differences were.

I don't know much about the UC80. The UM88/UC88 are the same thing, but the UM is flat, and the UC curved.

The UC98 is 75hz while the others are 60hz. If you plan to use FreeSync, based on my experience I prefer the 40-60hz of the cheaper models, than the 55-75hz of the UC98.
I'm in the Seattle area.

A Spyder 5 Express (~$120) and DisplayCal (free) can give you a good baseline calibration. Your Ultrasharp displays are already pretty damn good out of the box (when set to their pre-calibrated profile).
Alienware AW3418DW
There's definitely a difference, though I'm fine leaving mine at 100hz.
Overall I like it, but it's been less than a week. For my full opinions, check my submission history. I just posted a thread last night on the out of box measurements and calibration results.
Wait till you see the completed redesign :)

I'm almost done.
Not all jobs pay minimum wage. My wife and I do very well for ourselves.
So my options are:

* Buy the Powerplay, which messes with my current custom mousepad aesthetic
* deal with it having a cable going across the desk, or,
* drill a hole in the desk to hide the cable

My other alternative is to...

* keep the Logitech G703 and charge 1-2x per week

I'll keep the G703 as is unless a white G603 comes out.
[Logitech G703] (https://www.logitechg.com/en-us/product/g703-wireless-gaming-mouse) :)

Select Color button is below the price.
There are definitely times where I'd prefer it to be flat, or at least have a less aggressive curve.

Please show me a 3440x1440 IPS monitor with G-Sync that isn't curved, and I'll return this one in a heartbeat :)

Disclaimer: Must also be of similar quality.
Still a fair recommendation for some, definitely leave it up :)
There's more (hidden around the cropped photo). I'll post the full setup later this year after I finish swapping out a couple more things.
[Alienware AW3418DW] (https://www.dell.com/en-us/shop/alienware-34-curved-gaming-monitor-aw3418dw/apd/210-amsv/monitors-monitor-accessories)
> I don’t even worry about frame rate unless I’m down below 40.

Agreed. I target ~60 for typical gaming, with drips in the 40-45 range being perfectly acceptable.

> I could have sworn that panel was VA. Must be thinking of the asus and acer versions.

This particular panel is relatively new. It's used by this and the new (just released) Acer X34P, as well as the upcoming LG 34GK950G.
> so only 100hz.

Same here. I don't run at the OC 120hz setting.
> Isn't that monitor VA?

Alienware AW3418DW has an LG-sourced IPS panel.

> Low FPS doesn’t matter that much when you’ve got G-Sync anyways.

Agreed. That was largely my point. A lot of people think that a 1060 is a bad choice for this monitor, but it works just fine for my needs. With G-Sync (or FreeSync), just because a monitor can his 120hz doesn't mean you need to sustain 120fps.
> Anne Pro

I appreciate that. What I'm looking for right now is:

* Fullsize keyboard w/num keys
* wireless (not Bluetooth)
* white
* Scissor switch preferred, Omron/Romer-G as backup

I tried the MS modern Keyboard w/Fingerprint ID. It's grey, wireless, and scissor switch, but the BT was laggy (overly aggressive sleep on it as well, as it would fall asleep during games). Can't access system bios with BT keyboards either (plugging them in for this is a minor inconvenience at worst).

The Logitech G613, if offered in white, would check all my blocks.
> PQ on this monitor was garbage when compared to a VA(yes, I had them side by side).

Your subjective opinion, or did you take measurements with a tool?

Based on the impressions I'm seeing from other users, there is a lot of variance in the out of box color (as there will be on any uncalibrated display). Mine measured ~7k white point, 2.5 gamma, 850:1 contrast, and ~2% DE (this is off memory). Mine was acceptable, but a little dark.
> Driver-based GPU scaling can get around this issue.

It doesn't give you 2560x1440 as a selectable full-screen resolution. You have to set a custom res either in the NVCP, or via CRU (AMD's drivers, at least back when I tried it, didn't allow for custom res).

The model that I had was the UC98. Using CRU to set 2560x1440 locked you to 60hz, with a FreeSync range of 55-60hz.
Adds a cable to the desktop and clashes with my setup. Thank you for the suggestion though.
It looks nice to me, understated. But it LOOKS like plastic in person. The matte finish seems to do that.
My first recommendations are going to be the Dell U3415H (old model) and U3417W (newer model). They're listed as $799 and $879, respectively, on Dell's website right now but are continually offered for much less. They are very similar with the key takeaways being:

* 34"
* 21:9
* 3440x1440
* IPS
* Factory calibrated to a high level of accuracy (DE can be close to 2%, however)
* No FreeSync, unfortunately

For your stated needs, these displays get as good as it gets. However, there are some alternative options within your budget.

* [LG 34UC80-B] (https://www.amazon.com/LG-34UC80-B-34-Inch-21-UltraWide/dp/B074JKT894/ref=cm_cr_arp_d_product_top?ie=UTF8)
* [LG 34UM88-P] (https://www.amazon.com/LG-34UM88-P-34-Inch-UltraWide-Thunderbolt/dp/B01BV1Q0L4/ref=cm_cr_arp_d_product_top?ie=UTF8)
* [LG 34UC88-B] (http://www.microcenter.com/product/460907/34UC88-B_34_Curved_QHD_IPS_219_LED_Monitor)
* [LG 34UC98-W] (https://www.amazon.com/dp/B019O78DPS?tag=price475-20&ascsubtag=297428671&m=ATVPDKIKX0DER)

These are all essentially variations of the same display with different features, curve, connectivity, and in some cases, refresh rate (the last model, at least, supports 75hz). They are all 3440x1440 IPS displays, and factory calibrated by LG (gamma tends to be slightly off from perfect with them, and slightly over saturated). They support FreeSync, typically with a narrow range.

The biggest flaw, for me, with the LG models wast the complete lack of support of 2560x1440. If you have 16:9 content that won't fit in a window, you're using 1080p full screen. This doesn't impact most people, just those who like to game with exclusive fullscreen mode with legacy non-21:9 games.
I don't regret buying mine, but I still wear my Moto 360 v1 more often. The LG Watch Style is the poster child for the worst of AW 2.0, which is lacking features. It's the basis of the modern Fossil watches, but the LG Style looks like a cheap plastic toy by comparison.
https://imgur.com/gallery/kplph

Scroll to the bottom. I uploaded it with the old setup.
If you check his post history, he screencapped me and ratted me out to his incel community (incels = involuntarily celibate, no woman will touch them, and they self branded themselves under this label).

It's cringe worthy.
In this case, it's Ryan Reynolds.

https://getyarn.io/yarn-clip/2721cbe7-6c40-4577-8435-e9c846c8ac8c
> IMO if your finances are fine, food and mortgage covered etc and its not impeding any future considerations youve discussed and you catch shit for buying something nice for yourself then youre with the wrong person.

I feel like you've posted already (and removed it). But just in case, here's my stance.

The post was more tongue in cheek. I didn't need her approval. My wife and I both work. We don't make the same, but because of our kids (and how when one isn't working, that person takes care of the kids), we understand that we each enable the other to work and so both paychecks belong to both of us. There have been times where I made more than her, and times where she has made more than me. It's cyclical, but we've always supported each other.

We have a hybrid approach to joint/separate finances. We pool our money, pay all bills, then count the remainder every month (next month's bills are prepaid at this point). If we're short, we pull from short term savings. If we're ahead, we decide how much goes into savings, and how much is play money. We then split the latter 50/50. She can buy what she wants with her money, and I do the same with mine. Last month we came out $6k ahead (tax return and a few other things), so we threw $4k into savings, and ~$1k apiece for play money (I already had enough for the monitor, so I'm already planning my next big waste of money).

The only considerations she had input on with the monitor purchase was on what to do with my old one. I wanted to sell it, but she asked if I would store it for when her ~9 year old Apple Cinema Display dies. I agreed.
Inked gaming.

I uploaded the wallpaper and ordered their custom panoramic.
> if you need to explain a meme it ain't a meme

The "meme" was made by me, using Paint. I'm not worried about it spreading or being copied. Anything that I do with graphic work is just going to be inherently low quality. I was more concerned with spreading info on a good, underappreciated movie :)

> nice setup bro, thanks for sharing ! I welcome the absence of freaking RBG lighting.

Thanks for the feedback :)

I plan to have the full redesign posted at /r/battlestations later this year after a couple more changes. The only light that I use is white LED (preferably 6500k) for the sole purpose of illumination. I personally detest RGB and I think that in most cases, it cheapens the feel. There are exceptions, of course.
I prefer my desktop to be as clean as possible (wireless keyboard is next, if one I like is released). That mouspad would mess with the custom one I've got, and add another cable to the desktop.
> inb4 the standard "You shouldn't have to explain anything. Do what you want with your money!"

Too late, scroll to the very bottom to the most downvoted post :)
That monitor launched with an MSRP of $1,499.99, since reduced to $1,299.99 officially. Dell usually sells it on their website for $1,149.99, and the common sale price every other month is $999.99, which is what I paid ($1,192 after tax).

There are only two monitors with these specs. The other one, the Acer X34P, just launched at a suggested MSRP of $1,099.99, and was discounted last week for $100 off to match the Alienware.

The Alienware has arguably better build quality, and a measurably better warranty (advanced replacement, free shipping both ways). The Acer has speakers built in, support for 10-bit color through FRC, is thinner, and weighs less.
Yup, but I already had the 703 by the time that the 603 was unveiled. Additionally, going for a white build.

No complains on the 703 (even though the first died and needed an RMA). Just lazy and prefer the battery implementation on the 603.
Those are nice looking, thank you.

But after going from MX Browns to Romer G, I think that I prefer the latter (Omron switches when not branded on Logitech's board).

If Logitech ever releases a G613 in white, I'm sold. Otherwise, I'll just keep what I've got for now.
> Do you have the link to the wallpaper?

https://imgur.com/gallery/kplph


Scroll down, it's at the bottom.

> What mouse pad is that?

Custom made.

https://www.inkedgaming.com/

Uploaded that same wallpaper, used it for the 36x11 (panoramic desk pad).
Different people will give you a different answer for that. If you're looking for quality sound and a lot of bass, then yes, but you'd probably want larger speakers anyway. If you're going for minimalism, then smaller speakers like this are ideal, and you don't have room for a sub.

These speakers are for those who are going for minimalism and want good sound for size. They do not have good sound for the cost.


If you want more bass (but worse overall sound) with a similar appearance, look for the Swans M10. They look simlar-ish and have a mid-woofer.
> If you play competitive fps

I don't. I play single-player games. Mostly. I've played HoTs, mostly with/against bots.

> then you really wanna deal with AA batteries?

If they last numerous months instead of several days? Sure, I'm ok with that.


As for VA vs. IPS (and TN as well), a lot of people try to rank these as 1-2-3 (in their preferred order). But it's really more of a rock-paper-scissors comparison as each one has a strength that counters the others' weakness, and vice versa.

The problem with the current crop of high refresh VA ultrawides and even the upcoming 200hz ones is that going above 100-120hz on these leads to excessive ghosting/smearing. That means they're really no better than the current IPS champs (AW3418DW, X34P) that are 100hz native, 120hz after OC.

At that point, it comes down do contrast vs. viewing angles, since overall response time/ghosting is similar (slight edge to IPS on dark transitions) at that refresh rate. If you want more accurate (mostly due to viewing angles) color and uniformity, go IPS. If you want better contrast (and detail in dark scenes), go VA.

I prefer IPS so I went with the AW3418DW. But there's really no wrong answer so long as you get the right match for YOUR tastes. The rest of us can only tell you what works for our individual tastes.
I'm upgrading piecemeal. Before this was an i7-7700k in December. Yea, it's unbalanced.

That said, I'm not a high refresh gamer. I target the best visual quality possible at or near 60fps. And the 1060 does this in spades.

Forza Horizon 3 runs max settings, 2sMSAA, no FXAA, no motion blur, at 55-65fps on this thing. Tomb Raider 2013 dips to 45-50fps with everything cranked (FXAA used), but dropping down to High pegs it around 70fps (TressFX murders the GPU at this res). World of Warcraft goes from 55-100fps at 100hz and V-Sync on, depending on what I'm doing. Wife and I are in Pandaria right now, so 65-75fps most of the time.

It's fine for my current needs, but I do plan on an x70 at least, maybe an x80 even when the next gen comes out. I'm not a high-end high-refresh gamer. I just wanted 1440p, true IPS, variable refresh, all in one high-quality display.
Yup, and there are various ways to address that.

First is to use V-Sync. This introduces a small amount of lag, and I'm used to it and don't care.

Second is to cap below your refresh by about 4, as multiple tests have shown this to be a great way to minimize V-Sync related lag, while staying within range.

As for me, I target ~60fps, so I set the visual quality as high as I can so long as I don't go meaningfully below that number. I don't target 100+fps.

Basically, I didn't buy this monitor for the refresh rate. I bought it for every other reason. YMMV
I'd kill for a white G603 due to the battery life (and I don't mind a heavier mouse) :)

And a white G613 to pair it with!
Good automoderator :)

TIL
> Shaming language

Just to be clear, when someone says that to **YOU**, it's 'shaming.' But when you tell a **WOMAN** to 'shut-up,' that's perfectly acceptable. Oh you poor sensitive snow flake. Again, I see why you're having problems with women.

> ow I understand why you need to give an excuse for using your money

It's not my money. It's OUR money. We both work. We are a team when it comes to financial decisions.

However, the post you're responding to is a joke post. It didn't take much to get her blessing. We allot ourselves a certain amount of play money each. I was within that budget. I didn't need her consent.

> thanks for the screen capture

EDIT:Check the last post submission in his profile.

Some real cringe material, you've been warned.
No, but I am mostly joking. Wife and I are a team, and she approved of the purchase (personal approval, she honestly didn't care what I spent the money on). I told her what I was looking for in a monitor awhile back. When this monitor came out, I said, "I expect it to hit $999 after a few months." It has hit that prices numerous times, and the last round she said go get it. And here it is.

But my wife is somewhat tech savy. Maybe it would work if she weren't.
Alienware AW3418DW, mounted to an Ergotech Freedom Arm.
Here's the original album (prior to current upgrades):

https://imgur.com/gallery/kplph

Scroll down to the wallpaper, it's a full size upload. Right click and save.
> Its not as if we are having financial problems, but since I have just recently (November-December) used almost 2.500€ for upgrading the PC it becomes hard to explain why you just have to pay 1.000€ more for yet another tech gimmic when we already have a good looking monitor.

I think what worked for me was the fact that I outlined for my wife (who is somewhat tech savy) exactly what I wanted in a monitor up front. She and I both knew that it didn't exist and that every monitor available would be some form of compromise. So, after a lot of comparison shopping I settled on a $440 monitor (Dell U2717D) that met many of my criteria (1440p, 27", IPS-type panel, great build quality) but lacked support for FreeSync/G-Sync. The gamer variants were using AHVA lottery panels and had poor build quality outside of that, making them a no-go for me.

When the AW3418DW came out I showed it to my wife, and we waited for reviews and a sale. She knew in advance that I was going to buy it at $999, so when it hit that price, she gave me the all clear.

If you're constantly feeling the need to hide the justification for a purchase, there's a good chance that you can't justify it. Trust me on this one, if your current significant other is a keeper, don't hide this from her. Talk it out with her and then go from there.
> Eh tops out about 33% in my experience.

Most tests I've seen would disagree, but that's ok. The next point will allow me to better explain it.

> The reason the 7700k performs like the i5 8400 is because clock speed.

Here is the main issue. The 7700k does not perform like the i5-8400. It performs like the i5-8600k.

https://www.youtube.com/watch?v=uwUEVEbZxI4

I've done the numbers for this before, but basically of the games tested, the 7700k pulls ahead in one, the 8600k pulls ahead clearly in one, and the rest are competitive. Except for games where all CPUs tested are on the same level, the 8400 matches the 7700k in Witcher 3, and falls behind in everything else.
Agreed. Intel's HT is about 50% efficient, meaning that each core is equal to about half a real core. So in theory, that means that the 4-core i7s with HT can perform like a real 6-core.

In theory. In reality, that's only for 8 or more simultaneous threads. If you're using more than 4 threads, but fewer than 8, you're only getting ~50% benefit from those threads.

* If a game uses 5 threads, the 8400 performs like 5-core, and the 7700k performs like a 4.5 core
* For 6 threads, it's 6 vs 5
* For 7, it's 6 vs. 5.5
* and then at 8, they equal out again at 6

I apologize if this doesn't make sense, as I'm not the best at explaining things. But basically, for 4 threads or fewer, the higher clock speed of the 7700k puts it ahead of the 8400. At 8+ threads, the 7700k is again generally ahead due to similar core count (when accounting for HT efficiency) and higher clock speed. But at 5-7 threads (where a lot of modern games reside), the real cores of the 8400 closes the gap over the HT of the 7700k.
Absolutely. If on Windows 10, right-click the Recycle bin on your desktop and select "pin to start menu." Click start and verify it's there as a tile.

NOTE: You'll have to open it to empty it, you can't just right-click and empty like you could on your desktop.

Once the tile is there, do the following:

* Click Start
* Settings
* Personalization
* Themes
* Desktop Icon Settings (it's on the right side now)
* Check what you want on the desktop, and uncheck what you want hidden
> Im on this boat currently.

I'm sorry to hear that, but be careful. If you feel the need to hide money/expenses from your significant other, you could be running into other problems.

In my case I'm joking about my situation. I didn't have to explain it, and she fully supported my purchase.
I prefer a clean desktop so it's removed. It's one of the tiles in the Start Menu.
> I need that mouse.

Logitech G703

>  And those speakers.

AudioEngine A2+ on Kanto S2 stands

> I’ll upgrade the monitor to that one while I’m at it too

Alienware AW3418DW on Ergotech Freedom Arm (FDM-PC-S01).


Just in case you needed a shopping list :)
> but what I also know is that it legitimately does look better than a normal IPS.

Except...it is a normal IPS. It is the same panel, sourced by LG, just at a smaller size and higher resolution to improve perceived clarity.

>  I had a MacBook Pro with RD and it is easily the best looking computer monitor I have ever seen

Likely because it was higher density than competing models (higher resolution at that size), and calibrated to some degree (most laptops aren't). A 4k 13-15" display is always going to look sharper than a 1440p or lower 27" display.

> You’re saying Dell makes a high pixel density IPS monitor as well?

Dell has traditionally always sold an Ultrasharp monitor using the exact same panel found in the competing Apple Cinema/Thunderbolt display of the same generation. This isn't a new thing. The problem is that Apple always went glossy and Dell typically went matte. I prefer glossy as it gives a better picture than matte, provided you can control the lighting in that room to reduce glare/reflection.

However, 5k monitors aren't as popular, so Dell isn't doing as much with those as Apple/LG today. Dell is doubling down on 1440p, 2160p, and Ultrawide 1440p, as well as HDR tech, whereas Apple is currently avoiding those in favor of 5k.
Security updates are USUALLY not subject to he OTA review process for carrier.

Source: I wrote that guide :p

That said, OTA links being posted is typically a precursor to the broader rollout (not always). They usually go up within 24 hours of a rollout (sometimes the day after, sometimes the day before).

If it hasn't happened already, I'd expect the rollout to begin tomorrow based on all of this.
IIRC, you can star the bug, then when you get your first spam from it, scroll to the bottom of that email and unsub from that discussion (let's you keep it starred, but not get spammed from it).
> I mean the Retina Display is amazing, I wish I could have just a 1440p 144hz Retina Display (not a full iMac).

Because retina is a marketing term for high pixel density, you won't see a 1440p "retina" display (unless it was really small, like 9.5"). Aside from being marketed for pixel density, the term "retina display" has no functional meaning in terms of monitor tech.

>  Best colors you’ll ever see outside of OLED.

This is also due to marketing. They use the same IPS panels as competing monitors, and can be calibrated to the same degree.

My wife's Apple Cinema Display had subpar calibration out of the box. I just re-calibrated it yesterday. Out of the box it was:

* 7000k white point (too blueish, not bad)
* 1.8-1.95 gamma curve (too bright, blown out, this was the worst part)
* DE average of ~3%, pretty good

I got it down to ~6590k, 2.21 gamma, and 0.24% DE average, and I'm happy with that. But since it's an ICC profile, you can forget about gaming in exclusive full screen mode, and it messes up Google Chrome (have to disable HW acceleration in Chrome).

I much prefer Dell Ultrasharps for this purpose as they use the same or similar panels, and they come properly calibrated from the factory, so you don't have to use workarounds. While we have a U2717D boxed up that would be a suitable replacement/upgrade, she craves glossy, so ACD it is until it dies.
The back at that point becomes a triangle. And Dell recesses their VESA screws to allow for their proprietary stands to just snap in. The end result was that, yes, the corners stuck out a little too much.

Using the thin spacers wasn't enough. The thick spacers worked, but the short screws didn't fit them, and the long screws were too long, making the use of stacked spacers a necessity.
Turns out that I have the standard Freedom Arm, not the HD. It works, but due to the rear design, I had to use all the included spacers and the long screws.
Let me give an example. SSDs are rated in TBW (Terabytes written). That's how much (minimum) they should be able to write before failure.

My current Samsung 960 EVO 1TB is about 3 months old. The software for it currently reports 1.6TB written, or 0.4% of the rated 400TBW for the warranty. That includes a fresh OS install and numerous game/benchmark installs. At 1.6TB over 3 months, it will take me ~750 months (or ~62.5 years) to hit that number.

And again, this number is front loaded due to a ton of installs and shifting things around the first few weeks.
Just saw that you do a fair bit of posting in MGTOW, so I guess I understand why you'd advocate being verbally abusive towards women.

Let me make this clear to you. You're not alone due to YOUR choice. You're alone because most women won't tolerate that sort of abuse.
You left out...

LG Watch Style if you hate yourself
> If you don't mind answering

I don't mind at all.

While the OG and the new both use PWM dimming (backlight flickers rapidly instead of actually dimming through DC control, and this is known to cause headaches and other issues), the new one flickers at such a high rate that it is near indistinguishable from DC dimming.

The same panel in this monitor is used in all the current gen monitors, and even though none of these gaming displays come pre-calibrated, improvements in manufacturing have led to better color accuracy out of the box, meaning it should look just as good (or bad) as an XG2401/2402 out of the box, subject to the usual per-unit variance.

But it still lacks G-Sync/FreeSync, which is a major deal for many. However, I believe it still includes Nvidia 3D Vision, for those who use this feature (it's a small but devout userbase).
I'm explaining to my wife, not my mom. It's OUR money.

Though the funny thing is, we also spent $1,000 on HER monitor back around 2009 (Apple Cinema Display). She's still using it today.
Sounds fair. And if you're buying new, the latest revisions are nothing like their OG counterparts.
It's been my excuse for EVERY upgrade :)
Official stand.

https://www.microsoft.com/en-us/store/d/xbox-design-lab-controller-stand/8t07q5srtv11/CT4J

Color options are below the listed price. Everyone else in the house has a Design Lab controller for their system (I went with the standard Xbox One S white edition) and a color matched stand.
I've been following the history of this panel through TFTCentral's LG contacts. Basically:

* 2017 = this panel (3440x1440, 100hz native)
* 2018 = upgraded version (144hz native)
* 2019 = up-upgraded (144hz native, HDR support)

No products have been announced with the 2018 revision yet, as even the upcoming LG 34GK950G appears to be using the 2017 revision, with Nanocell (Quantum Dot) applied.

So, if you're willing to wait until 2H 2019, you could get an HDR version of this, though more than likely, those first versions won't be FALD anyway (based on current trends).
> I hope this helps:

It doesn't, because I already explained it.

The overdrive is adaptive. That means that the response time changes based on the currently displayed framerate/refresh rate.

You set your display to 120hz, but you have G-Sync enabled. That means your refresh rate is equal to the actual frame rate. If your game drops to 100fps, you are at 100hz. You will have the same response time as if you had set it to 100hz.

I'm not knocking 120hz, mind you. I'm just trying to clarify that the monitor adjusts the response time to suit the currently displayed refresh rate, not the maximum refresh rate.

This is also why so many FreeSync monitors have reported ghosting issues when compared to near-equivalent G-Sync monitors, because they cannot adjust their response time on the fly (the Nixeus EDG 27 is the only known FreeSync monitor to support this feature).
I think the reviewers know this. The 2200G/2400G from AMD got WAY more attention than they would normally have gotten in a typical GPU market. But the lack of interest from gamers in GPUs due to current market conditions is likely driving a decline in views/clicks for these video reviewers, so they'll take what they can get.
1. No monitor has a 1ms GtG average under any setting. It's a false, misleading marketing spec. "Well, we can get one oddball transition to hit 1ms, but it's going to overshoot!"
2. The above also applies to the XB240H that you currently have.
3. The Asus VG248QE is the 2nd worse known 144hz monitor. Only the Acer GN246HL is worse (of known, tested monitors). The VG248QE has gone through several revisions and the latest revision is nowhere near as bad as a day 1 edition.

Considering the above, if you're buying used, your XB240H is likely superior to the Asus. If you're buying new, can you afford any of the more modern, superior alternatives (and your XB240H is still likely superior)?

The Asus VG248QE is like the CM Hyper 212 Evo - great for it's time/price, but trading on their prior reputation in the face of newer, better competition.
I'm still waiting on a white keyboard that I'd like. I had the Ducky Shine 5 (no white edition), and while the Shine 6 was offered in white, it had this weird "like a rifle" design. We'll see what they do with the Shine 7.

The keyboard in the pic is a Logitech G810. I haven't seen white Omron/Romer-G keycaps yet. But the switch is becoming more common outside just Logitech, so we'll see what the custom community does. And FWIW, I actually prefer these switches over Cherry MX (a heathen, I know!).
1.  Yes, that's my current setup, as of 10 minutes ago. (AW3418DW, Ergotech Freedom Arm)
2.  It's based off my prior build (redesign in progress) posted here - https://www.reddit.com/r/battlestations/comments/6sr9bg/finally_completed_my_attempt_at_a_whiteblack_build/
3.  The meme is from the movie "Just Friends" where he rents a Porsche to impress the girl, and when she's taken aback, he tries to play it off.
1.  Yes, that's my current setup, as of 10 minutes ago.
2.  It's based off my prior build (redesign in progress) posted here - https://www.reddit.com/r/pcmasterrace/comments/6unfh4/my_its_not_much_but_its_mine_post_a_whiteblack/
3.  The meme is from the movie "Just Friends" where he rents a Porsche to impress the girl, and when she's taken aback, he tries to play it off.
> someone have a real clue (i have ideas, but i'm not specialist) of why they are still not manufacturing?

The AHVA panels used in these are made by AU Optronics. They had (and still have) issues with their prior generation AHVA panels when compared to actual IPS or PLS. It seems that they may be having even more issues trying to get 4k and high refresh out of the same panel technology. That's my best guess considering that both known monitors using this panel have been delayed simultaneously with no known hard release date.

> Please A M D, seriously, just throw a new GPU out of nowhere that directly compete with 1080ti...

The sad thing is that if AMD were prepping a GPU to compete with the 1080ti today, they'd already be behind Nvidia's soon-to-launch new generation. The sadder thing? They may not even have a GPU waiting in the wings that can compete with the 1080ti.

All indications are that 7nm Vega will launch in late 2018/early 2019 and for professional use only. That means Vega/Polaris are the main products they'll offer throughout 2018 to compete with Nvidia. We've already seen what these products can (and can't) do. That means that 2018 belongs to Nvidia (unless mining continues to be popular, in which case both brands will continue to sell out).

As for 2019/2020, AMD is readying Navi. But the latest rumors stress that, as a GCN product, it's having the same issues scaling up as Vega and Polaris did. That means you can expect a large, hot, power consuming chip that is pushed as far as it can to hit a certain performance target [(163W RX 480 and 198W RX 580 vs. 116W GTX 1060, 292W Vega 64 vs. 166W 1080, etc.).] (https://www.techpowerup.com/reviews/AMD/Radeon_RX_Vega_64/29.html)

And if Navi is a 2019/2020 product, that means their first post-GCN release (if rumors of them moving from GCN are true) would be at least 2021/2022.

That means that just maybe /u/Laimis666 was correct about AMD needing 5 years to catch up to Nvidia, and perhaps he shouldn't have been downvoted.
Digital Foundry and other tests show that the 7700k (stock) is roughly on par or marginally slower than the 8600k (stock).

Extrapolating this downward, the i7-7700 (and the marginally slower i7-6700 from the prior gen) should be in the same ballpark as the i5-8400. I wouldn't say that one is definitively faster than the other, as some here are assuming, but I would be confident that they are in the same ballpark.

But I do agree with the majority. If you want to save money, get a 6700/6700k/7700/7700k and retain your existing motherboard. This will be cheaper than going Z370 + 8400, and as I've explained above, should offer you ballpark similar performance.
To you.

The beauty of PC gaming is that everyone gets to create their own experience. I've made an experience that works for me.
Newegg is generous about returns. Just go find your order under their account, select return, and select one of the available defect reasons. They'll provide a return label, and once received, they'll issue the refund (or send out a new unit if you chose an exchange instead).

Don't mention the dead pixel. Just be a general defect that closely approximates the issue.
Then I feel as if my speech is limited. Maybe they can give me unlimited speech?
Check his post history. This is all that he does.

EDIT: It was /u/maxfurry

That kind of disrespect deserves to be called out. Some posts were removed by moderators, and some were then deleted by him.

He won't see this though, as he supposedly has me blocked :)
Then get it on GOG?
At the time that I made that post, I was using the Dell U2717D. Both that and its predecessor, the U2715H, are excellent options for a user who wants 2560x1440, IPS (U2717D uses Samsung PLS, similar quality), and excellent picture quality due to outstanding factory calibration. They're great for casual gamers who prefer visuals over gaming features.

As for me, I just recently upgraded to the Alienware (Dell) AW3418DW, which gives me that 1440p IPS with high quality AND gaming features, offering most of both worlds (it's not factory calibrated). But this monitor is way outside the budget of what I was originally discussing.
> Did you just correct me by telling me i was right?

Nope, I pointed out how you were wrong.

>  I can't believe your reading skills are so poor, truly baffling.

Uh-oh, I'm sensing a triggering...

> Don't bother replying i'm blocking you, don't want to have to read or educate other peoples kids.

And there it is! And being wrong about something on the internet upset you so much, that you:

1. Went against your own definition of what upvote/downvote is for and downvoted me because you disagreed with me.
2. Demanded the last word ("don't bother replying")
3. Threatened to block me (we both know you didn't, and you were already going to reply until this point made you possibly reconsider to save face)

Oh, you poor sensitive snowflake...If you ever experience the real world outside of mom's basement, you are in for a world of hurt.

-----

Decided to take a look at your post history. This is apparently a theme for you. All of your most recent posts are insults aimed at people, and/or threats to block them. You are a toxic person who can dish it out but cannot handle when it's thrown back at you.
Vegans would be wise to support this. Because it's tech based, higher volume usually leads to lower cost per unit. Lower cost will make it preferable to meat from slain animals.

Basically, quicker adoption of this would lead to fewer animals being butchered. There are many other benefits, but this is one vegan specific example.
In Washington state, 3rd party items on Amazon are taxed as of 1/1/18.
Same with the Asus VG248QE. Something was good for its time and no longer a good option today, but it continues to sell off its prior reputation.

A general rule of thumb - if you're seeing a review for a PC part you're considering, and the review is several years old, then chances are it's been superseded by better products.
Hey, I'm glad that you like this thread so much, that you keep contributing to keeping it at or near the top of the first page. And, since I support your stance so much, I'm replying in solidarity with you.

To the top of the front page! (now, make another reply so we can boost t further!)

EDIT: As of this post, it is the top non-sticky post in the subreddit, just as you apparently wanted. Good job!
> they are for one thing only, to agree that a post promotes discussion or does not

Nope, at least not according to Reddiquette.

https://www.reddit.com/wiki/reddiquette

> **Vote.** If you think something contributes to conversation, upvote it. If you think it does not contribute to the subreddit it is posted in or is off-topic in a particular community, downvote it.

If there is already discussion on a topic on the front page, and someone else makes a new post outside of that thread on the same topic, one could say that downvoting it makes sense, since it doesn't contribute to the coversation (it's basically side talking).

>  I'm counting on you.

And that's what I'm here for, to correct your misunderstanding. Glad I could help, and you are most certainly welcome.
The best ones right now are the LG 34UC79G (FreeSync) and 34UC89G (G-Sync).
The beauty of Reddit is the voting/comment system. This allows the community to police the content.

Don't want this on a front page? Down vote and don't comment. By commenting, you influenced the part of the algorithm related to popularity, thus moving this topic further up the list.

Basically, you're complaining about the actions that you yourself are taking.

Some members found this post useful. If you didn't, down vote and move on.
3440x1440 60hz will run you ~$600 on a good day. Adding in high refresh costs more with or without G-Sync.

The 100hz IPS panel from LG just came out late last year and so far only two monitors use it, and both cost $999 when on sale.

You're going to have to lower your expectations, or raise your budget. For $600, you can get 1440p 60hz, or 1080p 144hz in ultrawide configurations.
Correct. It's why many warranties have that "void where prohibited by law" item at the end. As a kid I was like, "why would a warranty be illegal?" Now I know, it's the individual clauses that are voided by local laws.
I can't confirm since I don't own the X34P, but I can say that now that I'm looking for it, that shadowing really isn't there on my AW. It's either non-existent or insanely subtle. Might be subject to panel variance.
Yup, we don't have those European protection laws. Retailers here have return/exchange policies ranging from 14-90 days. Outside of that, you're at the mercy of the manufacturer's warranty. This is why EVGA is so popular in the USA. Relative to other companies, EVGA treats their customers like royalty (and as always, there are exceptions).
I would probably use off or adaptive. It's unlikely you'll be maintaining 144+ fps in every game, so V-Sync on will lead to stuttering.

Don't disable in your global settings though, as that can mess with your desktop, videos, etc. Change it on a per-game basis.
You REALLY have to look for it, and it's so slight, it borders on placebo.

I have it as well, A04 revision, January manufacture date.
> BTW EVGA's Warranty has a bunch of bull crap clauses.

To be fair, every company does this. The idea is to prevent consumers from exercising their rights.
Good to hear. I hope your case isn't isolated, and that they've begun to improve.
> I only trust their motherboards now.

Until you have to RMA one, then you'll swear off the entire brand.
> So I am looking at possibly getting one of these guys. However, it’s an AMD Freesync monitor and I have a GTX 1080.

It will run like a 144hz monitor with no adaptive sync tech, but will have the benefit of FreeSync should you ever switch to an AMD GPU.

> Would it still run almost games at max graphic settings and good FPS? Or should I go with a more affordable Dell Gsync monitor ?

The Nixeus will, in most cases, have better picture quality. G-Sync will help you if you'd rather crank the settings for better graphics at the expense of lower framerates. But given that there aren't major differences between High and Ultra in many games these days, the AHVA (IPS-like) panel of the Nixeus will make a bigger difference.

Case in point, I can run Tomb Raider (2013) on my monitor (3440x1440) with a GTX 1060 6GB at maxed out settings at ~45fps, or at High at ~80fps. Yet visually, they don't look much different.

If this is a tough call, save up another $200-$300 and get the best of both world, a G-Sync equivalent to the Nixeus model.
It's typical panel variance. If you got 100 of each monitor, you'd see a similar curve, ranging from 800:1 to 1,200:1 (with few extreme outliers beyond those limits).
Points in favor the the Alienware:

* Dell build quality
* Dell warranty

Points in favor of the Acer:

* 10-bit (8-bit + FRC), which leads to a slight reduction in banding (not noticeable to most users)
* Built-in speakers (for those who would benefit from it)

If none of those pros/cons matters to you, go off aesthetics.
In her defense, I got the impression she was trying to be funny on that one. Almost every guest on a comedy show does make the attempt, to varying degrees of success.

That said, as dumb as she is I may be giving her way too much credit.
http://www.chieftec.eu/en/marketing/archive/ctg-550-80p-detail.html

That 85% logo is meant to look like the official 80+ certification logo. If they couldn't be arsed to pass it, then their claim specs are dubious.

On paper, it's a solid PSU. But reality doesn't always match the paper. If they're lying up front, I wouldn't trust them with the rest.
I'm really sorry to hear that. Like anyone, they make duds occasionally. But objectively, they are among the higher quality options in this price segment.

That said, the S2716DG's build quality is typical for gaming monitors in this segment, IE, not great.
It should still work, but I wouldn't trust it. Anyone that fakes the 80+ certification doesn't get my money. I'd toss it and get a reputable PSU.

The EVGA B3-450 is an excellent budget PSU that often goes on sale for  a low price.

https://www.newegg.com/Product/Product.aspx?Item=N82E16817438124&ignorebbr=1
For 27", 1440p, 144hz, 'IPS' (AHVA panel), the best one near that price is the [Nixeus EDG 27] (https://www.amazon.com/Nixeus-27-Inch-FreeSync-Certified-Range/dp/B071G6PGP7/ref=sr_1_2?s=electronics&ie=UTF8&qid=1520018526&sr=1-2&keywords=nixeus+edg+27), currently $420 on Amazon. It routinely hits $400, so shop around.

If you value G-Sync due to using an Nvidia GPU, you can get the same specs (TN panel instead) from the Dell S2716DG/DGR, which routinely hits $399 at various outlets.

Be patient and you can score either at $399.
https://www.aim.org/aim-report/aim-report-a-black-eye-for-consumer-reports/

Don't trust CR for cars, or anything really. They've been caught sensationalizing when they knew it would drive subscription revenue.
I stopped trusting CR when I read this:

https://www.aim.org/aim-report/aim-report-a-black-eye-for-consumer-reports/

> Drawing on the testimony of a former CU employee and video-tapes made by CU of its tests of the Samurai, Suzuki showed how a “hit piece” was developed by CU editorial director Irwin Landau, who was both author and editor of the 1988 article. It charged that he set out to show that the Samurai was prone to roll over easily, endangering the lives of the occupants.

> CU videotapes and other evidence obtained in discovery show-ed that **when the car was put through the standard “emergency avoidance” test course, it passed with no problems.** This is supposed to replicate an emergency that any driver might en-counter. It involves the driver suddenly turning into the opposing lane to avoid an obstacle and quickly turning back into the original lane. The Samurai was run through the course, which had been used for 15 years, 37 times. It did not roll over once even though it was driven at a higher speed than other cars that were tested. Kevin Sheehan, a CU test driver, said the vehicle “never felt like it would tip over.” Another driver, Rick Small, gave the Samurai CU’s highest stability rating, saying it “corrects quickly” and “responds well.”

> Suzuki charges that **at this point Landau threatened his testing staff, saying, “If you can’t find someone to roll this car, I will!”** David Pittle, CU’s technical director, tried nine times to roll the car over without success. He finally succeeded in getting it to tip up by turning the car so sharply that it went off the test course. A video shows onlookers cheering and yelling, “Yeah!” One said , “I think I got that, I think I got that.”

> CU then decided to construct a new course that would make it easier to replicate Pittle’s successful maneuver. It was still difficult to make the vehicle tip, and CU employee Joe Nappi cried out, “All right, Ricky baby,” when test driver Rick Small finally succeeded in getting two wheels off the ground.

> **It was after all this special effort that CU announced at a news conference that the Samurai “rolls over easily” during routine driving** and was given the rating of “Not Acceptable” on the road. CU showed videotape of one of the few successful tip-up maneuvers. Suzuki charges that this was taped during another trip to the track, where the driver once again had difficulty getting two of the vehicle’s wheels off the ground. One tape shows Pittle complaining, “Can’t you just see it, we get no lift off the ground. Oh God.” After repeated attempts, however, the maneuver succeeded and the “picture-perfect” tip-up was obtained. Photos of this rare event were used repeatedly over the years to boost subscriptions and generate contributions to CU by showing how they exposed a safety hazard. From 1988 to 1996, Suzuki says, “CU sent out millions of fund-raising and subscription solicitation letters featuring the Samurai story.”

There's a lot more, but I don't want to copy/paste the entire article. It's a good read.
That's not how video works.

You can't see the benefits of a monitor, on a twice-compressed Youtube video, via your display.

It's like those television ads. "Your TV can't do this! (proceeds to display image on your television.)"
I'm going to do some more reading on this.

That screenshot shows it off, but it doesn't show that MFAA was the culprit (I'll give you the benefit of the doubt and assume you toggled MFAA in global, which caused that item to turn off).

EDIT:

https://pcgamingwiki.com/wiki/Glossary:Anti-aliasing_(AA)

Possibly also disable D3D11 Driver Command Lists, killing multi-threaded rendering (and thus performance when CPU-limited)
Could you please provide a citation for that? I'd really like to read up on it.

------

EDIT: Provided by above poster in another response.

https://cdn.discordapp.com/attachments/247515315825672203/419151367056261140/unknown.png

Thank you!
> The implementation of MFAA does not "improve MSAA's performance".

Ok, so it doesn't improve MSAA...

> I don't think it could produce higher than MSAA quality (but it may seem that way, because the improved performance lets you do say 4x, MFAA when you can only manage 2x MSAA).

But it does improve MSAA...

>  It improves normal, non-multisampled to MSAA-like quality.

It improves non-MSAA?

----

Let me be clear, what you posted is very incorrect, or, you wording is very poor (something I'm often guilty of).

MFAA only kicks in when MSAA is used, so by definition, it's an attempt to improve upon MSAA. It attempts to alternate the MSAA pattern every frame, supposedly. This gives you the perception of 2x the stated MSAA, so 2xMSAA looks like 4xMSAA, and 4xMSAA looks like 8xMSAA, etc. But, you retain the performance hit of the actual lower MSAA method.

The downside is that if you have a low framerate, this could lead to perceived visual anomalies.

CC for /u/temp0557


-----

EDIT:

https://pcgamingwiki.com/wiki/Glossary:Anti-aliasing_(AA)#Multi-Frame_Anti-Aliasing_.28MFAA.29

* Nvidia GeForce GTX 900 series and higher
* Meant to be used in conjunction with MSAA for lowered performance hit
*  According to Nvidia it reduces performance cost while used with high resolutions and is more flexible to needs of different game engines due to its programmability.
* One note of importance is that MFAA doesn't function properly below 40FPS. Below that threshold, MFAA causes smearing and blurring in motion.
* Possibly also disable D3D11 Driver Command Lists, killing multi-threaded rendering (and thus performance when CPU-limited)
Citation?

This would be good reading.
So...what's the question? Sorry, I'm only seeing a statement.
> 45-55 fps is choppy even on 60hz. I've played the witcher 3 and kingdom come on 60 fps @144hz and it feels a lot smoother than my 60hz monitor.

Of course, because you're dealing with stuttering (V-Sync on), or tearing plus jutter (V-Sync off). But previously playing ~45fps on a FreeSync display felt very smooth, for the time. Now, ~45fps on a G-Sync display doesn't feel as good, coming down from 100fps.

Bitcoin is becoming more dominant relative to other coins. As of this post it's 40.7% of the crypto market cap, per coinmarketcap. At peak profitability it was ~30% or just below. What this means is that altcoins are worth less in BTC value. So, even if you are mining as many altcoins, you get less BTC for you work.

On top of that, difficulty is up across the board due to more and more hardware entering the mining pools. This means the same number of coins are generated (pie remains the same size), but those coins are spread out among more hardware (everyone gets a smaller piece).

When you add those two together, profitability tanks.
It looks like that sticky was replaced, but yes, he could have searched.
You'd be surprised how little power those parts draw. I have an i7-7700k (more power than his) and a GTX 1060 SSC (less power than his), and total power draw at the wall is ~210W measured during gaming, or ~185W for the system after accounting for PSU inefficiency. His system will be comfortably under 300W. A reliable 450W PSU is fine for his needs, and all of EVGA's 450W models are reliable (if not world beaters).
> I've heard that going cheap on the PSU could fry the entire computer and I'd like to avoid that.

The problem is quality, not wattage. You'd be surprised how little power most components draw. I have a GTX 1060 SSC and an i7-7700k. Most PSU calculators recommend 550+. My Kill-A-Watt P3-4400 shows ~210W at the wall while gaming. When you account for PSU efficiency, that's ~185W system draw.

Which EVGA 450W model is it? The 450 B3 has a 5-year warranty and 100,000 hours MTBF (11.4 years) at 40°C. But their cheaper 450W PSUs have a 3-year warranty, 80k-100k MTBF @ 30°C.

I'd personally get the 450 B3 if you can swing it. It's a great value PSU that will serve your build well.
> How important is G-Sync? Would a pleb like me notice much difference?

If you've ever gone back and forth betwenen V-Sync on (input lag, stuttering) or off (tearing), or lowering your setting to make one tolerable, then G-Sync is for you. It syncs the monitor's refresh rate to the GPU's frame output (within a range) so that you no no tearing or stuttering, and you also get reduced input lag. It's a great overall solution. It's really allowed me to get more out of my GTX 1060.
> I prefer 21:9. Would stretch to around $800.

This removes the WQHD (1440p) ultrawides with G-Sync, which are typically around $1,000. You can occasionally find some of the prior models around the $850 mark.

>  I would like a balance between both.

>  Playing FPS so response time is important.

Response time is ghosting. Input lag is the delay you feel. People often confuse these largely due to the BS marketing done by monitor manufacturers. You can ignore this. All of today's gaming monitors have low input lag, and response times are adequate to keep ghosting at a minimum (though VA panels can have some bad dark transitions and inverse ghosting).

How much is the [LG 34UC89G] (http://www.lg.com/us/monitors/lg-34UC89G-B-ultrawide-monitor) in your area? MSRP was recently dropped to $899, and a few have had it on sale for $599 (current price on Amazon). It should meet your criteria.

* 34" 21:9
* 2560x1080p (WFHD)
* 144hz, OC up to 166hz
* G-Sync
* IPS panel, factory calibrated

You get a calibrated IPS panel (visual quality), as well as 144hz and G-Sync (performance). There's the balance you're looking for within your budget. This is likely the best you can do unless you add at least another $200 to your budget.
Generally, no. The jump from 60-144 is more than double. The jump from 144-240 is less than double, and since it's higher up the chain it's also subject to diminishing returns.

When you add in diminishing returns and a smaller relative gain, you're not going to feel it as much. But there is a perceptible difference.
> Ideally would like a 27inch-34 inch curved UHD (don’t think i’d need 4K)

UHD is 4k. TO be clear:

* 720p = HD
* 1080p = FHD (WFHD for ultrawide)
* 1440p = QHD (WQHD for ultrawide)
* 2160p = UHD (marketed as 4k)

> As my current screen is 1080p does it mean I couldn’t get a uhd screen?


Nope, no such limitation. Your main problem will be performance. And your mobile GTX 1070 is faster than my desktop 1060, so you'll be fine as a light gamer (I'm in the same boat).

If you could, please provide the following info:

* preferred aspect ratio (16:9, or 21:9 ultrawide - you an see a comparison [HERE] (http://www.displaywars.com/27-inch-16x9-vs-34-inch-21x9))
* Preferred budget (how much you WANT to spend)
* Stretch budget (how much you're WILLING to spend if push comes to shove)
* If you had to choose between visual quality (better graphics), and responsiveness/fluidity (higher frame rates), which would you pick?
* Name the top 3-5 games you anticipate playing
* Lastly, check your laptop's documentation. Some video-out ports are routed through the iGPU, and some the GTX 1070. You need to ensure that the DP or MiniDP is routed through the 1070, or you're not going to be able to use G-Sync via an external monitor (not sure if this is an issue with the Alienware 17, but it is with many other laptops). If you can confirm support for G-Sync via external model, that widens the options.
> also less light bleed

Off-axis panel glow (AKA, IPS glow) is worse on IPS than other panels. But backlight bleed is a physical defect and is independent of the panel. It's typically the result of poor build materials, poor build quality, physical damage, or a combination.

> without going with b grade panel like crossover, viotek or microboard.

While the Yamakasi Catleap and several other imports of the day were famous for taking LG's excess panels that were rejected by Apple for their Cinema/Thunderbolt Display, it is believed these days that there are no B-grade panels in the imports. It was only Apple rejecting panels, and other manufacturers weren't as stringent as they were (outside of your Dell Ultrasharps). In fact, it's believed by some that the Apple rejects were still "A-" (IE, still A-grade, but just not up to Apple's additional standards).

To be clear, it hasn't been confirmed either way regarding panel grade on import/budget monitors. The B-grade assumption is something that most of us are walking back on, pending actual confirmation.
It either works, or it doesn't (if every one hit 120hz, they'd sell it at 120, not 100). And you'll want to properly test for [frame skipping] (https://www.testufo.com/frameskipping), because it can "work" at 120 and you'd never no unless frame skipping was severe.

That said, electronics have variances from unit to unit, and if it works, then it's within spec for that unit. There's no long term harm from running it at that refresh rate.
The 200hz displays will be using VA panels. Better contrast, less off-axis glow, but worst ghosting. Most reviews have shown that going above 100-120hz (depending on your tolerance) is a no-go on these panels.

That puts them on par with these current IPS panels (AW3418DW, X34P), and it comes down to your preference of VA + HDR or IPS.

I still think that they will be excellent choices, but anyone planning to run them at 200hz may end up disappointed.
Points in favor the the Alienware:

* Dell build quality
* Dell warranty
* You can generally find it slightly cheaper ($999 vs. $1,099)

Points in favor of the Acer:

* 10-bit (8-bit + FRC), which leads to a slight reduction in banding (not noticeable to most users)
* Built-in speakers (for those who would benefit from it)

If none of those pros/cons matters to you, go off aesthetics.
I've always been a 60hz gamer. I got so used to it that 30fps was difficult for me outside of turn-based RPGs. I just got the AW3418DW, and after 24 hours at 100hz, even 45-55fps seems choppy at times.

Ignorance truly is bliss, and because I don't want to chase the highest end GPU every generation, I'm actually considering going back to 60hz before it's too late.
> I've always felt that the LG panels look more natural and Samsung colours have more artificial but more vibrant colours.

It comes down to the calibration, and both are calibrated very accurately. Per TFTCentral's measurements, the U2717D is slightly more accurate, but that's based on machine measurements. A human won't be able to tell the difference in general use.
A 1080p monitor will generally look better when using the PS4 than a  higher resolution monitor due to interpolation (TVs upscale, monitors interpolate). But the PC would benefit from a 1440p display.

Decide which one you game on more. If the PS4, go 1080p. If the PC, go 1440p.
Unfortunately, that display uses an AHVA panel by AU Optronics (advertised as IPS). While similar technology as IPS, it's known for a higher defect rate than traditional IPS or PLS panels.

If you're a gamer, the higher refresh rate (and FreeSync for AMD users) is a nice perk. For a gamer though, a pre-calibrated IPS/PLS display from Dell will typically offer better build quality and picture quality at a similar or lower price.
U2715H has an LG IPS panel. The newer U2717D has a Samsung PLS panel. U2715H has 1-frame less input lag (no big deal, even for casual gamers), and the U2717D typically has slightly less glow. U2715H has 3-side slim bezel and 4 USB ports in the rear, while the U2717D has 4-side slim bezel, 2 USB ports in the rear, and 2 USB ports on the left side.

Otherwise, they really are spec identical. Get whichever is cheaper. And both are factory calibrated to a very accurate degree.
Having used a 27" 4k, I would say that the text is just so much cleaner and sharper at 4k, even with Windows scaling. It's beautiful. As for gaming, 1440p + 2xMSAA doesn't look drastically different than 4k with 2xMSAA, though the sharpness does make the 4k picture look somewhat more 3D. It's difficult for me to describe.

Basically, 4k looked sharper, but it's harder to drive with your GPU, and it's 16:9 instead of 21:9 (and the difference between the two aspect ratios is personal preference).
>  All the research I've done shows that 50hz is maximum refresh rate. I got it to work at 60 with a custom setup through my NVidia settings.. any advice would be welcome. 

Try a lower resolution. It's a bandwidth limitation. You'll have to deal with interpolation though.
> I have a Dell S2716DG and the big flaw of this monitor is that the default gamma makes the image very washed out and is not adjustable in hardware

Because these monitors are not pre-calibrated, the picture quality will vary from unit to unit. If you get one with a horrid picture and don't have the means to calibrate, absolutely exchange it for another.

Ideally, Dell gets so many exchanges/RMAs that they realize, "Shit, it's just cheaper to calibrate them." Because lack of factory calibration is really the only flaw of these monitors (outside of panel weaknesses).
Just to expand on this, all G-Sync monitors are required by Nvidia to support adaptive overdrive. This allows the overdrive to be adjusted on the fly based on the current refresh rate.

If you have the monitor set at 100hz or 120hz, and your game is running at 90fps at a given time, the response time will be the same. But, the response time at 120fps will be lower than 100fps. In both cases, the response time will be adequate to minimize both ghosting and overshoot ghosting.

I do appreciate your advice, but I prefer to have some form of cap for my own reasons. Heck, I might even go back to 60hz! (doubtful).
You can get and iPhone 7 with Android preinstalled. It's called the Pixel 2.
That fixed it, thank you. 
I love glossy (that autocorrected to I love goat, which is also true), and there are precisely zero 144hz glossy displays. Sorry :(

And yes, glossy is superior.
Dell U2715H or U2717D are the top two choices based on your criteria and stated preference. I'm on mobile now, but can provide more details tonight when I get home if you want.
Can you PM me the original comment if you have it? If this is a study on GPU longevity, i'd like to see it.
Haven't tried yet. I don't plan to use anything over 100hz full-time as I'm primarily a 60fps gamer. But I'll try the OC this weekend and add that info into the updated post.
I only returned a monitor if it was truly defective. I don't like to abuse return policies.

One of my tricks was to buy on sale and/or from tax free locations. Then when the sale was over, I'd resell on Craigslist for about what I paid, often offering in-home setup for a bit more. A friend would piggy back off this and offer calibration for a fee, and paying me a kickback (I only recently started doing calibration myself after he moved out of the area).

I would estimate fewer than 10 monitors were returned to the store during that period.

Also, I didn't pay for every monitor. There are a few more in the area like me, with more money than me, and we'd swap our monitors to try out different ones.
Android OEMs: "I don't get it. Why is everyone buying Apple and Samsung? Why won't they buy our phones?"

I keep hoping some day they'll get it. If you want an iPhone, you buy an iPhone, not an imitation. If you don't want an iPhone, then you don't buy an imitation of the iPhone. And that leaves us mostly with Samsung, because everyone else is just (poorly) imitating the iPhone.
It was the automod. My post was removed due to possibly being a question or request for help. Human mods approved the post, and "deleted" the automod's post, which still counts towards the total IIRC.
I have an Ergotech Freedom Arm HD boxed up. It supposedly can handle the weight. If it can't, I'll buy something else.
We are actively voiding the contract. The contract itself voids because he hit an incentive, so we do retain our eligibility for the pick.

This is all moot if Wentz is healthy week 1 and plays all year.
That display is fine. Not everyone views a display with an all-black background, in a dark room, with brightness cranked, while dangling from the ceiling.

Off-axis panel glow (AKA IPS glow) is literally defined by the name - off-axis. Sit in front of the damn thing. That's how you use a monitor. Good posture, proper viewing distance, and brightness appropriate to the room's lighting all help greatly as well.
It reminds me of a juvenile stunt that a friend and I did as teens when we first gained internet access (on a 14.4 kbps modem, mind you).

We decided to enter a chat room on a gay website. We were joking around when we suddenly decided to ask, "Ok, in all honesty, how many people here are actually gay, and how many are just jerks like us screwing around?"

Turned out only one person in the chat room was gay. He left, came back with an admin, and we were all banned.

Trolling trolls is exactly what I think most flat earthers are. The very few who actually believe it are just too stupid that they are beyond reason and should just be ignored. They can't be reached.
No.The scaler on the monitor has to support the ability to adjust the refresh rate on the fly. There are still cheap scalers in use that cannot support this. There are some other ultra cheap scalers used for basic FreeSync support (~20hz range) that would not meet G-Sync specs. Also, a hypothetical adapter would need its own G-Sync module, which would be approved by Nvidia. They won't do this.

They COULD (but won't) support the DisplayPort Adaptive Refresh standard (as the already do on laptops, called G-Sync Mobile). Many people would call this "FreeSync," but it's not. FreeSync is one implementation of DP VRR, and Nvidia could brand theirs as well.

But again, they won't do this anytime soon. Buying a G-Sync monitor nets Nvidia a double-win, a hardware sale (the module), and you being locked into their ecosystem (requires Nvidia GPU for your next purchase, or are you going to toss that monitor?).
> The fact that you went to personal attacks so quickly proves you don't really have an argument to

The first personal attack was you calling me a jackass. So by your logic, you have no argument (though to be fair, your argument so far was based on not reading or comprehending what I wrote).
> Comp picks are not guaranteed. You can't bank on getting a comp pick. 

This was addressed before, and I'm not repeating myself this time. I don't know if you are unwilling to read, or simply unable to comprehend, but that sounds like a life problem you'll need to deal with.
I've been all about removing my old ROMs and buying games as they are released on PC. I'm all about supporting the developer and paying them when they release games I like.

But this is just a lazy cash grab. I won't be supporting it. I got burned on the mobile to PC ports of FF 3-6, and I'm done playing that game.
> I also love how in your scenario Wentz is completely out for the year. Come on


Selective reading. From my prior two posts:

> Let's look at the "worst" possible scenario - Wentz needs all 12 months to recover

Followed by:

> We're getting a 3rd as a comp pick for him (most likely) if we need him to start this year. If that's the case, I wouldn't trade him. If Wentz is truly going to be ready to start week 1, then I'd trade him if the value was there.

So, your concern was addressed. But you were so busy arguing, you forgot to read the post that you were arguing against.

Now, go ahead and downvote and get the last word in. That seems important to you :)
> Or Foles plays like crap next year and doesn't sign a contract a big contract somewhere else, we get no comp picks and then completely miss out on getting anything in return for him.

That's not how it works. If our starting QB leaves via FA, and signs somewhere else and ends up their designated starter, we're likely getting a comp pick. (and yes, there are other variables)

> Trade Foles now, get a day 2 pick and save some cap space. This is the year things are tight. Not next. Foles value is maxed out right now and only gonna drop.

Horrible idea.

> Banking on getting a comp pick is a terrible plan

And yet, some of the top GMs do it. We're getting a 3rd as a comp pick for him (most likely) if we need him to start this year. If that's the case, I wouldn't trade him. If Wentz is truly going to be ready to start week 1, then I'd trade him if the value was there.
That would be a bad idea due to how compensatory picks work.

Let's look at the "worst" possible scenario - Wentz needs all 12 months to recover, so he's not ready until December. At that point, why not rest him? Make Foles the designated starter the rest of the year. Why?

If he's still an Eagle after his 2nd year, his contract voids and he becomes an unrestricted free agent. He would have been our designated starter at the most important position. And he would not be replaced by a FA since Wentz is already on the roster.

That's a 3rd round compensatory pick by most estimates, 4th at worst if he goes on to suck at his next destination that following year.
I'm not bothered in the least by small amounts of input lag. I just use V-Sync on. Simple solution, no tearing, and lag is acceptable for me.
You left out /r/AMD, the alt-right technology subreddit.

Last time I posted there, I was called a pedophile.

I take a negative view of AMD products because of their fanbase.
I would start telling customers, "we are required to do ANYTHING that you ask. If you tell us to load a product that you didn't pay for, we have to do it. What would you like for free?"
The problem with Nicehash is that it is a super easy to use tool for a not so easy to understand subject. You have your work cut out for you and I wish you luck.

You're also going up against the Dunning-Kruger effect.
You can do that, but you still will not get an option from the game's menu for 2560x1440. The highest option for 16:9 games will be 1920x1080.

You'll need to add a custom resolution to get it to show up.
It does not.
Yup, I've already confirmed that the AW3418DW supports 2560x1440 pillar boxed, as do most other 1440p ultrawide monitors.
All of LG's 1440p Ultrawides so far have the same flaw, no support for 2560x1440 content. It forces 1920x1080 for 16:9 full screen games. If you still have a decent library of 16:9 games that you intend to play, avoid LG.
I can't recommend the ICC profile from TFTCentral or any other source. Monitors vary unit to unit. You'd be surprised how much. Their ICC profile corrects for their unit's deficiencies. For yours, it could overcorrect, undercorrect, or fall somewhere in between.

It's a $1k monitor. A Spyder 5 Express is $130 or less, and DisplayCal is free.

CC: /u/code4109
Just want to second this. Ordered mine Sunday via Dell.com price match. Arrives Thursday. OP, consider price matching and ordering direct from Dell.