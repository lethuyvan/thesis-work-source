Somehow I expect you wouldnâ€™t give that sort of treatment to something like Frank Dikotterâ€™s book, Maoâ€™s Great Famine. 
Iâ€™m sure the independent socialist will provide a fantastically unbiased look at this. 
>scientific principles guided by Marxism

I hope itâ€™s not the same principles that starved 50 million Chinese to death during Maoâ€™s Great Fuckup Forward. 
/u/zombiesingularity is moderator of r/fullcommunism, and his post history is full of examples defending atrocities committed by communist regimes. 

I guess we can hope itâ€™s simply Poeâ€™s Law in effect, but some people really are that ideologically possessed. 
The United States sends your entire family to labor camps and kills them if youâ€™re caught escaping?


You really need a long hard look at your life and how you wound up this low when you are caught unironically defending the worst human rights violating, oppressive state the history of mankind has ever known. 
Posts in r/fullcommunism. 
> I haven't played BOTW, but Horizon Zero Dawn definitely deserves its GOTY award.

Lmao
Which includes no smartphones...
From the article:

> Whatâ€™s particularly interesting is that the photo itself doesnâ€™t even have to be particularly high quality

You donâ€™t even need a â€œmugshotâ€ photo. Just a photo where their eyes are visible. 
Is it?

All you need is a picture of someoneâ€™s face and apply an IR filter over it. You can get a decent facial picture from anyoneâ€™s social media account. 

Then all you have to do is plop some cheap contacts over the eyeballs on the photo, and presto, youâ€™re in. 
[You were saying?](http://bgr.com/2017/05/23/galaxy-s8-iris-unlock-hacked-video/)
But it can be fooled with a photograph and disposable contact lenses and no fancy equipment. 
They can just pull an accurate face picture from your social media. 
Depth sensing windows hello tech does not exist on smartphones. 
It would be an ergonomic nightmare to type on the iPhone X, especially one-handed, if you were to move the keyboard to the very bottom.  You need space to let your thumb not overextend. 
The iPhone is not *meant* to watch videos. Itâ€™s a general computing device and phone that happens to also display video content. 
Breath of the Wildâ€™s approach to the open world concept wasnâ€™t standard, which is why it won. 

It was a refreshing approach to open world design, it was *actually* open world and non-linear. Most open world games, Horizon included, are actually fairly linear. With a set path for how the campaign plays out. Breath of the Wild is completely open and letâ€™s the player decide how to approach it. 

It was simply the more ambitious game. 
And it canâ€™t possibly be because theyâ€™re just *great games.*
No. It wasnâ€™t. 

For the game of the year category, votes were decided by an organization of 52 industry experts and influential figures and writers of the industry. 
Name a single Nintendo game besides BotW to have ever won Game it the Year from The Game Awards or its predecessor, the Spike Video Game Awards. 

Yeah. Thatâ€™s what I thought. 
As far as open world games go, Horizon is fairly basic. Itâ€™s just another map game. 

BotW really was a breath of fresh air into the genre.  Itâ€™s general gameplay was also a bit deeper than Horizon, being heavily systems and physics based. 
Fans have very little voting power when it comes to the GotY category. 
Which is exactly why it didnâ€™t deserve to win. 

These are games first and foremost, not movies. If your gameplay just â€œstandard,â€ then it doesnâ€™t deserve something like GotY.  

Breath of the Wild reinvigorated the open world genre. 
Read carefully. 

Horizon didnâ€™t lose because itâ€™s story was cliche and unoriginal, it didnâ€™t win because itâ€™s a fairly generic game, mechanically speaking.  Itâ€™s another by the numbers â€œopen worldâ€ game. BotW might not have a story that matches the kind of production values Horizon has, but as far as *gameplay* goes, itâ€™s got Horizon beat.  Itâ€™s take on the open world concept is refreshing and the gameplay mechanics are much deeper and heavily systems based.  BotWâ€™s story delivery is also novel, being completely non-linear as a compliment to the gameplay design. 
This isnâ€™t a surprise to anyone.  Have you been living under a rock?
Itâ€™s a pretty game, for sure. But at the end of the day, itâ€™s just another map game that doesnâ€™t do anything particularly novel or exciting with its open world concept, and itâ€™s narrative is filled with cliched plot elements weâ€™ve seen done better in other mediums. 


Thatâ€™s because BotW DLC is an expansion pack filled with original content for the base game, released more than half a year after launch.  

Battlefrontâ€™s microtransactions simply limit already available content behind a gambling paywall. 

Thereâ€™s a reason why one is good and the other is bad. 
How so?
> That's "A[?]  Agree."  

FTFY
Itâ€™s not. Trader Joeâ€™s is a company that started in Pasadena, CA. The company was purchased by Theo Albrecht, a German, who also happened to own Aldi. 

Today, Trader Joeâ€™s still operates off the original store model, and not Aldiâ€™s. 
I am so burned. 
Sob story + mundane picture = karma
Only debit cards can add money to the Apple Pay Cash cardâ€™s balance. 
Do Europeans not read articles or something?

It says Apple deposited the money into an escrow account, while it appeals the ruling. Itâ€™ll withdraw the money out again if successful. 
Itâ€™s sarcasm. Meant to ridicule your delusional explanation.  
> I was countering with people here possibly not using mobile photo editing because they use far better hardware and software for that.

Uh huh. That explains it. 
Iâ€™m done talking about this with you people. 

Reread the person I was originally agreeing with and understand the context of the discussion.  Yup. 
No. I said people here donâ€™t understand the appeal of photo editing on smartphones, which people here commonly claim â€œno one does editing on smartphones.â€ 

The argument is that tons of people do, primarily for social media. 
Well, what is â€œproper photo editing?â€

For a huge amount of people, doing quick touch ups on apps like Prisma or FaceTune before posting things on social media or sharing with their friends is all the photo editing they do. 
I hope the irony isnâ€™t lost on you. 
Whatever makes you feel better. 
> You can easily argue this is systemic racism based on the fact they only test their products with white people.

*On the fact?* 

You are privy to this information, how exactly?  The fact that this isnâ€™t a widespread issue points to the fact that it works on Asian people just fine, and this is something unique to OP. 
Am I in a fever dream or something?

The topic of the entire post was about SoC power specifically. Take it up with the OP, not me. 
Iâ€™m not talking about HDR. Iâ€™m talking about actual photo and video editing, like Lightroom and photoshop. Those kinds of things. 
ðŸ™„
Ok?
The fact that iPhones routinely outperform everybody else by a significant margin specifically in photo/video editing tasks says otherwise.  
Having a good camera, and having an SoC thatâ€™s fast at photo and video editing and exporting are two entirely different things. 
RAM, storage speeds, and operating system optimization. 
I have no idea what youâ€™re talking about. The topic of this post is about SoC performance, not overall phone performance. 
Whatâ€™s the difference between a good film and an entertaining film? Is entertainment not a good quality?
Only because of RAM. When it comes to real world testing of apps that are actually CPU intensive, Appleâ€™s SoCs are the clear winners. 
Spoken like someone whoâ€™s never played the game before. 

The truth is that this game is more densely packed with content and activities than every other â€œopen worldâ€ game released in the last couple of years. 
Well, it is the break of dawn and the player is looking directly into the sunrise. Normally, the colors are fine. 
I think itâ€™s because itâ€™s a more feminine style, so they are limiting it to the smaller size since most women have small wrists. 
This. 

People here are so quick to dismiss advantages in photo editing, because â€œhow often do people edit photos?!â€  Actually, a lot of people do, for the reasons you outlined. But I suppose the techie, anti-social shut ins of r/Android wouldnâ€™t understand. 
If weâ€™re being honest, r/Android doesnâ€™t care about performance only because Apple has been smoking everybody with their SoC designs. 
I picked that article, not because of what was written. In fact, you could totally ignore everything he said and my point would remain the same.   

The point was to showcase the two different approaches the phones use in blurring the background. The Pixel oversharpens the subject and attempts to completely isolate the subject from the background. The creates a more or less pleasing photo overall, but not exactly a natural looking one. 

The iPhone Xâ€™s approach is to create a bit more softer image, both in the face, as well as near the edges of the subject. While some people may see this soft blurring of the extremities as the iPhone X â€œmessing upâ€ depth detection, itâ€™s actually emulating what a DSLR would output. 

So, again, I didnâ€™t pick that article because of what it said, but because of the sample pictures it used were highly indicative of what Iâ€™ve been talking about. 

Again, learn to think for yourself. Are you going to at least admit that proper shallow depth of field effect with a DSLR will blur the extremities of a subject when taken close up, as depicted in the sample photos I provided you?  
MKBHD isnâ€™t a photographer by trade, and seemingly, neither by hobby.  The fact that you make *videos* doesnâ€™t inherently mean you are knowledgeable or experienced with photography, which is an entirely different art form. 

And you donâ€™t have to take my word for it. Take a look at any real portrait photo taken with a good DSLR and come tell me you shouldnâ€™t be blurring the edges of ears, hair, and shoulders. 

[1](https://neilvn.com/tangents/images/models/olive/DSC_0134.jpg)

[2](https://i.pinimg.com/736x/50/84/97/50849712bfbf489ce98885e771d3fff9--beautiful-redhead-young--beautiful.jpg)

[3](https://images.macworld.com/images/howto/graphics/150150-dof-blong-figure-5_original.jpg)

[4](http://keyassets.timeincuk.net/inspirewp/live/wp-content/uploads/sites/12/2011/07/11134_7C000001921_7Cf4d3_Nikkor_AF_S_50mm_sample_image2.jpg)

[This article](https://m.imore.com/iphone-x-vs-pixel-2-portrait-selfie-showdown) really demonstrates what Iâ€™m talking about. Besides the iPhone X more naturally capturing the subjectâ€™s skin tones (author confirms this), the Pixel 2 isnâ€™t emulating shallow depth of field properly.  The subject is way too in focus and looks like a cutout. The iPhone X naturally adds a little bit of blur towards the edges of the subject, like you would see on a DSLR. 

Think for yourself. 
Yeah, turns out that most tech reviewers, like yourself, donâ€™t know the first thing about photography. 

What is â€œbetter?â€  I guess thatâ€™s subjective, and some people may actually prefer oversharpened, overly cool photos with an unnatural application of depth of field bokeh, brand bias aside. 

But if weâ€™re defining â€œbetterâ€ as â€œbeing as close to DSLR as possibleâ€ than the iPhone X is much more natural looking and something you would expect out of a good DSLR. 

You can call me deluded if you like, but I wouldnâ€™t talk until you actually educate yourself on some simple photography basics.  Otherwise, you will continue being the textbook example of the Dunning-Kruger effect as youâ€™ve always been.  
Most people Iâ€™ve seen praise the Pixel 2â€™s portrait selfieâ€™s usually donâ€™t know what theyâ€™re talking about or have any experience with photography. 

Most of the criticism thrown at the iPhone Xâ€™s selfie portraits center around the fact that things like hair and ears are blurred, while the Pixel 2 perfectly cuts around the personâ€™s head and separates it from the background via blur. 

Thing is, thatâ€™s not how shallow depth of field works in real portrait shots taken on a good DSLR.  Besides the fact that the Pixel 2 is way too aggressive at edge detection, making subjects look like cutouts on a blurred background, thatâ€™s not how shallow depth of field works. On *real* portrait shots, the ears, shoulders, and the back of the hair should be slightly blurred, since they are slightly behind the focal point (eyes, nose, and mouth).  

Not to mention that the Pixel 2â€™s photos are *waaaay* oversharpened. Less sophisticated users might like extreme artificial detail in their selfies, but ask any photographer and theyâ€™ll point out the ridiculous oversharpening Google adds to their photos. 
Good? Yeah. Greatest game ever? Letâ€™s dial back the hype a bit...

Itâ€™s just another map game at the end of the day. Which doesnâ€™t do anything particularly new, and with a story thatâ€™s a hodgepodge of narrative elements weâ€™ve seen done better in other mediums before. 
Yup
Well, itâ€™s worked 100% fine for my usage in Los Angeles.  Again, Iâ€™ve never not found a business or POI I was looking for. 

Is my experience less valid than yours?  Iâ€™m sure I could find people whose experience with Google Maps *isnâ€™t* 100% fine. Are their experiences not valid?
The point was that any digital mapping solution should be 99.9 correct when using it in order to call it â€œfine.â€
I find it pretty hard to believe Apple Maps is lagging an entire km behind your car. 
And itâ€™s certainly not 99.9 percent perfect either. 
Sounds like thereâ€™s a defect with your GPS chip, or youâ€™re spouting some grade A bullshit. 
Iâ€™ve never not found a business or restaurant I was looking for here in Los Angeles. ðŸ¤·â€â™€ï¸ 
Be quiet. Youâ€™re interrupting the circle jerk. 
Letâ€™s not pretend Google Maps is infallible. All digital mapping solutions are imperfect. 
Some people have deleted Facebook or never had it. Also, some people donâ€™t want to share that kind of important information with a company like FaceBook. 
Apple Maps is fine, stability wise. What it lacks is data, which is not something you can just create and gather in a year to the levels weâ€™re demanding. 

And seriously, Iâ€™ve used Apple Maps in major population centers like Los Angles, Chicago, Washington DC, New Orleans, and New York, and itâ€™s never once failed me.  Apple Maps is fine. 
Nope. 

Iâ€™ve traveled to the Metro Kingdom via painting before I was able to get there via the Odyssey. 
Is Eddie Cue the Meg Griffin of Apple?
You mean recalled, quickly replaced with the promise of them being safe again, only to explode again, causing a full total recall? 

Yeah, I remember that. 
Thatâ€™s not what makes google great.  What makes google great is their massive collection of personal data and just data in general. Itâ€™s what makes services like google maps, assistant, and their search engine work so well. 

When it comes to hardware, where they canâ€™t really benefit from their data, their weakness and lack of vision really shows.  One just has to look at the PixelBuds to see how bad at hardware and design they really are. 
The marketing speak from some people on this sub is like... corny as hell. 
> The marketing speak from people on this sub is like...corny as hell. 

...

> I personally am fine voluntarily giving data up for better services and better ad experiences.
Google needs vision and focus. 

The reason most services and hardware that come out of google flop or go back into development hell is because the company lacks visionaries and a workplace culture that emphasizes focus in design. 

Google seems to just throw shit at the wall and see what sticks, rather than deliberately design around a particular experience they want consumers to have.  This is why Android Wear is so haphazardly limping along while Apple Watch dominates marketshare and mindshare.  Itâ€™s why Android tablets are all but dead. 
> I really don't care to have this discussion.

*Has discussion.*
iMessage seems like the only messaging platform I would ever trust with my data and privacy. 
Ozen does imply some actual time dilation going on, as she says one of the reasons Lyza might still be alive is because time for her hasnâ€™t passed nearly as much than it has for those on the surface. 
Thatâ€™s what the free market is for.  

If you donâ€™t like what a company is doing, stop purchasing games from them. There are lots of developers and publishing giants that donâ€™t feed into this predatory business model, like Nintendo. 
He was way cool. 
Jim Nabors is way cool. 
Who doesnâ€™t love starving to death?
Gotta love those secret hobo spices. 
> Seeing FaceID work on multiple devices the past few weeks I've seen FaceID take up to a second to work at times. Fingerprint sensors are still much faster.

Not in all situations. Any situation where the phone requires a fingerprint scan in order to get into an app from the lockscreen, or to authenticate before entering the app (banking apps) are done significantly faster with FaceID. This is because it takes time for your finger to physically move from the touch screen to the button. 

> The swipe up makes it so that even things like OnePlus's face Unlock is 5-10x faster than FaceID.

Except that solution blows past your lock screen, doesnâ€™t work in the dark, and can be unlocked with a photo.  Swipe to unlock is a good thing. 

You donâ€™t know what youâ€™re talking about. 
By the time I hold the phone up, itâ€™s already powered on and scanned my face.  Raise to wake is a thing that exists. 

Having two biometric options would just confuse the user.  With how integral Apple seems to be setting up FaceID to the user experience, itâ€™s stupid to distract any attention away from it.  
FaceID doesnâ€™t require more steps. You just swipe up and it unlocks. 

In fact, unlocking your phone into an app via notification with FaceID is significantly faster than TouchID ever will be. 
Are you acting as if fingerprint scanning fails from damp/oily/dirty fingers and smudged scanners donâ€™t happen?


What about Splatoon?

While there is a choice of which sex you want your Inkling to be, thereâ€™s no question that the most popular and the most promoted character is the Inkling girl. 
Itâ€™s also a completely different and physically larger device. 

So, no, not interesting. 
Youâ€™re the expert, right?
No, he couldnâ€™t. He barely fit the jack in there as is, and that was after moving some other parts around. 

Adding water sealing gaskets would be next to impossible. 
And the waterproofing. 
Nice straw man. 

You can identify yourself as a woman, but if you still look like a dude, people will identify you as a dude.  You can control how you look, but your identity is still ultimately left up to society. 
Not sure if being sarcastic. 

For a long time, and just until recently, gender identity disorder was an actual disorder.   And you donâ€™t actually get to decide what your identity is, your identity is determined by societyâ€™s perception of you based on your physical and behavioral presentation. 
From videos Iâ€™ve seen of him, he clearly does believe that transgender people exist, he just acknowledges them as people with a mental disorder, and he doesnâ€™t believe that people can dictate how others address them. 
[S8 definitely has bezels](https://phandroid.s3.amazonaws.com/wp-content/uploads/2017/04/galaxy-s8-first-things-1.png)
[Shut up.](http://www.businessinsider.com/displaymate-iphone-x-best-smartphone-display-ever-2017-11)
FaceID doesnâ€™t work on pictures because FaceID is looking for depth information which a 2D photo lacks.  A mirror, however, does provide 3D information. 

> So do you guys think the IR scanner bounced off the mirror and onto my face, or read my reflection in the mirror?

The infrared light bouncing off the mirror and onto your face *is* reading your reflection. 
No problems on the Apple TV 4K app. 
And weâ€™re calling it...

**iPhone**
These are not three separate devices. This is one device!
Are you getting it?

A phone.  An iPod. And an internet communicator. 
The fact that a nation is wealthy overall doesnâ€™t mean people who buy android there donâ€™t do it because itâ€™s what they can afford. 

The United States is pretty wealthy, but iPhones dominate the upper sections of society.
As a revolutionary mobile phone, a widescreen iPod with touch controls, and a breakthrough internet communications device. 
Because India and the US are different countries with different spending habits and overall wealth. 

India, for most people, is simply priced out of Apple products, even with a payment plan.  The people who can afford the payment plan probably have enough money to buy it outright no problem anyways. 
Apple is not going to do monthly installment payments of only 2500 Rs a month for their premium phones.  Thatâ€™s way too low. 
$55 a month isnâ€™t crazy expensive here in the US. Thatâ€™s well within the reach of a whole lot of people. The equivalent in India, however, is crazy stupid expensive. 

I donâ€™t think you understand India. 
Almost cut myself there. 
India is a developing country. No flagship phone is doing well there. The only kinds of phones selling in India are things like the CoolPad Pro or Samsung On7 4G. 

As for why there are no installment plans for India, itâ€™s because Indians are largely priced out of Apple. Vodaphone charges something like the equivalent of 55 USD a month for their high end phones, which is considered crazy expensive in India for the vast majority of people. An iPhone level monthly payment would be absurdly high. 

Chances are, if someone can afford the monthly payments, theyâ€™re the type of person that could just buy them outright anyway. 

Furthermore, Appleâ€™s direct sales channels in India still arenâ€™t that developed. 
Sure, the S7 and S7 Edge took fifth and ninth spots respectively for best selling smartphones of the year. 6th through 8th place were all Samsungâ€™s lower tier handsets, like the J3. 

Considering the S7 Edge is way down in ninth place, and there are three other models of phones besides the S7, I think your claim is very dubious. 

[http://news.ihsmarkit.com/press-release/technology/apples-iphone-6s-topped-list-best-selling-smartphones-2016-ihs-markit-says](http://news.ihsmarkit.com/press-release/technology/apples-iphone-6s-topped-list-best-selling-smartphones-2016-ihs-markit-says)
Source?
Itâ€™s irrelevant because fill rate is determined using the ratio  of active illuminated area to dead space.  It doesnâ€™t matter how large the pixels are, since itâ€™s a comparative value between the size of the pixels and the other variable, being the dead space.  If pixel size increases proportionally with the dead space, fill rate stays the same. 

Give this just an ounce of thought. 
You shouldnâ€™t have to worry about these things if theyâ€™re on the Playstore. Google should do a better job protecting their customers. 
> And the fact of the matter is that not all Android phones are shitty cheap ones. 

Never implied otherwise. However, the *majority* of android phones out there in the wild are cheap junk.  The fact that Apple, which essentially sells only premium phones, sells more premium phones in a single quarter than most Android flagships combined annually says a lot. 

Most importantly, it says that even in wealthy European countries where android is dominant, the majority are not made up of flagship devices. 
Cool story. But my point was simply that the United States isnâ€™t alone in heavy iPhone adoption. 
A lot of the world is simply priced out of Apple products in general. 

As far as developed countries go, the United States isnâ€™t alone in its iPhone dominance.  More than half of all smartphone shipments in Japan were iPhones in 2016.  Android market share in Australia is barely more than half, despite several manufacturers competing against Apple. 
The majority of people that own androids only do so because theyâ€™re cheap.  When the majority of the population resorts to having to buy low to mid range junk, itâ€™s no surprise that people naturally associate Android with crap. 

Now, mid-ranges have certainly gotten better lately, a lot better in fact, but the majority of Android users settling for things like the Galaxy J Prime Echo 4G doesnâ€™t help much.  For quite a while, your average Android that wasnâ€™t a flagship was just junk, and that stigma takes a long time to clear. 
Your entire is based around the meaning of â€œtrend.â€


It shows a trend of rising market share between 2016 and 2017. 

Now whoâ€™s arguing semantics? Lol
Except the source I gave you clearly shows that iOS market share increased from 2016 to 2017. 
There are color variations, but theyâ€™re largely consistent as it helps with branding.  When people think â€œrose goldâ€ for electronics, Apple is usually one of the first things that come to mind. 
I thought Android had â€œTrue Multitaskingâ„¢?â€
Whatâ€™s a â€œreal computerâ€ is entirely up to the person. iPads are â€œreal computersâ€ for a lot of people because it happens to meet all their needs for a computing device. Which was the entire point of the video. 
Whether the subpixels are big or large is irrelevant. Whatâ€™s important is the ratio of dead space to illuminable area. 

Itâ€™s certainly possible for you to have a higher PPI and higher fill rate. 

[Here](http://www.displaymate.com/Samsung_Diamond_Pixels_2.jpg) is a picture of the S5 display taken by Displaymate. 


LCD displays and OLED displays are radically different technologies. Your comparison is bad. 
What? So you *donâ€™t* like a mouthful of just sesame seeds and rice when eating sushi?
You said it has *only* gone down. My Link shows this is demonstrably false. iOS market share has risen in many markets. 
Are you seriously committing the genetic fallacy?
Good thing these speedtests are more than app loading times with apps like photoshop thrown into the mix. 
> Continue with your childish kindergarten-style arguing with others, and good luck.

Says the guy whose comments got deleted for literal name calling. 

ðŸ’…
Yeah, [lol](https://imgur.com/a/SVPzx)
My upvotes say otherwise.  
At least I can see my arguments. 
Youâ€™re not gonna get all worked up and get a warning from the mods again, are you?
Then it went and did an actual test and demonstrated it was correct. 
Actually, my link did say that. It went over the hypothetical reasons why it should happen, then did an actual test that demonstrated that itâ€™s true. 
> Then find one or do one, if you are saying the new iPhone isn't slower in day to day use, then you have to prove. 

For all your whining about the burden of proof, you sure donâ€™t seem to understand it very well. 

The person stated that the newest iPhone are slower in day to day use. Me asking if they have anything to substantiate that claim isnâ€™t a claim itself.  Therefore, the burden is on them, not me. 

> Have you considered the reason who no one have done a test of the newest beta is because nobody have seen any speed improvement hence why it would be pointless for anyone to use time on it?

[Citation needed]

> The way you are setting it up we also have to test the note 8 every time the monthly security patch... 

It only seems fair that if youâ€™re going to compare two phones, they are tested with the latest software available to them. 

> Unless you actually have proof of the new iPhone being faster than the Note 8 in day to day use, all of your arguments are empty, and there is nothing to discuss with you.

Unless you have proof of the newest iPhone running the latest version of iOS is slower than the Note 8 in day to day use, all of your arguments are empty, and there is nothing to discuss with you. 




 
You arenâ€™t paying attention very well. 

He stated â€œthe newest iPhones.â€  The newest iPhones are iPhones also running the latest version of iOS. 

There are no speedtests I know of that are comparing iPhones with the latest iOS version. 

Also,

> First argument, which was wrong

No, it wasnâ€™t. Itâ€™s true that the iPhone won the first round in this speedtest.  It only lost overall due to RAM management issues. 
The screen also has the ability to get significantly brighter. Not to mention that battery life tests are highly variable. It entirely depends on what youâ€™re testing.  If youâ€™re testing with impractical sustained load scenarios, then yeah, it wonâ€™t fair so great. 

Yeah, I get it, the link I gave isnâ€™t convenient for you. 
Itâ€™s an actual question. 

Burden of proof requires me making a claim. I asked a question. 
The fact that iPhones, especially the Plus models have excellent overall battery life despite having extremely small battery capacities compared to many android phones is proof enough. 

If you want to see a technical analysis of the race to sleep/idle concept and itâ€™s practical applications, refer to the AnandTech article I posted earlier. 
No, I just asked him a question, which put the burden of proof where it belonged. 

Or are you not concerned about that anymore?
Again, they use more power, but generally in burst scenarios.  The trade off is that while they spend more energy actually performing the task, they complete the task much faster and spend more time at idle. 
I donâ€™t think you thought this through, considering the first sentence I said started with â€œwe donâ€™t know.â€
No, itâ€™s how these chips work. 

The faster you complete a task, the more time you can spend having your SoC rest at idle. 

> Why would being faster make a microprocessor use less power? The concept is called race to sleep. At idle the CPU in an SoC is mostly clock gated if not power gated entirely. In this deep sleep state, power draw is on the order of a few milliwatts. Under full load however, power consumption can be well above a watt. If a faster processor consumes more power under load but can get to sleep quicker, the power savings may give it an advantage over a slower processor. 

[Source](https://www.anandtech.com/show/4971/apple-iphone-4s-review-att-verizon/15)
I guess thatâ€™s a no then. 
They use more power when driven to the max for sustained loads, but the fact that they can accomplish most tasks quicker and then return to idle means it is using less overall energy throughout the day. Thatâ€™s what race to idle is. 

Hiccups and stutters arenâ€™t really a problem with iPhone X. Especially not with iOS 11.1.2. 
Didnâ€™t Oreo officially launch over three months ago?  Holy moly. 
You see that current Oreo adoption rates are at 0.3%, right?
[https://cdn57.androidauthority.net/wp-content/uploads/2017/10/Android-Platform-Versions.jpg](https://cdn57.androidauthority.net/wp-content/uploads/2017/10/Android-Platform-Versions.jpg)
Not so luckily, basically no one is on Oreo or will be able to even get Oreo.  Not to mention I find that claim dubious. Iâ€™ll believe it when I see it rolled out en masse. 

Android is terribly fragmented. 
iOS allows apps to make calls in the background, itâ€™s just limited to five or ten minutes, depending on what the call is. 

Itâ€™s a double edged sword, really.  Thereâ€™s a reason Android is notorious for having wildly unpredictable and inconsistent battery life due to background apps run amok. 
Well, we donâ€™t know that. 

Have you ever seen a benchmark comparison featuring an iPhone X running the latest iOS 11.1.2 release or the latest 11.2 betas?
The RAM management issue has already been partially addressed in iOS 11.1.  And iOS 11.2 is just around the corner. 

Samsung is familiar with this kind of thing as well. The S7 was notoriously shitty at RAM management up until an update a few months after launch 
> Lol. I'd disagree. But I think 99.99% of the average user wouldn't care or be bothered by it, considering editing a photo isn't a daily thing. Also exporting photo on the default Samsung editor is pretty instant. 

 2 seconds vs nearly instantly is pretty significant.  And when youâ€™re using beautification apps and editing via instagram daily, you start to notice. 

> It would if they were common instances. But no, in common instances the iPhone X isn't consistently and noticeably faster at all.

Games and photo editing arenâ€™t common instances?  Are you sure youâ€™re not just out of touch with the general public?

> As is gaming, and video exporting. The iPhone X's CPU isn't the only one capable of it.  

Besides the iPhone being significantly better at both those things, thatâ€™s not the point. The point is that while other phones may be able to technically handle those tasks, they do so with less stability and battery efficiency. 



> Maybe by 1-2 seconds. Which is pretty negligible. No one buys a phone so they can export photos few seconds faster. 

Two seconds is actually pretty long. And no, no one buys iPhones because theyâ€™re 2 seconds faster at photo editing, but all these instances add up and groom the user into thinking iPhones are overall faster, snappier devices. 

> Citation? Battery usage is mostly dependent on the cpu generation. 


AR is incredibly CPU intensive.  What is your point?
A more powerful SoC impacts a lot of what your average user does, even if they donâ€™t know it. Battery efficiency, for one.  By a race to idle, extremely fast SoCs provide pretty great overall battery life by being essentially inactive for the vast majority of time. 

It also makes for an overall smoother experience with less hiccups and stutters. 
Maybe youâ€™re not thinking very hard?

The iPhone 7 Plus has the same amount of RAM as the X, yet it still trounces the Note 8 in these speedtests, despite having half the RAM.  So itâ€™s clearly a problem that can be fixed with software, since software is what caused the problem in the first place. 
> Photo editing is really quick on modern smartphones. I don't see why it would be an issue. Like I've said, negligible. 

And itâ€™s significantly quicker on iPhones.  Which is the point. 

> AR is actually not very processor intensive. Snapchat, and PS Vita have been doing it on weak processors. 

And they eat up the battery and arenâ€™t that smooth. 
I wouldnâ€™t call photo editing a rare occurrence.  Beauty touch-up apps are incredibly popular. Other processor intensive things like AR and real-time shallow depth of field effects visible in the viewfinder are commonly used. 
Basically, any application where the processor is actually being put to work. 
So iPhones have better processors and memory storage. Ok. Got it. 
Sure. But app loading isnâ€™t the only criteria these kinds of speed tests are measuring. Video exporting, for example, is heavily processor intensive. 
Considering that the iPhone 7 running iOS 10 consistently beat the Note 8 in these same type of tests, youâ€™re probably very wrong. 

There are no more bottlenecks for light apps these days.  All load variances are explained by wireless signal or animations, not processors. 

What actually shows the differences are heavy apps, like games or video exporting. 
Well, consider this. 

The iPhone 7 Plus running iOS 10 has been able to beat the Note 8 pretty significantly in these types of speed tests.  Obviously thereâ€™s something funny going on. 

Itâ€™d be interesting to see what speedtests would look like if the iPhones were running iOS 11.1 or 11.2. 
Itâ€™s almost as if they make great products.
In these â€œspeed testsâ€ the iPhone is actually faster in the first rounds. They only slip up due to poor RAM management in the second round, which will be fixed in the upcoming iOS 11 updates. 
Thatâ€™s how itâ€™s supposed to work. 

If you look similar enough, but not enough to unlock it, itâ€™ll scan your face and adapt if you provide a PIN. 
> The take away you should have gotten is that the size of the pixel (i.e. the clear opening that allows the light through) is immaterial to the brightness of the LED/screen which is determined by the conversion of electricity to light of each LED.

Displaymate disagrees. As they specifically mentioned fill factor as a primary reason why the iPhone X display can reach such high brightness for high apl levels.  If it was solely pumping more juice to the display, they would have never mentioned this. 
JNCD from D65 is important because it signifies how big the difference the displayâ€™s white point is from the D65 standard.  The bigger the JNDC value is, the further that displayâ€™s white point is from the D65 standard. 

That being said, JNCD values below 1 are indistinguishable by the human eye. 
Yeah, and they arenâ€™t the same nit levels, whatâ€™s your point?

> You don't need to be a genius to understand that, that's not linear. 

And?  Power draw to full screen brightness is never linear for any OLED display. 
Great argument. 
Fill rate hasnâ€™t changed much. Subpixels just seem to get smaller. 
What does that matter?

There is more dead space between pixels on the Note 8. If we normalize the sizes of the subpixels between the two displays, you can clearly see that the Note has more black, empty space inbetween itâ€™s own pixels. 
Looks like Iâ€™ll have to spell it out for you. 

If, according to you, an OLED needs to draw more power in order for a display with smaller pixels compete with the brightness of another display with normal sized pixels at the same resolution and size, then youâ€™ve proven my point. 




PPI doesnâ€™t affect this at all, since the size of each pixel has to shrink, but the fill rate remains the same. 




Space between pixels in relation to the pixels size.   
Iâ€™ll give you five more minutes. 
Thatâ€™s irrelevant, since weâ€™re concerned with the ratio of active illuminated area to dead space.
> It could if you pumped more current through it.

I'll give you a minute to think about how you just played yourself.


So you think that if sub pixels *shrank* while keeping resolution the same, it would still output the same amount of brightness?

El oh el.
You're right, the numbers do speak for themselves.

JNCD of 0.2 from D65, compared to 0.8.
That's not actually what we're talking about. My point was that given the same nit levels, the iPhone X display will burn in less due to the fill rate.  Obviously, that also means when we go into territory that other displays can't output, you'll have worse burn-in.


"Quit acting like you know what you're talking about, when you clearly don't."  - masterofdisaster93
> "Rounded subpixels" have nothing to do with fill factor with respect to the note and iPhone x. It could be if the subpixels were the same size but there is zero evidence of this since the two pictures are completely different magnifications.

Just look at the pictures then.  There is clearly less black area in between the subpixels. 


>You can just increase brightness by reducing black area between pixels, that makes zero sense.

So what's brighter, an open window or a window with a screen door?
>Power usage on highest brightness on manual is excessively high.

It's also much brighter than the Note 8 can achieve at the same APL.
> No, I'm not. I was talking about power efficiency. Quit acting like you know what you're talking about, when you clearly don't.

You clearly were, as Displaymate has not measured either the power draw of the two displays at the same nit level, only the same APL level.

>That's the whole point, genius. The iPhone X can reach higher brightness at manual, which increases the chance of burn-in. The higher manual brightness and much higher power usage will have a clear effect.

That's actually not the point, genius.  The point is that the iPhone X display can achieve the *same nit levels* as other OLED displays without stressing the sub pixels as much.  Of course you'll have higher power draw when you go significantly beyond the brightness other displays can achieve.

>Did it also ever enter your mind that the iPhone achieves its level of brightness with much fewer pixels, stressing them even more?

No, because I'm not an idiot.  The iPhone achieves such high brightness precisely because of the higher-fill rate, which means more illuminated surface area.  

>On the iPhone X the resulting Sub-Pixel fill factor is much higher than other OLEDs, which is a key factor in providing the much higher full Screen Peak Luminance of over 625 nits.  - Displaymate
>The pictures are not even the same magnification and don't even show the same number of pixels. they don't clearly show anything. I've repeated this a million times.

Magnification completely irrelevant.  Subpixel size is constant throughout the display.  There is simply more dead space between sub pixels on the Note 8.  I'm not sure why this is so hard for you to understand.  It doesn't matter if the Note 8 picture is more magnified.  It still shows that per area of active pixel there is more dead space.

>Where do you think the illumination comes from? Subpixels. How do you get the same illumination with less subpixels? But pumping more current through them to make each subpixel brighter. 

That's one way to do it.  The alternative, which is what Apple has done, is too simply increase the active illuminable surface area.  

What's brighter?  An open door or a screen door?
>They archive a good fill factor by stressing the subpixels more.

Now you are talking absolute nonsense.  They achieve a good fill factor by not using rounded sub pixels.  Fill factor refers to the amount of dead space vs. active illuminated areas.

>Brighter subpixels but less in number than the note. The tradeoff of this is of course burn in by your own admissions.

They don't need to be stressed, actually. Since there is less black area to compensate for. The more black space between sub pixels means each individual pixel needs to work harder to overcome it.
Look at the numbers that matter.  

the iPhone X is significantly less deviant than D65 white with a JNCD of 0.2, compared to the Note 8's 0.8.
It's possible because of the higher fill-rate, meaning more surface area that's illuminated.  

Displaymate says so themselves:

>On the iPhone X the resulting Sub-Pixel fill factor is much higher than other OLEDs, **which is a key factor in providing the much higher full Screen Peak Luminance of over 625 nits.**
>We don't know if the fill rate is better for the iPhone x or the note. You keep ignoring what I say and harping on this like its fact when it's clearly not.

Except we do know.  The pictures clearly show less black area by pixel area.  For every area of pixel, the Note 8 has more black area than the iPhone X. 

>How do you think the iphone achieves higher brightness with less pixel than the note if they're not jacking up the intensity of the subpixels? 

Because there's more illuminated surface area.   
>Regardless we were talking about burn in, so on a per pixel basis the iphone x is both pumping out higher nits at a lower efficiency than a comparable sub pixel on the note. This both increases the chance of burn in and decreases the power efficiency of the display.

We don't know this. 

Displaymate compared the two displays at equal average picture levels, but not brightness.
>If you have less pixels and achieve the same brightness you by definition have to have the pixels pump out more light per pixel.

Not if they cover more surface area than a display with less fill rate.

>Displaymate did not confirm anything.

The pictures speak for themselves.  There is less black area between sub pixels on the iPhone X.  That is a fact.

>"Other OLEDs" does not mean "all other OLEDs". This is you just substituting your opinion as fact with little basis.

This really reeks of denial.  

>Also pixels being oval shaped does nothing to show a difference of fill factor when the other picture is at a completely different magnification.

Yes, it does.  Given the same measurements of the horizontal and vertical middle, a rectangle will always have more area than an oval.  Had the green pixels in the Note 8 been given angular corners, the fill rate would've been better.

There is simply more empty space per pixel area on the Note 8 than the iPhone X.  Deal with it.
That's not entirely true. 

The only scenario in which the Note 8 achieves higher brightness in high ambient lighting is when it's displaying an image with an average picture level of 1%, which is extremely uncommon.  You're also not mentioning that the Note 8 has worse contrast in high ambient lighting.
According to displaymate, the iPhone X has a 0.2 JNCD from D65 white.  

At best, using the cinema mode color profile of the Note 8, it has a 0.8 JNCD from D65 White.

So, no, the S8 is not as accurate for whites.  Or for any other colors for that matter.
> Regardless fill factor doesn't change the fact that increased brightness speeds up LED degradation. I don't see how that bodes well for the iPhone.

True, increased brightness speeds up OLED degradation.  But the point is that sub pixels in a display that has less fill rate have to work harder to achieve the same brightness than a display with a higher sub pixel fill rate.
The Note also has physically larger display, of course it has more pixels.


>A higher sub pixel fill factor with less subpixels means a higher intensity of brightness per sub pixel, which is the exact opposite of what you want to reduce burn in.

Uhhh....no.  It means a higher brightness for the overall display.  The *individual pixels* don't have to work as hard to achieve the same brightness if there's less black area to compensate for.

>The photos are not at the same magnification and don't even show the same number of subpixels. For all we know the different shape of the subpixels can be due to stronger magnification because the note has 55% more pixels.

No.  Displaymate has confirmed the high sub pixel fill rate.  It's clear that things like the green pixels on the Note 8 are very much oval shaped, while the iPhone X's green pixels are much more rectangular.  Get over it.

>Again "other OLEDs" does not mean "All OLEDs" you're twisting things around and coming to conclusions that are not sustained by facts.

Despite Displaymate's intentions being obvious, the pictures do clearly show less black surface area for the iPhone X.  You're being delusional now.


>No, it's not. Stop making up stuff. Look at the efficiency scale. At same brigthness both devices have same efficiencies.

You're confusing average picture level (APL) with brightness (nit).

>Yes, excactly. Look at how much power the iPhone X uses at full brightness in manual mode. Despite being ~40% brighter, it uses much more power than this over the Note 8 at full manual brightness.

The iPhone X uses much more power than the Note 8 in the Displaymate tests because the iPhone X's power draw was measured at a much higher nit level.

Yes, the Note 8 can only achieve extremely high brightness in specific situations, but thatâ€™s not the issue here. Even in normal scenarios, the Note 8 is pushing its individual pixels harder to maintain the same full screen brightness as the iPhone X. This is due to the fact that the Note 8 display has less overall illuminable surface area. 

Burn-in can certainly be caused by static images, but whatâ€™s important is why. The reason static images cause noticeable burn-in is because OLED subpixels degrade at different rates. For example, blue degrades much more quickly than green, which is why blue is generally the largest of the subpixels in a typical PenTile layout.  When you have a static images, certain colors will degrade faster than the surrounding area, leaving an imprint behind. 

Heat speeds up this degradation, which is caused by the power used to illuminate the subpixel.  A display that rarely goes beyond 50% of its maximum manual brightness will have less burn in than your standard retail store display unit, which is generally always at 100% brightness. 

When each individual pixel needs more power to make up for the lack of illuminable surface area, then youâ€™ll see worse burn in. 
Read my above response to the other person. 
Burn in caused by uneven degradation of different subpixels. The intensity of the brightness of these subpixels furthers this process along the higher the intensity becomes. 

A higher subpixel fill factor means that more surface area of the display can be illuminated, which means each individual pixel doesnâ€™t need to get as intensely bright in order for the overall display to reach a certain nit level.   This allows for the iPhone X display to output the same brightness without straining each subpixel as hard, therefore reducing burn-in, at least in theory. 

From displaymate:

> A high resolution screen shot obtained with an optical microscope camera shows a Diamond shaped layout for the Sub-Pixels on the iPhone X. This Diamond Sub-Pixel layout is used on many OLED displays. **On the iPhone X the resulting Sub-Pixel fill factor is much higher than other OLEDs, which is a key factor in providing the much higher full Screen Peak Luminance of over 625 nits.**

These photos of the diamond pixel layout shows how much more surface area can be illuminated on the iPhone X than on the Note 8:

[iPhone X](http://www.displaymate.com/Diamond_41a.html)

[Note 8](http://www.displaymate.com/Diamond_40.html)

The subpixels of the iPhone X are much less rounded than those of the Note 8. The rounded corners cut off more area than more angular corners. 

> even if it did it would have to be about 55% better fill factor to make up for the density disadvantage it has against the note 8 anyway. 

As the photos demonstrate, thatâ€™s not how it works.  You can have more individual subpixels per inch but still have less illuminated surface area. 
Stop restoring the phone or rescanning your face. FaceID improves over time. 

When it fails to recognize, input the password. After a while, it should improve. 
Itâ€™s not entirely the same. The iPhone X display has a higher subpixel full rate than any other display samsung makes.  This will probably help a lot with the burn-in. 
> It has got to be really well-timed.

Of course. The beauty of not being on live TV. 
The pressure is not measured in the bezel of the phone.  Pressure is measured by electromagnetic sensors imbedded within the backlight reading distance changes between itself and your finger on the cover glass. 

Applying pressure to the edge of the phone still warps the cover glass enough to trigger a pressure reading. Though, as you can see for yourself, itâ€™s a bit harder than actually force pressing on the touchscreen itself. 
It could be a video for all we know. 
This needs to be brought up more. 

Too frequently people like to chime in on these threads with â€œX is too cool, Y is too warmâ€ without ever having a reference to make these judgments.  These photo comparisons should always have a control photo from a good DSLR. 
The UI being easier to use is just a result of familiarity. I personally canâ€™t stand using Spotifyâ€™s UI. 
How does this have upvotes?

Apple Music is available on Android via the Google Play Store, and available on PCs via iTunes. 
It just doesnâ€™t have as many songs and artists as Apple Music. 
Apple Music has all of those features. It also has a much bigger category of songs and artists. 
It is misleading. Thatâ€™s the point. 

OnePlus has done the same thing with the 5T. Using glare to make the bezels seem non-existent. But Samsung gets a free pass because r/Android is gonna r/Android. 
So, in other words, misleading and exactly what OnePlus is doing here?
Showing the phone from angles makes the bezels look smaller than they actually are. 

You should work for Samsung.  
Tell me about it.  I provide photographic evidence.  One picture where Samsung sneakily hide all black bezels to make it seem like itâ€™s completely edge to edge. The other picture of a real S8 shows noticeable black side bezels in real life. 

Denial all around. 
Identical?

I guess youâ€™re just ignoring how there practically zero black bezel in the first picture. 
Youâ€™re delusional, then. 


Itâ€™s significantly larger than the bezels they show in promotional images, which is near non-existent. 

Who exactly are you trying to fool?  Yourself?
[Dem bezels](https://phandroid.s3.amazonaws.com/wp-content/uploads/2017/04/galaxy-s8-first-things-1.png)
[THIS](https://phandroid.s3.amazonaws.com/wp-content/uploads/2017/04/galaxy-s8-first-things-1.png)

does not look like 

[THIS](https://drop.ndtv.com/TECH/product_database/images/329201783846PM_635_samsung_galaxy_s8.jpeg)

Get real.
[These do not look like the renders.](https://phandroid.s3.amazonaws.com/wp-content/uploads/2017/04/galaxy-s8-first-things-1.png)
[Pictures don't lie.](https://phandroid.s3.amazonaws.com/wp-content/uploads/2017/04/galaxy-s8-first-things-1.png)
You think that'll make the bezels any smaller?

Oh, this sub....
You think some Galaxy phones just have bezel variances?  

Denial.
Why?  The picture I gave is just fine.
[Ahem](http://www.samclones.com/wp-content/uploads/2017/01/DSC_0005-169x300.jpg)
Really?   That's your excuse?
Are you arguing against a photograph?
I'm with you in that his comment made no real sense, but what about it is bigotry?
No different than what Samsung or Xioami does.

- [What Samsung wants you to believe.](https://drop.ndtv.com/TECH/product_database/images/329201783846PM_635_samsung_galaxy_s8.jpeg)

- [What you get.](https://phandroid.s3.amazonaws.com/wp-content/uploads/2017/04/galaxy-s8-first-things-1.png)
If you're looking for "depth" then I would suggest you stay away from nearly every modern "AAA" game with their shallow game mechanics that are supposed to be propped up by contrived, cliched narratives done better a dozen times in other mediums.

If a game's not fun, then what's the point?
A lot of people *here*, sure. But if weâ€™re talking about the general public, definitely not. 
Female actors in commercials that are not bouncing their tits or working in the kitchen are therefore genderqueer tomboys to you?

Youâ€™re a weird guy. 
Is it?

I just saw an androgynous looking kid who needs to ride their bike places to get around. What exactly was a new gender portrayal in that commercial to you?
You missed the point entirely. 
The quote â€œwhatâ€™s a computerâ€ makes a lot of sense in this context. 

iPads and tablets in general are not really laptop replacements, rather, they meet the general computing needs of most people. 

A tablet doesnâ€™t need to be able to replicate every single use case scenario of a laptop to be a â€œlaptop replacement.â€  It just has to fulfill the needs of most people.  Word processing, media consumption, emails, social media, a great camera, gaming â€“ what more do most people need?
Their appearance is androgynous, if thatâ€™s what you mean. 
Reported for what?  Making you look bad?
There are ways to both be responsible to shareholders and maintain integrity as game developers and artists.  Just look at Nintendo and their corporate philosophy and development ethos. 


Unlike the PixelBuds, putting the AirPods back into its case isnâ€™t a hassle. 
A gimmick not relevant to the lives of most people. And even for the people that might use it, itâ€™s half baked implementation at best.   Itâ€™s much easier to have real time translation done via speaker phone for both parties, rather than the one-sided affair the PixelBuds are offering. 
When the AirPods are actually wireless and have in-ear detection sensors, itâ€™s a little ridiculous that Google thinks they can charge the same price for them. 
What do you think gambling should be legally defined as?
Except when they explicitly stated on-stage that FaceIDâ€™s security is less with people that have close genetic relationships, like identical twins. 
Market share is nowhere near as important than total revenues for a company like Apple. 
They â€œhackedâ€ FaceID probably by training it by inputting the PIN after several failed attempts. 

And by probably, I mean definitely. 
Yeah, and Snapchat further demonstrates why itâ€™s better on iOS due to the new iPhone X enhanced Snapchat filters. The new filters look a lot better than the standard filter. 

Is that worth $1,000 by itself?  No. But itâ€™s an improvement that is noticeable and people really like. 

I would say that the best feature of the iPhone X is the fact that itâ€™s just premium all around. Best SoC, best display, and top tier camera. 

Animoji are a lot more sophisticated than any face tracking applications on the market without dedicated 3D sensing equipment. 

What makes Animoji so interesting to a lot of people is just how life-like it is. Yeah, I know, talking poop doesnâ€™t exactly scream â€œrealistic,â€ but the way Animoji tracks the various muscle groups in your face at nearly 1:1 makes everything feel so natural.   

This kind of stuff isnâ€™t really possible with just a standard camera. 
TIL: Worlds that are based around a dystopian future where robot fauna roam the wilds is â€œrealistic.â€
â€œThe world is realistic.â€

Robot dinosaurs are part of that worldâ€™s setting. 
Robot dinosaurs are still part of the setting, narrative, and visuals. 
Robot dinosaurs are realistic?
> At WIRED's suggestion, Malik asked his wife to re-register her face to see what would happen. After Sherwani freshly programmed her face into the phone, it no longer allowed Ammar access. To further test it, Sherwani tried registering her face again a few hours later, to replicate the indoor, nighttime lighting conditions in which she first set up her iPhone X. The problem returned; Ammar unlocked the phone on his third try this time. It worked again on his sixth try. At that point, Malik says, the phone's AI seemed to learn Ammar's features, and he could consistently unlock it again and again.

Figured. 

FaceID trains itself after failed attempts, but only when a PIN is provided after the rejection.  But if youâ€™re providing someone with your PIN in the first place, you arenâ€™t concerned with security anyway. 

An example of this can be found [here.](http://bgr.com/2017/11/06/iphone-x-face-id-brothers-fail-explanation/). This explains why most â€œFaceID failedâ€ stories are probably misleading. 
$160 and itâ€™s not even completely wireless? Hahaha. 
This needs to be publicly confirmed. 

For all we know, this security firm is taking advantage of a certain FaceID feature where faces that fail the recognition, but are past a high enough resemblance threshold, are rescanned after inputting the PIN and FaceID is trained to accept the new face.  Under this situation, itâ€™s possible that the 3D printed mask failed the first verification, but later broke through on subsequent attempts after inputting the PIN numerous times. 

FaceID does this in order to learn and adapt to small changes in the userâ€™s face.  

Details about how this works can be found [here.](http://bgr.com/2017/11/06/iphone-x-face-id-brothers-fail-explanation/)
â€œScarves will work so long as itâ€™s not covering up your nose and mouth.â€

â€œBut what if it does?â€

â€œThen pull it down a tiny bit for the scanning.â€

â€œYou donâ€™t know what youâ€™re talking about!!!â€


???
Then you quickly pull it down a bit. Jesus fucking Christ. 
Most sunglasses will work with FaceID. Scarves will work so long as it isnâ€™t covering up your mouth and nose. 
> The essay is about non binary pronouns, it's literally in the title. Are you satire? 

Yeah, and it covers *some* historical attempts at making non-binary pronouns. â€œThey/themâ€ as pronouns for specific, known individuals isnâ€™t anywhere on that historical list you fucking gob.  So, yeah, you didnâ€™t read it. 

Ok, so you are that intellectually dishonest. Or maybe youâ€™re just not smart enough to understand the differences between how singular *they* is used. 

I acknowledged from the very beginning that singular *they* is valid.  Using singular *they* when referring to a hypothetical or generic subject with indeterminate gender has a rich history in the English language. 

**However**, using singular *they* to refer to a specific individual that is known and not indeterminate **does not** have a rich history, as evidenced by the ADS and the essay you provided.  Such usage in this manner is a recent invention. 

I donâ€™t know whether youâ€™re resorting to deliberate equivocation, or youâ€™re just not smart enough to understand the nuance here.  So, intellectually dishonest, or a fucking idiot?  Which one are you?
Itâ€™s like you didnâ€™t even read the essay. 

The essay further reinforces that notion that using singular they to refer to specific individuals that are known is a recent invention. All historical uses of â€œtheyâ€ in that essay were for generic subjects or subjects that were indeterminate. 

So, still looking for that rich history of singular they used for specific, known individuals. 
How intellectually dishonest are you?  Two sentences after your quote leads directly into mine, in that *the usage you are referring to* is a recent invention.  Your doing a bit of equivocation here, probably purposeful. 

Your quote is in reference to the generic usage of â€œtheyâ€, which is for hypothetical subjects or subjects with an indeterminate gender. 

> If you bothered to actually do any research

Oh shove it. If **you** bothered to do any research, you wouldâ€™ve read that voting was done â€œjust for funâ€ by a bunch of people, which included â€œstudentsâ€ and a bunch of random people we donâ€™t know the credentials of.

So, still waiting on that rich history. 
> While editors have increasingly moved to accepting singular they when used in a generic fashion, voters in the Word of the Year proceedings singled out its newer usage as an identifier for someone who may identify as â€œnon-binaryâ€ in gender terms.

From your link.  Using â€œtheyâ€ to refer to known individual whose gender is not indeterminate is a recent invention.  Hardly â€œrich history.â€

> Society is using the singular "they" en masse, and as a result it won word of the year. 

You mean 187 people from ADS voted for it?  As if they speak for society at large and are not just pushing a political agenda?

So whereâ€™s this rich history? 

Your link specifically states that there isnâ€™t any rich history of singular *they* for a specific individual with a known gender, that itâ€™s usage in such a way is a new thing. 

Furthermore, if you think the ADS has any significant impact or authority on American society, youâ€™re dead wrong. 

So which is it?  Is there a rich history or not?
> There is indeed a rich history of the gender neutral they, it has been used for centuries in a variety of ways **including referring to people of known gender.**

[Citation needed]

>You can scream in the corner about how the random strangers you don't know should gender themselves the way you want them to, but the rest of us are moving on.

Sorry, the real world isn't an SRS echo chamber. 

Edit:  Hey, maybe if you downvote me more, you'll be right.
>No, the reason that gender neutral "they" is becoming more common is because the public acknowledgement of non binary genders is relatively new.

Public acknowledgement?  You mean a fringe group of cultural marxists and post-modernists?

>There is no valid argument that the singular they is incorrect as there is a rich history of it being used to describe primarily people of unknown gender but **also people of known gender too.**

Bullshit.  Historic usages of singular *they* pronouns are almost entirely limited to hypothetical subjects or subjects of indeterminate gender.  The usage you are referring to are almost entirely inventions made within the last decade by fringe groups of society.  There is certainly no "rich history" of *they* being used in the way you are referring to.

> "B-b-but there's technically precedent!!"

Yeah, probably.  There's technically precedent from an area schizophrenic, who goes by the name "Skaggs," of referring to people as shitfucks and tittysprinkles.  That doesn't mean it matters or is valid.

The fact that a few silly people think there's more than two genders and that "non-binary" isn't something to instantly roll your eyes at, doesn't mean it's valid.
There is no *significant* precedence for it.  It is *typically* used to address hypothetical subjects or subjects with an indeterminate gender.

Again, there is technically precedent for anything.  What matters is when a critical mass of the population both understands and uses certain linguistic tools in such a manner and number that is becomes accepted and "valid."

Singular "they" for known subjects has never had this kind of precedent.  Historically it has been restricted to hypotheticals and subjects of indeterminate gender.

It's not moving the goalposts, its just you being manipulatively obtuse.
There's a precedent for practically everything.  That doesn't it's therefore appropriate.
There is precedent for gender neutral pronouns in referring to unknown parties or subjects where the gender is indeterminate.  

There isn't much precedent for the singular "they" in the context you are referring to.
From Wikipedia:

> Singular they is the use in English of the pronoun they or its inflected or derivative forms, them, their, theirs, and themselves (or themself), as an epicene (gender-neutral) singular pronoun. **It typically occurs with an antecedent of indeterminate gender**

Where gender or the actual identity of the subject is unknown, singular "they" pronouns are typically used.  Referring to known people with known gender, directly, with "they/them" pronouns is a trend that has only recently gotten traction.
Gender neutral â€œtheyâ€ does have a grammatical purpose, but only for referring to subjects who are hypothetical or whose gender is unknown. 

It does not apply to known people we are directly referring to. 
> I've proven every point you've brought up

lmao
>So you continue to ignore that the Note 8 has a better resoultion and PPI which are probably the two most important factors in getting a good picture from the phone.

To quote Displaymate:

*The display has Diamond Sub-Pixels (see below) and Sub-Pixel Rendering with 458 pixels per inch (ppi), providing significantly higher image sharpness than can be resolved with normal 20/20 Vision at the typical viewing distances of 12 inches or more for Smartphones, so the display appears perfectly sharp. As a result, for Smartphones it is absolutely pointless to further increase the display resolution and pixels per inch (ppi) up to 4K (3940x2160 pixels) for a silly marketing wild goose chase into the stratosphere, with no visual benefit for humans!*

In other words, unless you're using VR (which no one does), you won't see that much more clarity bumping up the PPI beyond 450.  Since we've more or less plateaued in the resolution department for most high end displays, things like color accuracy, color gamut, HDR, brightness, and contrast are much more important when it comes to overall picture quality.  The Note 8 display looks like shit in the sunlight, while the iPhone X still looks bright and contrasty. 

>People run benchmarks while Android phones are running the same benchmark in twice the resolution the iPhone is capable of and wonder why it scores higher.

For benchmarks like Geekbench, resolution has literally no effect on the scores.  What affects results in benchmark tests like Geekbench are the CPU, so higher resolution displays will have little effect.  Thanks for showing us you have no idea what you're talking about.

>Further more A11 Bionic only works well in Apple's ecosystem, that'd probably chug running Android. 

Considering that top of the line Androids are just *barely* hitting single core scores iPhones were hitting back in 2015, and the A11 is king in multicore scores, I think not.  Benchmark scores are platform independent you silly goose.

> There is a reason Android is the industry standard

Yeah, poor people.

Except you specifically stated â€œweaker tech.â€ 

The iPhone X has indisputably the most powerful SoC, and the highest quality display on the market. 
> The Note 8 can also reach 850 nits easily, so do your research instead of just relying on a single site. You realize the AVERAGE isn't the MAXIMUM. We're not talking about the average because if needed I can get the Note 8 to go brighter than an iPhone X whenever the time comes.

Incorrect.

According to [Displaymate's laboratory analysis](http://www.displaymate.com/Galaxy_Note8_ShootOut_100.htm) of the Note 8:

- Peak home screen brightness is 601nits (compared to iPhone X at 728).
- 525nits at 50% average picture level, which is typical for most screen content and average usage scenarios. (Compared to iPhone X at 700nits)
- 728 nits at peak brightness with 1% average picture level.  (Compared to iPhone X at 801nits)

The only scenario in which the Note 8 can achieve higher brightness is during high ambient lighting conditions with auto-brightness on and only with 1% average picture level.  This means the Note 8 can only light up very small portions of the displays brightly outdoors.  In 99% of conditions, the iPhone X is substantially brighter according to Displaymate's objective, scientific data.

And here are more areas the iPhone X display is better than the Note 8:

- Absolute color accuracy
- Full screen contrast in high ambient light (Note 8 looks washed out in sunlight).
- Brightness variance with viewing angle.
- Screen reflectance.

*"Based on our extensive lab tests and measurements the iPhone X becomes the Best Performing Smartphone Display that we have ever tested, earning DisplayMateâ€™s highest ever A+ grade. The iPhone X is an impressive display with close to Text Book Perfect Calibration and Performance."*

>So you can continue circle jerking with displaymate whom I've never heard of before while people who look at the raw specs and work with these phones daily (myself) can see the difference in real life.

Displaymate is the leading display testing laboratory, and every display manufacturer refers to their data.  They know more than you.

Also, please tell me a single SoC in the mobile industry that's better than Apple's A11 Bionic.  Good luck.
> iPhones are customized from the outside with flashy cases and are for people with simplistic minds (much like the sheep 99% of iPhone fan boys are)

r/iamverysmart

r/cringe
According to displaymate, the iPhone X display is actually significantly brighter, reaching 623 bits for average picture levels. The Note 8 display averages around 560nits, and can only achieve 1200 bits when only 1% of the display is active. 

Which is why displaymate crowned the X display as the best display they have ever tested, beating out the Note 8. 

So, please, answer the question. What phone on the market has a better display, and a better SoC?
We also donâ€™t know the setup process this firm used. 

For all we know, the mask itself couldnâ€™t unlock FaceID by itself, and needed to have trained FaceID by having the PIN entered after a few times of failed authentication. 
By inputting the PIN after a failed recognition, the FaceID trains itself to the new face. 
When you have to straw man your way out of an argument, I guess itâ€™s probably best you bowed out anyway. 

Good riddance I guess. 
Have you ever inputted your fatherâ€™s PIN into the iPhone X?
Except that Apple has the most powerful SoC, by a wide margin, and has had for the last couple of years. 

Are you just a blind fanboy who just doesnâ€™t like Apple?
> Well... I could argue with you about the speed and the user experience of some android phones. 

Not really. iPhones and some flagship android phones are noticeably smoother and less stuttery then your average $200 phone. Especially after a couple of months.  If Iâ€™m using this device every day, I would prefer using the phone to be as smooth and stable as possible. 

Maybe youâ€™re just used to jank, so you donâ€™t notice it. 

> Many mid-rage devices are smooth enough to make you forget about those problems you talk about. 

Smooth enough? According to whom? 

â€œSmooth enoughâ€ to someone who has only experienced junk phones is going to be a lot different than â€œsmooth enoughâ€ to someone who is used to using iPhones. 

>Talking about the "security" is pointless because, in the end, the security of the phone rests in the hands of the user. Being mindless will result in a security breach having the best iphone or the crappiest android phone. 

Oh please. Do $200 android phones even have full hardware encryption?

> The "app experience" is something very subjective. There will be some "exclusive" apps that you won't find in android, but how many? Honestly, I don't see your point there. 

Itâ€™s not really that subjective. Besides maybe Reddit clients, iOS has developer priority when it comes to apps. iOS gets big name developer apps first, and the apps themselves are generally much more polished than the stuff you see on android. 

You also have the overall Apple ecosystem, how various Apple devices play with each other. Things like iMessage sync with iPads and Macs.  Or Airplay. 

> You talk about the lifespan of the phone. And yeah, probably it won't last as long but, most of the times, the lifespan of a phone is determined by the battery. 

Maybe for the junk youâ€™re used to buying. But my old iPhone 6 that I gave to my mom still has about 90-93% battery capacity. 
Too much to justify?  According to whom?

Being that these devices are in use for a big portion of our days, everyday, having that experience be secure, smooth, and fast is important to a lot of people. Your average $200 dollar android phone is not going to be as fast and smooth as your average iPhone, and definitely wonâ€™t have as great of an app experience and definitely wonâ€™t be lasting as long. 

If, for example, someone loves using an app like Snapchat, buying an iPhone seems like a no brainer. 


Name a single phone that has a better SoC and display. 
Really now?

The iPhone X has the best SoC on the market by a wide margin, the best display according to displaymate, and the best app ecosystem. 
Its just quality of life things that make it worth it for some people. Premium phones like iPhones, and to some extent, flagship android phones do little things that you donâ€™t get with junky, cheap phones. 

Not having using your phone be a chore to use is one thing. Cheap, junky phones slow down after a while. Things stop working right, start lagging, freezing.  Those donâ€™t really happen with new iPhones.  The last batch of iPhones to really start showing issues with new updates was the 4s. 


So which is it?  Android will finally be getting specs and performance with the new Snapdragons?  Or, Appleâ€™s chips are better but who cares because who needs all that power?

You canâ€™t have both. 
**$849**

And you call that *fine*?
You honestly think the 845 is going to come close to the A11 Bionic?
Itâ€™s secure for 99.997% of the population that donâ€™t have evil twins. 

And before you cite the YouTube videos of non-identical brothers breaking in, you should know that if FaceID fails, but is then provided a password immediately, the other brotherâ€™s facial data is used to train FaceID, allowing for a subsequent match. 
Definitely Breath of the Wild. 

Odyssey is fantastic, and definitely second place for GotY for me, but Breath of the Wild was a much more ambitious, experimental game from Nintendo, which I really appreciated. It wasnâ€™t an easy thing to take such a radically different approach to one of their most important properties. 

It also offered a much better world to explore and survive in, which I like more than the fun, action adventure platforming of Odyssey. 
Both statements are true.
Yeah, the colors on displays are definitely accurate in most lighting conditions, except direct sunlight, but not exactly *natural* looking.  Itâ€™s a bit disorienting to be blasted with cool bright light from a displayâ€™s white color point when Iâ€™m in a room with extremely soft white lighting. 

Appleâ€™s all about making technology disappear to the perspective of the average user, so I guess thatâ€™s why they went with the â€œpaperâ€ look. 
I guess it affects accuracy in the way that had the screen been a piece of paper, the colors you are seeing with TrueTone are accurate. 
From Apple devices, itâ€™s gone. TouchID is dead. 
No, you're being condescending.  

The swipe up to go home is intuitive, and feels great.  And users are taught to do it in the setup process.  It's not a completely different paradigm, it just replaces the functions of already existing functions.  

Multitaksing and swiping between active apps isn't new, it's just done using a different gesture now.  In a few days, it should be second nature to just about anyone.
Again, if you think a transition to a new gesture is going to plummet productivity, you are being condescending.
If you're trying to make the impression that you aren't being completely fucking condescending to the general public by insinuating they're too stupid to learn basic gestures, you're failing.  Miserably.
You like big colors wasting space and tacky drop shadows from the 2000s better?
The absolute condescension in here is palpable.  

Do the out of touch nerds of r/android really look that disgustingly down at the general public to assume they can't learn basic gestures that are about as complicated as "swipe up to go home?"
Swiping up is so hard, amirite?
What an argument. 
Trust me, the United States is in peace time.  Even during our darkest hours we were never Japan during the signing of the surrender. 
> The point of Nanking - The Japanese are not some perfect culture out of nowhere. Germany, Spain, Italy and others have all had violence and crazyness at some point.

Well, again, it's complicated.  Do we differentiate between military and civilian culture?  Obviously there are differences?  But is that fair?  That's a good question.  *It's complicated.*

If we agree that Japanese civilian culture should be judged for their military culture, fine.  So what do we see?  Japanese "culture" has shifted dramatically.  But again, it wasn't because of their own laws.  It's was because of the complete, emasculating, and humiliating surrender the Americans forced upon them after burning Tokyo to the ground and unleashing two nuclear weapons on them.  Their beaurcpatic culture was forcibly changed by force of war, and their government was designed by outside forces.  And then you have decades of military occupation to further meddle in things.

Point is, just saying "they can change their laws to change their culture" is incredibly naive, idealistic, ignorant, and just plain silly.  

>The person I first responded to is trying to say "these guys are not inherently evil like Americans". This is obviously bullshit and weirdly racist? culturalist? The point in my reply is that you could pick and choose from literally any country and find the same results.

Except that you can't because the whole point is that the United States is *unique* in it's violence. The Swiss are about as gun-loving as Americans, with an extremely high firearm prevalence rate despite the tiny population, yet there gun violence statistic are exponentially lower than ours.  Do their stricter regulations on gun/ammo purchase, storage have something to do with this?  Absolutely.  But more importantly are the aspects of social conditioning and overall national temperament.

So, the point is, it's not just our gun laws that cause this overwhelming violence.  Sure, it's a factor.  But America simply has a more violent and aggressive culture that causes these things to happen while a place like Switzerland does not.  Can you change laws to help curb the violence?  Probably.  But deep, imbedded cultural shifts don't happen in a generation in peace times.
Apple announced a Q4 2017 revenue of $52.6b: 46.7m iPhones, 10.3m iPads, 5.4m Macs.  Apple does not provide specific breakdowns for these figures, so you'll just have to guess.  

In my opinion, the majority of the 46.7m iPhones were probably NOT the iPhone 8, and mostly an amalgamation of iPhone 7/Plus and SE sales, maybe some 6s, in that order.

Your figures are interesting, I wonder if those are sustained figures, or just a peak high on a specific day.  As Samsung's quarterly reports do not suggest anything like that.
> Boy do I have a Nanking to sell you! They designed their culture through law, just as all the 1st world nations did, and America could one day join modernity as well.

What kind of 4th grade, revisionist history lesson is this?

"They", being the United States, forced an emasculating cultural shift as one of the conditions of their surrender, which included forcing an entirely Allied Powers drafted constitution on them.  Not to mention decades of military occupation and oversight.  Other cultural forces, such as their cultural traditions and social dynamics, such as the emphasis on group dynamics over individualism, which descend from shintoism, confucianism, and buddhism, are also important factors that play into this which don't apply to the US.

>On top of that, your Spanish/Denmark example isn't much different from Alabama/California comparisons. There is a shared history, law, culture, and goals. 

Alabama and California have a lot more in common than Denmark and Spain.

>They choose to foster that situation though, they could easily fix it if not for scapegoats like 'cultural differences'.

They chose to foster that?  Broad societal cultural dynamics are a bit more complicated than that.

>Don't over nit-pick shit.

Complicated issues demand complicated analysis.  Look, you may not be up for it, but the adults are talking now.  If it's too confusing, I'm sure r/politics would love to have you.
>His example is that Japan's culture somehow makes them less violent than America.

Yes, I agree.  Their culture creates an environment where violent crime is relatively uncommon.

>Can you name a culture closer related to the US than Canada or many in the EU?

Just because they're close doesn't mean they're the same.  This nuance is important when discussing extremely complicated sociological issues like culture and gun violence.  

>Do you not realize how dumb it is to say "there is no such thing as European culture"?

It is stupid.  Do Spanish people have the same values, attitudes towards certain things, and culture as Denmark?  And which Spanish people?  Catalonians?  Basques?  

The only thing fucking stupid here is an insistence that we can make broad sweeping cultural generalizations to explain highly complex social issues.  

>Are you arguing that American's are somehow inherently violent and evil compared to the rest of the world?

Yes.

"Evil" may be a bit of purposeful, manipulative hyperbole on your part.  But otherwise, yes, various aspects of American culture do seem to uniquely create an environment that fosters violence and aggression relatively more than other cultures.
That's all fine and good.  But think of this:

This quarter is only counting sales from iPhone 6s/Plus, 7/Plus, 8/Plus, SE.  Apple doesn't officially sell the regular 6/Plus anymore and the X wasn't counted in these quarterly reports.

Samsung, on the other hand, offers: S7/+, S8/+, Note 8, S8 Active, Galaxy J7/Prime, Galaxy J3, Galaxy Sol 2, Galaxy Amp Prime 2, Galaxy Halo, and probably a bunch more that are counted towards this quarter's report. 

Even with all these other models, most of which are extremely cheap and make up the bulk of Samsung's shipments, Samsung still can't match Apple's sales of practically solely premium phones.
Translation:  I said something stupid, don't make me explain!
For clarification, Apple's fiscal quarter reporting for last quarter covers up until September 30th.  Which means this is not including sales for iPhone X.
iPhones now have the best display, best SoCs by FAR, and best camera, if not rivaling top Android cameras. 

What else can Android fans argue with?
A magazine's opinion?  You mean the leading display testing laboratory that does a full technical analysis of every metric of display quality?

And what about geek bench?  Did you just forget about that?

Pathetic.
[https://www.androidauthority.com/iphone-x-galaxy-note-8-best-smartphone-display-812948](https://www.androidauthority.com/iphone-x-galaxy-note-8-best-smartphone-display-812948)

[https://browser.geekbench.com/v4/cpu/4773963](https://browser.geekbench.com/v4/cpu/4773963)

You can shut the fuck up, now.
Culture in Canada and Europe is the same as the United States?

Let's just ignore the fact that there's no such thing as "European culture."
Yeah, you could do a FaceID-ish solution with a single regular camera by just having the user tilt their head from side to side before unlocking, as that would give it the necessary depth data. 

You just wonâ€™t be able to do it in the dark with IR projectors, and itâ€™s obviously nowhere near as seamless if you gotta move your head. 
> No, stupid would be assuming that just because it's in the media, and it's being reported, that it must be true. 

For something as trivial as this, no, not really. Donâ€™t be ridiculous.  Samsungâ€™s iris scanner, as well as iris scanners in general can be spoofed using the methods I described above. Get over it. 

> The media is reporting on Face ID spoofing too, so according to your own reasoning, it must be true. Slow down and stop shooting yourself in the foot for just one second, please. 

I know you typically donâ€™t appreciate nuance in these things, but letâ€™s just try it for a second. 

The media has gone over, reproduced the techniques, and confirmed the iris scanning hack.  No mainstream media outlet has reproduced, gone over, and reported on the non-identical twins spoofing, all we have is YouTube videos. 

So thereâ€™s a difference. Crazy, huh?

> Fact is, no one has proved what you are claiming. It's not solipsistic at all, I'm merely adhering to the standard that **you** laid forth, not me. That "spoofing" just like the Face ID spoofing videos lacks transparancy. So your argument now takes on another fallacy: Special Pleading. 

I said wait for more information before making a judgement. News reports from respected mainstream media outlets that go over the techniques, reproduce them, and confirm their viability is that â€œextra information.â€    If youâ€™re going to continue being purposefully obtuse and demanding for more â€œskepticismâ€ you really will end up in dumbassery with solipsism. 

So no, itâ€™s not special pleading. Itâ€™s just not being an idiot.  Oh, and being called an idiot isnâ€™t inherently a logical fallacy either.  Just so you know when you go back to google searching for what logical fallacies there are. 

When you have various news agencies reporting and confirming the story, I think we have good reason to trust the iris scanning spoofing. 

Otherwise it seems the logical progression of your take on â€œskepticismâ€ is going into full blown solipsism.  Not even you would want to do something so stupid.  At least I would hope. 

Also, itâ€™s been shown that iris, and even fingerprint scanning can be [spoofed](https://www.scmagazineuk.com/starbugs-in-your-eyes-german-hacker-spoofs-iris-recognition/article/535281/) using high resolution pictures of faces and hands found on social media or google images.  After all, thereâ€™s no real difference between you taking a picture of someone elseâ€™s face, and someone else doing it and posting it to social media. 
Bahahahahaha!

Oh, bless your heart.  You must be a green bubble. 
The only person who claimed the â€œ100,000â€ figure was the Princeton Identity employee, but that was for the technology in general, not Samsungâ€™s specific implementation of it. These things can lower their specifications in order to mass produce it or to miniaturize it.   

Also, you donâ€™t need to take a medium range photo of someone without them noticing. You can just find any decent face picture of them from social media and apply an IR color filter to it.  So long as you use a decent laser printer, it should work.  I bet there are more people with decent head on picture on social media than there are people who have identical twins. 
[Bullshit](https://www.macrumors.com/2017/10/13/ios-gained-market-share-summer/)
Samsung has never actually given their false positive rate for their specific implementation of their iris scanner. 

And you donâ€™t actually need to make a fake lenses. You just print out the IR photo and plop some cheap disposable contacts onto where the irises are. Thatâ€™s it. 
Again, FaceIDâ€™s FPR of 1:1,000,000 is already pretty stellar. Coupled with the fact that fooling a 3D depth scan is a lot tougher than fooling an iris scanner, and youâ€™ve got the best biometrics on a consumer phone. 

Given Appleâ€™s security white papers and information we already know about 3D depth mapping techniques, technologies like FaceID have the lowest FPRs out of all the consumer biometrics. 
There are no videos showing it easily tricked for non-twins.   The videos you are referring to could have been edited, or could have been a result of the PIN issue. In which case, itâ€™s not being tricked, itâ€™s working as intended.  So those videos arenâ€™t worth anything. 

The normal false positive rate for FaceID is 1:1,000,000. Thatâ€™s higher than other false positive rates we do know, like for fingerprint sensors, which is around 1:50,000. As far as consumer biometrics go, there isnâ€™t anything with a FPR as low as FaceID, and no easy way to spoof the technology using photos or masks, unlike iris scanning. 
And those videos donâ€™t prove anything on their own. 

Iâ€™ve already demonstrated that two people who arenâ€™t identical twins can â€œfoolâ€ FaceID, but only when one of those people knows the PIN number.  

You want your argument to have any merit? Show an uninterrupted sequence of a fresh FaceID enrollee this, and then some guy just walking up and unlocking it. 
Iâ€™ve already said that we should withhold judgement on the validity of those videos. What exactly are you arguing?
So for 99.997% of the population, FaceID will be the most secure biometrics platform theyâ€™ll be using. 

Great to know. 
You also made the grand claim that brothers and unrelated people have broken the system. However, we canâ€™t know if those examples were just a result of the PIN issue. 

So, no, your claims havenâ€™t been supported. 
You never said anything about real sense. Windows Hello is a platform, not a technology. 

> Again, Face ID has been fooled. We've covered this, so try to keep up. Mashable demonstrated this well and showed their methodology. Fooled on day one, trivially, no questions asked.

With identical twins. 

So, in other words, not a problem for 99.997% of people. Got it. 


> I don't need evidence to say it was fooled. I only need enough to cast doubt on your bold assertion that it is the most secure form of biometric security.

You made a claim that FaceID was fooled, you didnâ€™t provide evidence, so we can throw that statement out then. 


> So far Windows Hello not fooled by twins & Face ID fooled by twins. They are currently the facts as we know them. I get they don't align with the narrative you are trying to push here.. what a shame.

[https://youtu.be/VY8gGlkyOgY](https://youtu.be/VY8gGlkyOgY)

[https://youtu.be/C5u0Ivv3R20](https://youtu.be/C5u0Ivv3R20)

You were saying?
So, in conclusion, you have no evidence to say it was definitely fooled, and twins are not an issue for 99.997% of people. 

Got it. 

Also, we donâ€™t know if Windows Hello canâ€™t be fooled with twins. The only â€œstudiesâ€ Iâ€™ve seen on the matter were conducted by The Australian, and they only used three sets of twins. Hardly a reliable sample.  Business Insider ran a twin test that FaceID was able to win. 
We donâ€™t know if siblings or others can get right in. Itâ€™s highly likely their faces were registered beforehand after being given the PIN. 

So, for 99.997% of people, the twin thing is a non-issue. 
> This is **my** point. You are confused, and you ask *me* if I'm dizzy? Christ, the irony.

YOU said it failed. When thereâ€™s a very high chance that it was doing exactly what it was designed to do, adjust to faces on the condition of a PIN.  If you were actually skeptical, you wouldnâ€™t be arguing this. 

> Besides, twins can clearly fool it anyway. That much isn't up for debate. The same can't be said of Windows Hello, nor Iris scanning. 

For 99.997% of the population, the twin issue is irrelevant.  And is this the same Samsung iris scanning that can be fooled with a picture and a contact lens?
> It's called skepticism. On what basis do we just assume that one rule holds true for all. That would be foolish. Again, you can't demonstrate otherwise, and don't expect me to take your word for it.


Actual skepticism would be to withhold judgement until more information arises. You just donâ€™t want FaceID to work for some bizarre attachment to Android. 

> Consent can be withdrawn. Situations change. Relationships change. 

If you give someone your password to your phone, whatever strength of the biometric security you have becomes irrelevant. If you decide to withdraw consent, then just reenroll your face into FaceID when you change your PIN.  Also, FaceID eventually throws this added facial data away overtime, to prevent exactly this kind of issue. 

This is some serious spin to turn FaceIDâ€™s adaptive nature into a negative. Impressive. Are you dizzy?
> No. Thinking harder would be understanding that one example doesn't apply to all. In that *one* particular example you might have a case. But not others.

With such limited information about what happened prior to those videos, on what basis have you concluded Iï¸ donâ€™t have a case?  My explanation seems perfectly plausible for most scenarios. 

> Yeah, having my biometric security learn other people's faces without my consent sounds ideal! When I change my PIN they still have access. What could go wrong? 

Without consent? Youâ€™re giving them your fucking PIN code.  It stops becoming a security issue when you start giving people your PIN code. 

You are trying way too hard. 
> You don't know that it wasn't.

And you donâ€™t know that it was. 

> even if what you suggested was the case, and I'm not agreeing it Is

â€œEven if?â€  You mean the video evidence and article I provided to you that directly shows it was the case?

>it's still a massive fundamental security flaw to have your biometric system start learning someone else's face. You would think it would be smart enough to tell the difference.

How is it a massive fundamental security flaw for the system to adapt to changes only under the conditions that you know the PIN to begin with?  It stops becoming a security issue when a PIN is required. Iï¸ mean, if the intruder has your PIN already, FaceID, or any biometric phone security becomes irrelevant. 

Iï¸ know you really donâ€™t want Appleâ€™s FaceID to be a success, but you need to think harder before replying next time. 
> According to whom?

Identical twins are 3.5 in every 1000 births.  For 99.997% of then population, the evil twin problem is not an issue and FaceID should work fine. 

> I'm sure it looks impressive, [when it works](https://qz.com/1121324/apples-faceid-technology-doesnt-wor

Oh please. Youâ€™re acting as if fingerprint scanners work flawlessly. I think we all have experienced having to reorient our fingers or wiping the scanner to get it to work a second or third time. 

> but in practice it was beaten on the first day it went live. 

Beaten by what?  Identical twins that arenâ€™t an issue for 99.997% of the population?  Or brothers we have no background info on?  How do you know it was beaten? And not just a result of the other person knowing their PIN and training FaceID to recognize them?
Yes, FaceID is the most secure consumer grade biometric system for people without evil twins. Which is about 99.997% of the population. 

For all these â€œbrotherâ€ videos, we donâ€™t have any information about the setup of their FaceID. So you donâ€™t have evidence to say it â€œfailed.â€  There exists a very plausible explanation for these videos. 

If the wrong person tries to open FaceID and inputs the password after it fails, then FaceID will train itself to add the differences to the original security profile. 
You mean the identical twins?  

Yeah, Apple stated during their presentation that evil twins may be able to get past FaceID. However, Business Insiderâ€™s identical twin test demonstrated that FaceID can differentiate between identical twins sometimes. 
Probably the same explanation.  
That and the best mobile display and processor, iMessage, and iOS in general. 
Boo-urns. 
[Update on the unidentical brother situation.](http://bgr.com/2017/11/06/iphone-x-face-id-brothers-fail-explanation/)

FaceID did not actually fail with the brothers. What happened was that the brother inputted the passcode directly after FaceID rejected him. When this happens, FaceID will assume you are the real user and just changed your appearance, so it will adjust and add the changes to your security profile. 

So, in other words, it worked exactly as intended. This shouldnâ€™t be an issue so long as your brother does not have your PIN. 
Yeah, the article I posted...


All biometric security can be fooled.  

That doesn't mean FaceID still isn't the most secure biometric platform on the planet.
No wonder this craphole isn't a default sub anymore.
That's not entirely true.  

It was Samsung manufactured, but according to Apple's specifications.  You can see this in the small differences, like the iPhone X's unusually high sub-pixel fill rate.  Which essentially makes the screens less susceptible to burn-in and degradation at higher brightness.
So who's talking about VR now?  

Oh, just the nerds on niche subreddits.
Sounds like you don't know what average picture level actually is.

Average pixel level refers to the percentage of sub pixels that are lit up compared to a full white display.  So, if you have a display that is completely blue, you'd have a 33% apl.  

Very rarely will you ever get to 1% APL.  Very rarely will you ever get to below 50%.  The average time browsing your phone should be around 80%.
Darker videos will actually be much better on the X due to the fact that it doesn't get washed out.  Contrast dips dramatically for the Note 8.

Look at any of the two phones in bright sunlight and come back here are tell me with a  straight face the Note 8 is better.
Did you even read the article?

The iPhone was judged as the better camera for stills.  So the *photography* crown goes to the iPhone.
[Fine](https://pbs.twimg.com/media/DMV6FlEU8AIs217.jpg)

[job](https://scontent-sjc2-1.xx.fbcdn.net/v/t1.0-9/22540116_1306651849445057_34054094653138870_n.jpg?oh=b3abda8cad984b77339a246c54e3968e&oe=5AA3A655)


The Note 8 can only reach above 1000 nits when 1% of the pixels are active.  Normally, with a fully active screen, the Note 8 can only hit 560 nits at peak brightness according to displaymate.
Better quality?  Are we just ignot that the Pixel completely failed at rendering background blur?  It's so jarring.
Quick summary:

- Significantly brighter when the full display is active at 630 nits, compared to the Note 8's max brightness with a fully active display which was 560 nits.
- Better color accuracy.
- Higher sub-pixel fill rate, meaning less burn-in prone at higher brightness.
- Better contrast in high ambient light, compared to the Note 8 which gets dim and washed out.
- Better viewing angles


So does Displaymate matter anymore?  Are we DxOing this too?
r/Android really has turned into a salt mine this past month. I donâ€™t blame you, considering DxO awarded the iPhone X as the best camera for taking stills, Displaymate crowned the iPhone X display as the best in the world, and the A11 Bionic is the best SoC by far. 
Thats not really true, at least not in the way youâ€™re framing it. 

The Note 8 display can only ever reach past 1000 bits of brightness when only 1% of the pixels are active.  When the entire display is on, the Note 8â€™s screen is noticeably dimmer and washed out compared to the iPhone X in high ambient light.  Displaymate confirms, as well as the XDA developer whoâ€™s posting around in here. 
No it wonâ€™t. 

At full screen brightness in high ambient light with the entire display on, the iPhone X demolishes the Note 8 by reaching 634 bits at 100% white.  The Note 8 pushes out 560 on auto brightness mode. 
The real shocker is that itâ€™s better than the displays in Samsungâ€™s own phones.  
Iï¸ think we all realize the incredibly niche applications VR serves. 
The point is that the iPhone X will look even better in direct sunlight. The perceived contrast of the Note 8 display, for example, drops dramatically in high ambient light, while the iPhone X does not. 
Literally tried out a friendâ€™s Note 8.  I saw blueshift. 

Itâ€™s a problem inherent to the technology. Apple just suffers from it less according to displaymate. 
Three months?  Android flagships are just about catching up to the performance of Appleâ€™s two year old A9 processor. 

And letâ€™s not kid ourselves, Samsun plays catchup to Appleâ€™s features all the time.  Fingerprint sensors and optical zoom cameras with a portrait mode come to mind. 
So are we ignoring displaymateâ€™s objective data that it has the best OLED viewing angles and brightness variations?
See? Denial, everybody. 
Gimmick. 
Give us a fucking break. 
And as displaymate stated:

> Some clueless reviewers have been pining for 4K 3840x2160 Smartphones, which would require more than triple the pixels, memory, and processing power of the 2436x1125 display on the iPhone X, but there would be no visual benefit for humans! As a result, it is absolutely pointless to further increase the display resolution and pixels per inch (ppi) for a marketing wild goose chase into the stratosphere, with no visual benefit for humans!
What a bunch of horseshit. Many brands of sunglasses work with FaceID.  The only reviewer who claimed there was a problem in direct sunlight was the Verge, which was later explained as him just holding the phone way to far away. 

Reviews of the iPhone X during a Disneyland vacation in sunny Southern California demonstrates thatâ€™s thereâ€™s no real issue with using FaceID outdoors in the day. 
The notch only â€œinvadesâ€ videos when you zoom in full screen.  But itâ€™s not unique in this. Any phone with curved corners will be cutting away content when you zoom in full screen. 
Wow, the denial is unbelievable. 
And iPhone X has lower screen reflectivity and lower brightness variance with viewing angles shifts. 
Samsung is fabricating the displays, for sure. But they donâ€™t just pull any panel off the shelf and sell it to Apple. Apple likely has given Samsung Display their own Apple designed specifications to be adhered to in addition to Samsungâ€™s usual fabrication techniques. 

This is why the iPhone X display has better viewing angles and reflectivity than Samsungâ€™s best phones. 
Not all the categories. Appleâ€™s record setting screen brightness variability with viewing angle, and lowest screen reflectivity are not simple software calibration tinkering. 
â€œSamsung would never give itâ€™s best displays to a competitor!â€

â€œApple didnâ€™t customize anything! Itâ€™s just a Samsung display!â€

â€œWarble warble warble.â€
I checked, and thatâ€™s bunk. Putting on sunglasses doesnâ€™t trick FaceID into thinking your eyes are open. FaceIDâ€™s IR can pierce through most consumer sunglasses.

Stop spreading misinformation. 
Where are you getting that idea?  

Many sunglasses donâ€™t block the IR light FaceID puts out, so it still checks to see if your eyes are paying attention.   Putting sunglasses on a a sleeping person wonâ€™t do anything. 
Source?
[Samsungâ€™s security foiled by a piece of paper and a contact lense.](http://bgr.com/2017/05/23/galaxy-s8-iris-unlock-hacked-video/)
By default, FaceID requires active user attention toward the display, to prevent exactly this kind of thing from happening. 

If you disable it, thatâ€™s your fault. 
Itâ€™s not visible light, genius. 
The salt mine Android fanboys have turned Reddit into these past couple of days has been amazing. 
Facerig canâ€™t do this level of detailed without an intel realsense depth camera. 

Thatâ€™s why Animoji look bette than most facerig setups, because itâ€™s actually tracking 50 different muscle groups.  
If youâ€™re insinuating that the Face ID IR system is giving you a headache, you can stop that pseudo-scientific hypochondriac bullshit right now. 
Whatever makes you feel better about your Android purchase. 
They donâ€™t give any data for the average picture level for those numbers, so itâ€™s sort of pointless. Was it at 100% white picture level? 50%?
Now featuring dual lenses!
With attention awareness enabled, Face ID will only unlock your phone if it sees that your eyes are looking at the screen. This is to prevent unintended glances from unlocking the phone. 
I donâ€™t think youâ€™ve actually read what displaymate has to say, then. 

Maximum nits at 100% screen white is 423. 

[Source](http://www.displaymate.com/Galaxy_Note8_ShootOut_100.htm#Brightness)
Barely. Most tests that Iâ€™ve seen put the Note 8 in the high 400s for average viewing scenarios. 
Displaymate's data is correct, but it comes with a few caveats.  

Samsung's displays can achieve screen brightness in excess of 1,200 nits, but only under very specific circumstances.  First, auto brightness needs to be on and boost the display brightness if it detects excess sunlight.  Secondly, the display can only push brightness in excess of 1,200 nits when only a very small percentage of the pixels are active.

I don't think any end user will ever experience their samsung displays ever going anywhere near 1,200 nits of brightness.
>I know it doesn't agree with the agenda you are trying to push - but you are wrong. Objectively wrong. Demonstrably wrong. The very best kinds of wrong :)

Says the guy who doesn't understand the difference between lux and nit.
That video is actually measuring the lux value of the screen brightness.  Lux and nit are different units of measurement.  One nit is approximately 31 lux.

The whole idea that Samsung's displays can achieve brightness in excess of 1,200 nits is a bit of a misconception.  It's technically possible for such screens to get that bright, but only outdoors with auto brightness on, and ONLY with less than 10% of the screen illuminated.  

In most situations, where most of the screen is active, you'll only be seeing around 400-500 nits of brightness.
As you use Face ID more, the more it understands your face. Iâ€™d be interested to see what the results are a few more days or even a week after using it. 
No, just specifically Face ID, which has advantages over fingerprint sensors in some ways, and disadvantages in other ways. 

Overall, I think Face ID is a net positive, especially considering what other features the TrueDepth camera housing can create besides authentication. 
Well, weâ€™re talking about the iPhone. So those gestures are universal for the iPhone X. 
Because there are many other ways to turn on the phone screen. Like raise to wake, the side button, or tap the screen. 
Youâ€™re gonna make me cry. 
Exhibit B, everyone. 
Exhibit A, everyone. 
An encrypted hash of your facial data can not be used to reconstruct a picture a 3D model do your face.  
In what ways?
Too easy?
Donâ€™t have to worry about wet or slightly oily hands or a dirty fingerprint sensor. Can still use your phone with capacitive gloves, for those people who live in colder climates. 

Face ID also provides some extra benefits that come with a 3D sensing camera hardware, like software that knows when youâ€™re looking at the screen or 3D facial tracking for Animoji or much more advanced Snapchat filters. 
You have to actually look at the screen for Face ID to work. This prevents unintended glances or people stealing your phone while you sleep from unlocking your device. 
ITT: r/Android gets triggered by positive iPhone X reviews. Excuse making and rationalizations as far as the eye can see. 

Delightful. 
Wedding dress with the sombrero. I pretend Mario is going to an awful quinceaÃ±era. 
No.

Infrared light is all around us everyday. Itâ€™s emitted by the sun, fire, lightbulbs, and lots of other things. 
As app developers optimize more for iPhone X, the homebar should be handled more intelligently, especially for video players. 
Had the same issue when I first got my JB iPhone 7 Plus. Fixed it by ejecting the tray and forcing it back in.  
Intended for the public... beta testers. The term â€œbeta-testerâ€ precludes any whining about things not working.  Itâ€™s a beta for a reason. If it was intended for the public at large, it would just be a normal release. 
Just goes to show that out of touch nerds on Reddit donâ€™t know what the fuck theyâ€™re talking about. 
I donâ€™t expect any major changes besides a thinner case and maybe a new display with rounded corners (but still rectangular).

The design itself is already pretty perfect, so I donâ€™t think there will be that many deviations from this iconic design. 
Sort of. The PokÃ©mon intellectual property is owned and managed by the PokÃ©mon Company, of which Nintendo is a major shareholder. 
Are you saying you *canâ€™t* hack the iris scanner in the manner posted above? 

Iâ€™m not sure what your point is. Not that it matters, since â€œfalse claimsâ€ like the ones you are referring to arenâ€™t civil torts in this situation. 
Sure they can. Asking for fingerprints is not compelled speech. Furthermore, you can just look away if the police try to scan your face. Face ID requires active attention. 
Believe it so strongly?  You mean the video evidence?
>if someone can iris scan you and produce contact lenses with your iris print, they can also laser scan your face and produce a 3D print of your head and use that to unlock. It's actually less sophisticated to laser scan someone's face and 3D print it than to manufacture contacts with an iris print. 

You donâ€™t need to get that fancy to break into something as awful as Samsungâ€™s iris scanner. All you need is a photo of their face (doesnâ€™t even need to be particularly high resolution), some cheap disposable contact lenses, and a laser printer. 

Print out the face, plop the contacts where their iris is, and youâ€™re in. 
> You seem to constantly be ignoring the fact that you need to take a picture of someone from a few feet away. It is objectively true that it would be easier to capture the information you need for fingerprint. 


Oh please. Not only is taking someoneâ€™s picture from far away incredibly easy, youâ€™re also assuming thereâ€™s a clean fingerprint to be gleaned off the phone.  Itâ€™s more likely any fingerprints found on the device would be so smudged as to be completely useless. 

Also, you can simply get someoneâ€™s face picture from their social media account and apply an IR filter to it. 

> If you look up Princeton Identity it looks like they know what they are doing. They do security systems for a bunch of sectors.

It sounds like the FBI is in big trouble then if Samsungâ€™s easily broken security is better than theirs. 
Using a laser printer to print a picture that doesnâ€™t even have to be high resolution and ploping some contact lenses onto it is a lot easier than creating a latex faux fingerprint. 

At the end of the day, Samsungâ€™s security can be fooled by something that can be done in a few minutes. So much for security. 
If youâ€™re under arrest, they can get your fingerprints too. What are you on about?
Wrong link, [this](http://bgr.com/2017/05/23/galaxy-s8-iris-unlock-hacked-video/) is the right one. 

> *Whatâ€™s particularly interesting is that the photo itself doesnâ€™t even have to be particularly high quality, and the image used in the demonstration was shot from several feet away. This suggests that the actual iris scan the phone is performing at a much shorter distance isnâ€™t necessarily identifying as many details as it likely could.*

> And look up Iris recognition, it's 1,200,000:1.

Not all biometric systems are created equal. Whatâ€™s the specific FRR for Samsungâ€™s system?  You know, the one that contact lenses can fool?

> And are you kidding? You can pick up a fingerprint off of any part of the phone. Anyone that swipes you phone would have everything they need to get a fingerprint right there on the phone. 

Creating the molds and printouts of the fingerprint requires specific skills and dedicated equipment. Breaking into Samsungâ€™s iris scanner requires nothing more than a camera, a shot of the face taken even 15 feet away, a laser printer, and some disposable contact lenses. 


[https://www.forbes.com/sites/ianmorris/2017/05/23/samsung-galaxy-s8-iris-scanner-hacked-in-three-simple-steps/](https://www.forbes.com/sites/ianmorris/2017/05/23/samsung-galaxy-s8-iris-scanner-hacked-in-three-simple-steps/)

Yeah, no. 

The photo neither needs to be high quality, nor printed directly onto the contact lense.  You just need a photo of the face, some disposable contacts lenses, and just plop them onto the paper printout of the face where the eyes are. 

> Also you know the FRR for Iris is apparently better than even the 3d facial scanning from Apple?

Ignoring the fact that Samsungâ€™s security is so pathetic that a photo and contact lenses can fool it, the FRR rate of Face ID is 1:1,000,000

> So were you complaining about how easy it is to spoof touchID when that came out or was it fine then?

Itâ€™s a lot easier to spoof Samsungâ€™s iris scanner than to fool fingerprint sensors. 
Appleâ€™s Face ID is already more secure than fingerprint sensors.  Much harder to fool 3D depth scanning than fingerprint sensors or even iris scanners. 
Not to mention it can be fooled with a picture and some contact lenses. 
Regardless of that?

The iris scanner can hardly be called secure if it can be fooled so easily.  I would  say Face IDâ€™s exponentially more secure platform, as well as faster recognition and unlock times puts it miles ahead of Samsungâ€™s implementation (shocker).

Touch ID is dead. Get used to it now, because otherwise youâ€™ll continue being disappointed every year itâ€™s missing. 
Samsungâ€™s iris scanner can be fooled with a picture and some contact lenses. 
>; I guess where I am now is that I understand your argument to be that any attempt to use a different meaning of the word racism is inherently a conflation.

Well, yes. Changing the meaning of a commonly understood word is incorrect. 

When people say â€œyou canâ€™t be racist towards white peopleâ€, thatâ€™s wrong. Of course you can be racist towards white people under the common definition. 

What these people mean to say is that you canâ€™t be racist towards white people, because racism is a combination of power and prejudice. The oppression that result from that is racism according to their argument. 

The obvious problem here is that theyâ€™re conflating the stipulative concept â€œinstitutional racismâ€ with the standard definition, which simply means â€œprejudice against someone else on the basis of race.â€ 

Had their argument simply been, â€œwhite people are not systematically discriminated against â€œ or â€œwhite people are not victims of institutional racism,â€ Iâ€™d have no serious problem with that, although I would disagree. 

Again, the stipulative defintion definition is perfectly valid. What I have a problem with is the attempt to justify dickish behavior against white people because youâ€™re trying to replace the common definition of racism with its stipulative form.  
Are you going to make a point relevant to our original discussion or no?
When people say â€œyou canâ€™t be racist against white people, only prejudicedâ€ they are redefining the standard definition with the stipulative one, thatâ€™s the conflation I was referring to. 

Itâ€™s not an alternate definition, itâ€™s a stipulative one. What they are actually referring to is institutional/systemic racism, which a stipulative form of the standard defintion of racism. 

> There's nothing marxist 

Gonna have to stop you right there, comrade.  This whole SJW movement is a branch of the postmodernist narrative, which is nearly indistinguishable from Marxism. The only difference is that the Marxist scholars who spearheaded the postmodernist movement in France simply replaced the capitalist vs working class narrative with one of minority identity groups vs cultural hegemons. 
Itâ€™s not that theyâ€™re simply not using the standard definition, itâ€™s that theyâ€™re *conflating* the two. Or even worse, pretending that the stipulative definition IS the standard definition. 

Changing the meaning of a commonly understood word to a fit a particularly nasty bend of marxist identity politics isnâ€™t a â€œnet positive.â€  
> "Racism" is just a word. People disagree on definitions, and one definition puts it in purely systemic terms and reserves personal attacks for other words, like "bigotry" and "prejudice". That's all the "can't be racist towards white people" folks are saying.


Then theyâ€™re wrong for saying that.  Theyâ€™re conflating the standard definition with a stipulative one. 
And?
Reddit sure loves to hate female comedians. 
r/Android
No. 

A damp finger works fine on a touch screen. Modern mobile operating systems are designed around the inaccuracy of large fat fingers. A fingerprint scanner only works when everything is highly detailed and scanned. 
I guess weâ€™re just ignoring the fact that the OP confirmed the iPhone picture displays a more accurate portrayal of the subjectâ€™s skin?
Hereâ€™s a quote from Margaret Cho that I think is very relevant to comments like this in this thread. 

â€œWhite people love telling Asian people how to feel about race because theyâ€™re too afraid to tell Black people.â€
LOL at the people in this thread claiming the Pixel photo is more â€œnaturalâ€ despite not knowing the real skin tone of the subject and the OP himself stating otherwise. 
â€œApple goes the extra mile to make their SoCs the best.â€

â€œQualcomm doesnâ€™t.â€

â€œBut thatâ€™s not Qualcommâ€™s fault!â€
Have you been under a rock recently? The Appleâ€™s SoCs dominate both single and multi core score now. 
Which is Qualcommâ€™s fault in the end. 
The blame falls on Qualcomm then. 
Apple uses the ARM instruction set as well. Thatâ€™s not an excuse. 
What is Apple using?
You honestly think Qualcommâ€™s SoCs will be catching up to Appleâ€™s single core speeds?
> Woah...take it easy. Everyone should maintain professional civil discourse while discussion and debating things here.

Says the person whoâ€™s posts get constantly removed by moderators. 
Spam musubi isnâ€™t sushi. The rice isnâ€™t vinegared. 
Spam musubi isnâ€™t served on vinegared rice.  At least, itâ€™s not supposed to. 
Trademark*
Almost correct. 

Itâ€™s actually huffy white liberals, acting on behalf of the poor minorities, who are policing how other people should dress. 
Thatâ€™s how you interpreted our interaction? You have quite the persecution complex, buddy. 
Thatâ€™s your opinion. 
And?
What a completely pointless thing to say. 

We know itâ€™s my opinion by the fact that I said it. 
Nintendo cares about its IPs. 

Sega does not. 
Is it really, though?

The story was all sorts of derivative, and the open world concept was handled much better by BotW. 
Iâ€™ve been a fucking idiot, Iâ€™m sorry for not realizing this. 
Didnâ€™t Google make fun of Apple for removing the headphone jack?
And you are?
[Delete](https://vignette.wikia.nocookie.net/deathnote/images/d/d8/Mikami%27s_Shinigami_Eyes.png/revision/latest/scale-to-width-down/310?cb=20170902111153)
The fact is, Apple solutions produces a better depth map at the end of the day, which allows for better blur application. 

What matters is results. 
Look directly at the sun for a few seconds and please tell me if invoking the inverse square law is going to help you. 
Machine learning isnâ€™t magic. Thereâ€™s only so much depth information you can get from a single sensor. 
Well, not in the case of the brightness of the sun as observed from the Earthâ€™s surface. 
Inverse square law doesn't mean very much if we're talking about a light source like the sun.
Weâ€™re talking specifically about the portrait modes, of which the Pixel 2 doesnâ€™t do nearly as good as a job as the iPhone with its dual camera setup. 

The fact is, a single lens will never provide as much depth information as two separate cameras will. 
Youâ€™re gonna make me cry. 
[https://pbs.twimg.com/media/DMV6FlEU8AIs217?format=jpg](https://pbs.twimg.com/media/DMV6FlEU8AIs217?format=jpg)

Yeah, the Pixel 2â€™s Portrait Mode is really shitty. 
What would you know about photography?  

â€œForeground elements shouldnâ€™t be blurred in shallow depth of field!â€
â€œPixel has more true to life color, gonna have to pick that one.â€

â€œTechnically, the colors in the iPhone photo are more accurate, but Iâ€™d still pick the Pixel. 

Hmmm?
Really?

When the Pixel 2 messes up the bokeh, it does so spectacularly. Itâ€™s totally jarring to see a photo where entire sections of the background are in focus next to parts that are blurred. 
Oh, please. 

We both know that if the new iPhones had this problem, all of Reddit would be making memes about it. 
If you donâ€™t have a password enabled to begin with, whatâ€™s the problem?

Itâ€™s not like your phone is secure anyways. 
Ouch. 
So says u/BlazeBro420. 
Definitely. 

The latest 11.1 beta has increased battery life, RAM management, and overall speed of my 7 Plus. 
You mean when Samsung said everything was fine with their new batch of Note 7s, which then proceeded to explode again, causing a full recall?
Eight-hundred and forty nine dollars. 
Samsungâ€™s iris scanner can be fooled with a photograph and some disposable contact lenses. 
Iâ€™m sitting at 104 upvotes, buddy. 

Iâ€™m not saying anything r/Android isnâ€™t thinking. Buh bye. ðŸ’…
uh huh.
The mental gymnastics on display here almost make the ones on r/GooglePixel seem tame in comparison. 
iOS 11.1 beta fixes issues. Soon, iPhones will be back to being as smooth as 10.3.3. 

It was a rushed update, thatâ€™s all. 
And Google thinks they are at Appleâ€™s level enough to price their phone that high?
It does, heâ€™s just mistaken. 
This is $850?
It has single tap to wake, along with raise to wake. 
Iâ€™m good lighting, thatâ€™s absolutely incorrect. The zoom lens preserves much more detail than a crop. 
No.  In fact, if you can read my post, I specifically say Iâ€™m not talking about a recall. 


Who said anything about a recall? Simply acknowledging that the issue exists would be something. 

Apple actually had the balls to go on stage and not only bring attention to the fact that the headphone jack was kicking the bucket, but defended their decision with reasons why. 

Google?  They didnâ€™t dare mention anything about the headphone jacks. Real lack of Courageâ„¢. 
And?

Iâ€™m merely demonstrating the different scales Apple and Google operate at to get people to understand the sort of experience Google has at hardware. 
At the end of the day, Google is a software company, not a hardware company. 

Itâ€™s not really a surprise that their initial attempts to get serious about hardware are going to come across as amateur.  As they get more experience with supply chains and design, things will improve. 

To put things in perspective, the OG Pixel only sold about 2 million in an entire year, while Apple sold 41 million iPhones in 3 months.  
Please. 

Google was practically tiptoeing around the headphone jack debacle, and didnâ€™t even mentioning its removal in their presentation. 

I doubt they have the integrity to do anything like this. 
1 month isnâ€™t exactly a long time...
Samsung isnâ€™t magic. They donâ€™t have infinite resources or technical capacity to produce unlimited displays. 

One of the big reasons Apple has took this long to make the jump to OLED (besides low brightness and inaccurate colors) was that no manufacturer had the ability to manufacturer OLED displays at the scale Apple requires.  It wasnâ€™t until recently that Samsung has been able to produce enough OLED displays to meet both their own requirements for their own phones and Appleâ€™s demands. 

On good years, Apple sells 40-45 million iPhones in **one quarter**.  Thatâ€™s more phones than pretty much all Android flagships combined for the whole year.  Thatâ€™s a hell of a lot of AMOLED displays. 
It wouldnâ€™t be able to fit if it was waterproof. Waterproofing the headphone jack takes up extra space. 
Great review.  Heâ€™s saying what weâ€™re all thinking about the Pixel 2. 
It's your property.  You can rescind anyone's license to be on your property for any reason except if the duration of the license is already determined by a former contractual obligation.


How condescending.
[Yikes](https://pbs.twimg.com/media/DMV6FlEU8AIs217.jpg:large)
Uh huh. 

ðŸ’…
Arguing that â€œbokeh should only apply to the backgroundâ€ is idiotic. 

You were out of your element and discussing things you had no clue about. That much was clear.  
The iPhone picture looks normal at first glance. The Pixel photo is glaringly obvious in that the portrait mode didnâ€™t work properly. Thatâ€™s not how shallow depth of field works. 
At least I didnâ€™t have to resort to making arguments so completely idiotic that I had to end up deleting all of them to save myself from embarrassment. 
Answer the question. 
ðŸ˜¥
Why not?
Says the person who argued that shallow depth of field shouldnâ€™t blur foreground objects. 
[https://pbs.twimg.com/media/DMV6FlEU8AIs217.jpg:large](https://pbs.twimg.com/media/DMV6FlEU8AIs217.jpg:large)
No itâ€™s not. It simply uses the telephoto lens for the portrait photo. No one in the DSLR world uses a wide angle for a portrait. 
Yikes, thatâ€™s really awful. I canâ€™t believe the Pixel messed up the depth effect that badly. 

Oh wait, yes I can, it only has one camera. 
As long as you say so. 
Theyâ€™re at different angles with different poses, so no, they arenâ€™t similar at all. 

My comparison is a better comparison, but the subject didnâ€™t move between shots.  Comparisons where the two photos are more similar are logically the better comparisons. 

> You just can't  accept that the Pixel outperformed the iPhone. Your bias is showing.

Pot, meet kettle. 
Because itâ€™s shows how absolutely terrible the Pixel is at judging depth. 

Did you even look at it? Look how amateur the bokeh effect is.  The entire right side of the photograph suddenly decided it didnâ€™t didnâ€™t need to be consistent and feature any blur. Then you have this weird halo effect near the guyâ€™s chin where the background is suddenly in focus while everything else is blurred. 

Also, itâ€™s a better comparison, because the two photos are the same, unlike OPâ€™s example. 
Hereâ€™s a better comparison. 

[Pixel 2](https://mobile.twitter.com/backlon/status/920324313764507648/photo/3)

[iPhone](https://mobile.twitter.com/backlon/status/920324313764507648/photo/4)

Looks how awful the Pixel is at determining background depth. The right side of the photo isnâ€™t blurred when itâ€™s supposed to be, and the background near the subjectâ€™s chin is in focus, which looks completely unnatural. 
[This](https://pbs.twimg.com/media/DMWl9p0VQAESlQE?format=jpg) foreground object is not the focused subject of the photo, the dark metal/rock thing is. Itâ€™s also closer to the camera, so itâ€™s correct thatâ€™s itâ€™s lightly blurred. 

So, again, you have absolutely no idea what youâ€™re talking about. 

If youâ€™re talking about the slight blur on the top of the dark rock thing, that is also correct. Since itâ€™s a round object, the top of it is actually farther away from the cameraâ€™s focused area. 

This is how SLRs works. 

[Like here](https://www.colesclassroom.com/wp-content/uploads/2015/05/how-to-get-a-shallow-depth-of-field-ring-1-of-1.jpg)  

The right part of the band is slightly further away from the cameraâ€™s focus point, which is the center diamond, so itâ€™s blurred. 

[Or here](http://keyassets.timeincuk.net/inspirewp/live/wp-content/uploads/sites/12/2016/05/Main_pic_2-599x400.jpg)

The right side of this metal post is angled slightly farther away from the cameraâ€™s focused subject, the flat part, so its blurred. 

Admit it, you donâ€™t know what youâ€™re talking about. 

Edit: I was right, u/HugOfThunder doesnâ€™t know what the hell heâ€™s talking about, already deleting his posts.
Because thatâ€™s how shallow depth of field works. Itâ€™d be weird if it *didnâ€™t* blur it out. 

Shallow depth of field blurs objects behind *and* in front of the focused subject.  Thatâ€™s how this works. 

[Like this](http://posterjackcanada.files.wordpress.com/2014/01/english-bulldog-puppy-dog-water-lake.jpg)

[Or this](https://library.creativecow.net/articles/terry_todd/depth_of_field_converters/gnome-dof.JPG)

[And even this](http://camerachronicle.files.wordpress.com/2008/06/dsc_5280bw.jpg)

[Donâ€™t forget about this](https://s-media-cache-ak0.pinimg.com/originals/ae/c4/7b/aec47b92d3d1a10fefea1143923f7b6e.jpg)

So, thanks for demonstrating you donâ€™t know what the fuck youâ€™re talking about. 

Edit: [An extra one for ya](http://www.aleckirkham.co.uk/wp-content/uploads/2012/08/132-SS-E-Session-1024x682.jpg)

[One more for your mum](https://i.pinimg.com/736x/cc/77/79/cc777978bf6d9119696a75871ee0d92a--engagement-photography-art-photography.jpg)

Notice how the right edge of the wall is lightly blurred because itâ€™s *closer* to the camera than the focused subject. 
u/Grizzly_Magnum_
What the hell are you talking about? In the pictures you provided, the iPhone didnâ€™t not improperly blur anything near the water. Show me. 

The fact is, you got embarrassed by not knowing what the hell shallow depth of field is, claiming foreground objects shouldnâ€™t be blurred, and now youâ€™re just defensive about it. 

Just admit that you have no clue about photography. 
In what ways is the shallow depth of field effect (which you clearly donâ€™t know anything about) better than the iPhoneâ€™s photo? u/Cuentanueva already spent some time educating you about what shallow depth of field is and why the iPhone was better at it. 

The blur has an actual gradient on the iPhone, and the pixel incorrectly applied less blur on the right wall. 
Hereâ€™s a better comparison. 

[Pixel 2](https://mobile.twitter.com/backlon/status/920324313764507648/photo/3)

[iPhone](https://mobile.twitter.com/backlon/status/920324313764507648/photo/4)

Looks how awful the Pixel is at determining background depth. The right side of the photo isnâ€™t blurred when itâ€™s supposed to be, and the background near the subjectâ€™s chin is in focus, which looks completely unnatural. 
In what ways?  Like how the pixel improperly decreased the blur on the wall on the right?
Which is done using shallow depth of field...

Are you being intentionally dense?
Thatâ€™s not what shallow depth of field is. You donâ€™t know what the hell youâ€™re talking about. 
Yes, thatâ€™s what shallow depth of field is.  Are you new to photography?

Edit: For those wondering what the deleted posts were, u/HugOfThunder was unironically arguing that foreground objects in shallow depth of field photographs shouldnâ€™t be blurred, and only background elements should be blurred. 

Obviously, stupid things to say, hence the deletion. 
Because thatâ€™s how shallow depth of field is supposed to work? Smh...
Are we just going to ignore the fact that the right side of the Pixel photo is incorrectly blurred?  Not to mention that background near the chin is incorrectly in focus?

This is what denial looks like, folks. 
Not to mention that the Pixel 2 canâ€™t determine depth properly. The right side is not blurred when it should be and the phone thinks the part of the background next to the subjectâ€™s chin is on the same plane as him. 
Hereâ€™s another comparison. 

[Pixel 2](https://mobile.twitter.com/backlon/status/920324313764507648/photo/3)

[iPhone](https://mobile.twitter.com/backlon/status/920324313764507648/photo/4)


The Pixel looks like it was photoshopped.  His shoulder was incorrectly blurred.  
Hair looks way better? The rift of hair sticking straight up in the Pixel photo practically got erased. 
If youâ€™re in the middle of a climb when itâ€™s starts raining, itâ€™s generally your own fault. The game has a weather forecast indicator for a reason. 
ðŸ™„
Probably wouldâ€™ve been quicker to say yes or no then that. 
Are there any phones that use Windows Hello in this manner? Or is this just limited to PCs?
What Windows Hello equipped device does 3D facial mapping?
On behalf of veterans?  I guess some of them are being that self-righteous in their anger, but I would think most of them are just pissed. 

Fetishizing minorities and being offended on their behalf does seem to be almost exclusively a thing for liberals with a white savior complex.  White people do it all the time, itâ€™s like they canâ€™t themselves. 
Being offended on the behalf of others is just something huffy white liberals love doing. 
If youâ€™re going to be so blasÃ© about security, it kinda makes you wonder what the point is about security in the first place. 

Would you really let any random stranger, coworker, or ex-girlfriend unbridled access to your phone?  I know I wouldnâ€™t, but I guess we just have different priorities and data on our phones. 
Thereâ€™s more to security than protecting against money withdrawals, unless you have a paypal, which you can definitely withdraw money from.  A majority of people are autologged into their online accounts from their mobile browsers.  Hell, if someone keeps their passwords for their online banking in their notes app, you could do a lot of damage if their bank has something like Chase Quick Pay. 

I mean, if you donâ€™t care about your privacy and other sensitive information, thatâ€™s fine. You donâ€™t think people carry sensitive data in their private messages, emails, and their web browser?  


Except, you know, access to your private messages, emails, and online accounts that are still logged in. 

The damage control is pathetic. 
Of course the government would downplay these figures, access to this information has generally been restricted by the communist government, up until they granted DikÃ¶tter access to their historical archives.  

According to Chinese journalist, Yang Jisheng, the massive amounts of dead in China were commonly ignored by government reporters and the overall population themselves, no one dared mention the catastrophe occurring around them should they anger the communist officials.  The true figures of the disaster exceed well over 15-30 million. 

> Regardless, the cumulative deaths in India massively dwarf those in China,

Cumulatively, sure, but youâ€™re talking about a span of 50 years. In just four years, China lost, at the very least, a quarter of what India lost in 50 years.  In terms of scale, the Marxist Chinese famine was significantly worse. 
What kind of excuse is this? The fact that youâ€™ve broken in at all is a huge failure for security. 
> The legendary economist Amartya Sen put the average excess deaths from malnutrition in India at around 4 million annually between 1950-2000, 

The Chinese famine due to Maoâ€™s policies were significantly worse.  Frank DikÃ¶tter, professor of Chinese history at the University of Hong Kong, estimates that 45 million Chinese starved to death in a span of only 4 years during Maoâ€™s Great Leap Forward, **at the very least.**. And this was while the total population of China hovered around 650-700 million. 

Due to the fact that these numbers were generally fudged by officials in order to save face (prevalent in reports during the grain shortages), the real numbers are most likely astronomically higher. 
No, thatâ€™s iris recognition. Fooling the facial recognition doesnâ€™t require the contact lenses. 
IKEAâ€™ss products are widely regarding as poor quality and shoddy because they arenâ€™t very durable, which is what you would expect when your main construction material is particle board. 

Are you going to pretend that one of IKEAâ€™s main criticism is their shoddy materials?
I think one reason is that portrait photographs should really emphasize the subject. Apple and Samsung use a telephoto lens for their portrait modes, which are much more flattering on human subjects than a wide angle lens. 

Also, Googleâ€™s algorithms simply arenâ€™t that great at determining depth and how much bokeh is going to be appropriate.  
Itâ€™s true. In some of those pictures, you have a background blur, but then suddenly a splotch where thereâ€™s no blur. It looks completely amateurish. 
Youâ€™re delusional, then. 

In a lot of those pictures, the Pixel 2 canâ€™t even blur the background correctly, with voids of clarity in the background blur.  The Pixel 2 is also significantly worse at halo artifacts around subjects, especially hair. 
Except the iris scanner is much less secure and doesnâ€™t work against bright backgrounds. 

The damn thing could be fooled with a photograph and a contact lens.
Which could be fooled with a picture and some contact lenses... ehm....
Whatâ€™s your source for the hundreds of millions dead from starvation for India?
Yes. Apple Maps has lane guidance, traffic rerouting, and even speed limits. 
Apple Maps. 

In Los Angeles, Apple Maps is just as good as Google Maps for everything I do. Never been lost, and business locations have been correct so far.   
Hearsay is any out-of-court statement offered for its own truth. That applies to recordings as well as recollections from witnesses. 

There are exceptions to this evidence rule, however. But generally, such statements are not allowed in as evidence. 
Thank goodness hearsay is generally excluded as evidence in trials. 
It looks like a bird shit on some moldy play-doh.  
This is because the actual building itself has no copyright protection, as it has expired. But the lighting setup at night is itâ€™s own artistic work, which has copyright protections. 

Essentially, imagine an old building that has no copyright protection, you can take a picture of that no problem. The , suddenly, some dude walks on by and paints a painting on one of the walls. An artistic creation has now appeared on that building, and you can no longer take a picture of that wall without infringing on that authorâ€™s copyright. 
Uri, is that you?
What do you think the word â€œorâ€ means here?

And yes, I would say many of their items are of poor taste, considering they look like theyâ€™re construed of cheap particle board, because they are. 

> You could say they are poor quality (though the durability of their items suggests otherwise),

What the hell are you talking about?  IKEA is commonly criticized for selling cheap items that fall apart due to their shoddy materials.  Do you think particle board is a durable material to make tables or desks out of?

Câ€™mon now. r/Android is really starting to give me a headache here...
Not really, no. 

â€œTackyâ€ is a broad term that more or less equates to anything of poor taste or quality.
Thatâ€™s an oddly specific definition of â€œtacky.â€
ðŸ‘
> "man this guy's being a dick by asserting that what he think of design is objectively superior to every other person on /r/android, Im going to remind him that that isn't what an opinion is"

Yeah, thatâ€™s what is entirely fatuous here. We all know itâ€™s an opinion by the fact that Iâ€™m saying it. Thereâ€™s no such thing as an objective opinion. The cliched response of â€œwell, thatâ€™s just your opinionâ€, as if that was ever in contention, is an utterly fatuous, stupid thing to say. 

> That's actually what I meant, albeit a little vaguely put on my part. You had two trains of thought, and I responded to both of them in one comment.

Oh, bullshit. You just werenâ€™t following the conversation correctly. Your backpedaling doesnâ€™t even make any sense. 

You clearly said I was referring to other peopleâ€™s sense of design as â€œfatuous.â€ 
> Not exactly that its a highly committed word or anything, but using the phrase 'utterly fatuous' just to describe someones opinion on design

Except that isnâ€™t what I did. The comments that I referred to as â€œfatuousâ€ was the comment where he stated â€œwell, thatâ€™s just your opinion.â€ (Paraphrasing)

Thatâ€™s what was fatuous, because it is. Itâ€™s a silly, empty, pointless thing to say.  Of course itâ€™s my opinion, I said it.

So get off *your* high horse. You canâ€™t even follow the conversation properly. 


Thatâ€™s a lie. 
In my experience, good hamachi is generally REALLY good. Much better flavor than your average tuna nigiri. But when itâ€™s bad, itâ€™s really bad. Gets a super funky fishy flavor that even bad tuna doesnâ€™t get. 
> Especially when itâ€™s deep fried sushi 

You need to stop right now.  Itâ€™s time to stop. 
Weight?

Aluminum is a much lighter material than something like glass. 
They come off as cheap, to me. It pushes it into tacky territory. 
Sorry, cheese doesnâ€™t belong in sushi.  

Although I understand cheap places putting lots of crazy sauce and cheese on their rolls. Really hides the crappy quality of the fish and rice. 
Itâ€™s not very impressive when the end result looks like it was photoshopped. 

Next yearâ€™s pixels will have dual lenses. Just like how Google introduced OIS this year.  Or how they crapped on Apple for ditching the headphone jack, then quietly took it out this year. 
What about it is particularly smart? The word â€œfatuous?â€ 

I feel like bringing up r/iamverysmart is a thing lazy people do when they have nothing to say. 
The depth information a single lens with dual pixels is nowhere near the level of depth information two different physical lens can give you. 

Thatâ€™s why the portrait mode pictures from the rear cameras have been pretty mediocre. 
Yeah, I get you. The second photo you were referring too did have a pseudo gradient. The wall to the right of the subject was less blurred than the background in the left of the picture. However, the left background had no gradient in its own region. 


The depth map in that video was an artistâ€™s rendition of the depth map, not an actual depth map of that scene.  

Iâ€™d be surprised if a single sensor can get that level of depth information.  Machine learning canâ€™t overcome physics. 
Radical. I can dig it. 
â€œDweebs?â€

Hahaha. What are you, a bully in a 90s sitcom?
Itâ€™s considered a sushi faux pas to put your wasabi in your soy sauce. 

Keep them separate. 
â€œTouchWiz is fixed now!â€

- Circa 2012, 2013, 2014, 2015, 2016, 2017. 

Maybe if we keep repeating it, itâ€™ll come true one day. 
I mean, if you want to deal with LagWiz, be my guest.
Yeah, you get what you pay for, better phones. 
Iâ€™d rather not have the option at all than a crappy version. If you want a proper front facing portrait mode, wait till the iPhone X. 
[https://cdn.dxomark.com/wp-content/uploads/2017/10/ref1_Bokeh-Indoor_GooglePixel2-e1506967768712.jpg](https://cdn.dxomark.com/wp-content/uploads/2017/10/ref1_Bokeh-Indoor_GooglePixel2-e1506967768712.jpg)

Ignoring that the left side of her hair has completely disappeared, the bottom right portion of the chair was improperly blurred, and the column behind her head was also improperly blurred, the blur effect in the background is completely unnatural. There is no gradient to the blur, since the dual pixels are not capable of parallax distinction at those distances.  And the actual bokeh effect is completely unnatural as well. 

I think we both know Google is going to do a 180 next year and include dual cameras.  Just look at how they handled OIS and the headphone jack. 
The video showed a portrait mode photo by filming the phone. Hardly enough to judge quality.  And the written review didnâ€™t show any Portrait Mode Photos, aside from a photograph of the phone displaying a portrait selfie. 

What are on about?
Honestly, when it comes to taste in fashion and design, r/Android is the last place anyone should be looking. 
We know itâ€™s my opinion by the fact that I said it. What an utterly fatuous thing to say. 
I donâ€™t see any Portrait Mode Photos in that review. 

Try again. 
> Some of you must be 12 years old.

Says the person who thinks parties last for 3 hours before itâ€™s bed time. 
An old rug has â€œcharacter.â€  Tattered jeans have â€œcharacter.â€  The Pixel 2 does not have character, itâ€™s just ugly. 
It looked nowhere near as bad as the Pixel 2.  Most of Appleâ€™s Portrait Mode improvements have just been for the halo effect around subjects. The Pixel 2 can barely handle backgrounds. Which makes sense, since there is only so much depth information you can get from a single lens. 
Saw that excuse coming a mile away. 
Your argument was that IKEAâ€™s success was ONLY because of their design. My objection was that their success has more to do with their prices and innovative shopping experience. 

I think youâ€™re just arguing for the sake of arguing at this point. 

> You're allowed your opinion on the new Pixels obviously, but I personally love the design. It has more character than the iPhone 8 for example.

Character? Is that what youâ€™re calling it? Oh my lord. Hahaha.  
[https://cdn.dxomark.com/wp-content/uploads/2017/10/ref2_Bokeh-Outdoor_GooglePixel2.jpg](https://cdn.dxomark.com/wp-content/uploads/2017/10/ref2_Bokeh-Outdoor_GooglePixel2.jpg)
No one said their design was an issue. My argument was that their success has less to do with design, and more to do with affordability and the ease of purchase to setup in the home. 

People shop at IKEA primarily because itâ€™s what they can afford.  Itâ€™s cheap, without being hideous.  But it does look cheap. 
Software isnâ€™t magic. No amount of google buzzwords, err, machine learning and AI will magically pull depth information out of thin air. 

Dual pixel or not, single lens solutions have obvious limitations. 
â€œLooks good for the price they paidâ€ isnâ€™t the same as â€œgood.â€
The iPhone X front camera system has actual depth sensing abilities thanks to the Face ID sensor housing. So weâ€™ll be seeing selfie portraits that donâ€™t look like garbage. 


If you think their stuff looks good, then youâ€™re as tacky as their furniture. 

People buy IKEA stuff and put up with their shoddy products primarily because itâ€™s cheap and easy to take home.  
People donâ€™t shop at IKEA because they look good, they shop at IKEA because they need a futon or a dresser right now, and IKEA offers a convenient, affordable prices option that isnâ€™t terribly hideous or terrible crappy. 

Although, as a personal anecdote, every person I know who bought IKEAâ€™s basic sofa futon had it break down within a year. Stay away from those. 
Do you shop at ikea?
IKEA isnâ€™t a success because they look good. IKEA is a success because of their innovative business model, prices, and commodifying furniture setup. 

No one looks to IKEA for fashionable home decorating.  Quite the opposite. Their designs are a bit tacky and cheap looking. But hey, they are cheap and theyâ€™re easy to take out of the store and assemble yourself. 
You can just disable it and have notifications fully visible all the time. 
The iPhone X has a real depth mapping system, thanks to Face ID. So weâ€™ll soon start seeing actually good front facing portraits. 
No wonder it looks so shitty, then. 
Looks awful. The backgrounds close to the camera have the same amount of focus blur as the trees across the road. 

Googleâ€™s single lens implementation is really showing its limitations. Looks absolutely amateurish compared to Samsung or Appleâ€™s dual lens implementations. 
Itâ€™s not just the front, but the back. The visor isnâ€™t â€œboldâ€ design, itâ€™s just ugly. The camera  region is also a complete mess, with the LED flash and other sensors having no sense of symmetry or well thought out placement. They seem just thrown around, haphazardly. 
I think we all have come to realize Joshua Topolsky doesnâ€™t know what the hell heâ€™s talking about. 
Googleâ€™s new products look like they belong in Ikea. 
Google Clips is supposed to be able to take and capture candid scenes. Just put it down and forget about it!

Only it lasts three hours and the charging port is on the bottom, so you canâ€™t leave it plugged in and recording.  Great design. 
If youâ€™re not gonna even put any effort into actually understanding what I stated, why even bother?

PenTile displays overcome this inherent fuzziness by cranking up the resolution.  PenTile really isnâ€™t good until you get into QHD territory, otherwise you start seeing the PenTile artifacting. 
Is that wasabi paste in your soy sauce?
Why do people defend this?  Well, I gave plenty of reasons why the 4.7 inch iPhones are stuck at 750p, which you have very conveniently hand-waved away. 

> I'm done with this conversation because you're picking specific devices and trying to justify it

What the hell does this even mean?  Iâ€™m picking specific devices?  Well, no shit. Weâ€™re talking about specific devices. 
Have you played 3D World?  Many of the levels absolutely do not operate as linear side scrollers.  
Ok?  At 4 inches, I hardly think you need more than even that. 

The smaller the display, the quicker you start seeing diminishing returns for resolution bumps. Also, the iPhone SE is very affordably priced anyway, so itâ€™s not like youâ€™re paying premium for the screen.

Are you going to address the rest of my response? Or just that part?
Well, iPhones, for a long time, were compact phones, with 4inch or smaller displays. At these sizes and average viewing distances, anything higher than 720p is is gratuitous.  Thereâ€™s no real point in adding full HD displays to devices that small, besides a checkbox the marketing team can tick off and affirming the purchase confidence of nerds on message boards. 

Now, Apple increased the screen size of their base phones to 4.7 inches, which output a 750p image. So why isnâ€™t this full HD?  Because itâ€™s 4.7 inches.  Besides overall image clarity not being that obvious on a display that small, it helps save battery for a device that is physically pretty small. 

You also have to consider the fact that comparing resolution between iPhones and the predominately OLED dominated land of Android flagships, is a bit tricky. 

Most, of not all, OLED Android flagships have used PenTile displays, which are inherently fuzzier than LCD displays given the same resolution. Will a 1080p PenTile display be sharper than a 750p IPD LCD? Yes, but not nearly as much as you might think. 

You also arenâ€™t considering other areas Appleâ€™s displays have traditionally been *better* than the competition, like color accuracy, color range, brightness, uniformity in colors, and viewing angles. 
The iPhone 6 Plus had a 1080p display. 
Oh please, Google based itâ€™s business on going through your personal information.  Hell, they only just started to roll back on spying through your Gmail for advertising purposes. 

This isnâ€™t a secret. 

[Source](https://www.washingtonpost.com/news/the-switch/wp/2017/06/26/gmail-will-no-longer-snoop-on-your-emails-for-advertising-purposes/)
The team that broke through the security stated that it doesnâ€™t need to be high resolution at all. 

The only source thatâ€™s ever claimed otherwise was a Samsung representative. Which of course they would say that. 
It takes a lot to make me cry, sunshine. 
r/Android can be pretty sensitive sometimes. 
Women generally have smaller hands, and the new Galaxy phones are *really* tall. So itâ€™s probably quite a bit of a reach to get to the fingerprint scanner for most women. 
Thatâ€™s all wrong. You donâ€™t need a particularly high res picture. You literally just plop the contact lens onto the paper, and youâ€™re in.

[Source](https://youtu.be/gtQ4yzbsi-c)
Unless youâ€™re behind a bright light source, like the sun. 

Not to mention Samsungâ€™s iris scanner can be fooled with a photo and some contact lenses. 
Itâ€™s not that. Itâ€™s just that the article claims out of all the phones it mentioned, the Note 8 had the most power, despite the iPhone X being there. Thatâ€™s all. 
What I like about Nintendo is just their overall game design philosophy. While many video game studios start off with a story they want to tell, Nintendo starts off with a game mechanic concept thatâ€™s seems fun, and builds around that, with the narrative evolving got merely fit around the overall game concepts and level designs and progression. 

So what youâ€™re left with is a game that puts its core mechanics and controls first, making something thatâ€™s firstly, fun.  

Many â€œAAAâ€ titles from other developers sometimes leave a sour taste in my mouth because they focus on the story first, and merely mood gameplay around to what organically fits the narrative.  So youâ€™re left with a game with generic mechanics, or fun mechanics that are underdeveloped. 

A good example of this would be the uncharted series. Less video games, more interactive action movies. Mechanically, itâ€™s a pretty generic, shallow game.  Run of the mill third person cover shooting interspersed with linear platforming segments, one solution puzzles, or set pieces that give an illusion of choice or agency.  And it has to be this way, any more freedom and the player wonâ€™t be able to experience the narrative the creators want you to see.
I found him both an awful person and an awful character. There was nothing redeemable about him for the entire show except for the last 5 minutes of the final episode. He wasnâ€™t relatable, because of his almost sociopathic selfishness, narcissism, manipulation, and total disregard for other people.  And when he wasnâ€™t manipulating Craig or Reese, or selling out his own father for whatever, he was whining about how much his life sucks or how everyone but him was an idiot. Endlessly irritating. 


Iâ€™m getting upvotes and youâ€™re getting downvotes because you posted something not so very smart. 

My point was not to compare the notch and the logo in appearance or function, but that they were both highly visible, distinctive designs that helped the branding of those devices, even though Apple never outright stated it. 

I was responding to a person who was arguing that the notch didnâ€™t serve any significant branding function because there was no evidence Apple ever stated that was the case. So I responded that the Apple never stated the glowing MacBook logos were a branding feature, but they clearly are because they are visually distinctive and cause people to notice and identify MacBooks in public. The notch is the same thing. People notice the iPhone X because the notch is visually distinctive. 

So shut the fuck up, alright?
Note 8 has the most power under the hood?  Hahahahaha.  
Yeah, theyâ€™re different. So what?  Got anymore dumb, empty things to say?
So in other words still an incentive? Got it.   I mean, if they never cared, they never wouldâ€™ve started it in the first place. 
Malcom wasnâ€™t an awful character because he was the â€œstraight man.â€  He was awful because he was endlessly shallow, manipulative, arrogant, and just an awful human being. This normally wouldnâ€™t be so bad, but his constant preening and posturing about his intelligence and how much better he is than everyone else really makes him off-putting. 
Seemed pretty trivial to me. But maybe itâ€™s just because Iâ€™m not being purposefully obtuse in order to feel better about a purchase I made. 
Because some people like tvOS, which is miles ahead of any other UI found in cheap, junky streaming sticks.  Not to mention an actual app ecosystem and cool integration with other Apple devices you may have. 

What the hell are you even doing on r/Apple if you donâ€™t understand this?
Apple doesnâ€™t need to explicitly state that the glowing Apple logos on the backs of their MacBooks are for branding and recognition purposes, but only a fucking idiot would argue otherwise. 
Itâ€™s about barrier to entry. 

Any monkey could spoof iris scanners. It requires a bit more sophistication and effort to spoof a fingerprint scanner, and currently impossible to spoof Appleâ€™s 3D facial scanning. 


[You can be quiet now.](https://youtu.be/gtQ4yzbsi-c)
Itâ€™s not hard to get someoneâ€™s picture when they arenâ€™t paying attention, especially in a crowded area. Itâ€™s not hard to print it out. Itâ€™s not hard to then place disposable contacts into that picture. 

Compared to other biometric securities, like fingerprint scanners, or Appleâ€™s 3D facial scanning tech, iris scanning one of the easiest to break into, just a step above standard 2D facial recognition. 

Thereâ€™s a reason iris scanning isnâ€™t secure enough to be used for banking authentication. 
This about the process of breaking the security, not stealing the phone. 

Is it that hard to admit the security of your phoneâ€™s iris scanner is mediocre?
Taking someoneâ€™s picture from far away using a camera with a zoom lens, printing it out, and placing some cheap disposable contacts onto it is hard work for you?
Thatâ€™s not very hard. Most real cameras have a night mode, and the photo actually doesnâ€™t need to be that high quality.   Just print a picture big enough for some cheap contact lens to fit over the iris, and boom, youâ€™re in. 

Certainly much less work intensive than trying to create a fake fingerprint. 
If IUP customers get preferential treatment, it adds
Value to the program, which would make more people join, which is better for Apple. 
iPhones have been getting thicker almost every generation for the past few years.
iPhones have been getting *thicker* for the past few years.  Same goes for Galaxy phones.
The side bezels of the S8 really arenâ€™t that small. Stock press images from Samsung are exaggerated. Not as exaggerated as Xiaomiâ€™s press images for the Mi Mix line, but still noticeably exaggerated. 

Edit:  To the r/Android shills who don't believe me and keep downvoting

- [Samsung's fantasy world](http://cdn01.androidauthority.net/wp-content/uploads/2017/03/samsung-galaxy-S8-black-840x876.jpg)

- [Real life](https://www.androidcentral.com/sites/androidcentral.com/files/styles/xlarge_wm_brw/public/article_images/2017/03/galaxy-s8-black-front-software.jpg?itok=ccXTNQ0E)

Let's not forget:

- [Xiaomi's fantasy world](https://cdn.goandroid.co.in/wp-content/uploads/2016/10/mi_mix_3.jpg)
- [Reality](https://cdn.vox-cdn.com/uploads/chorus_asset/file/9197185/dseifert_170907_1972_0010.jpg)
Theyâ€™re actually not. Samsungâ€™s press images are exaggerated. In real life, the side bezels of the S8 are indeed small, but not as small as the press images would have you believe. 

Also, screen-to-body ratios are a bit misleading, as itâ€™s not synonymous with actual bezel size. You can have a greater screen-to-body ratio by simply increasing screen size. 

Edit:  To the r/Android shills who don't believe me and keep downvoting

- [Samsung's fantasy world](http://cdn01.androidauthority.net/wp-content/uploads/2017/03/samsung-galaxy-S8-black-840x876.jpg)

- [Real life](https://www.androidcentral.com/sites/androidcentral.com/files/styles/xlarge_wm_brw/public/article_images/2017/03/galaxy-s8-black-front-software.jpg?itok=ccXTNQ0E)

Let's not forget:

- [Xiaomi's fantasy world](https://cdn.goandroid.co.in/wp-content/uploads/2016/10/mi_mix_3.jpg)
- [Reality](https://cdn.vox-cdn.com/uploads/chorus_asset/file/9197185/dseifert_170907_1972_0010.jpg)
Thanks. 
Is there still lag when opening 3D Touch app shortcuts? Howâ€™s overall performance?
When youâ€™re broke and your lifeâ€™s in the shitter, I guess itâ€™s normal to try and feel above everyone else based on what cartoons they watch. 
Though they are closely related. Jackfruit is durianâ€™s less smelly, sweeter cousin. 
Is that a series 3 LTE exclusive watch face?
Thatâ€™s not how it works. 

Why do you think the FBI tried so hard to get into the San Bernardino terroristâ€™s phone? 
They turn over information they have when a legal request is made.   What they have is limited for jigs tired on icloud servers. They canâ€™t remotely hack your iPhone to retrieve local files, they donâ€™t have the encryption key. 
If a music source is playing, lifting up the watch should default to the Now Playing screen, where youâ€™ll be able to adjust the volume with the Digital Crown. 
This is not a surprising comment on r/Android. 
As a generality, you should never close apps through the task switcher unless absolutely necessary, like if itâ€™s frozen or something. iOS is generally pretty amazing at RAM and energy management, so force closing apps ends up wasting more battery, as it takes significantly more energy to cold-boot an app if you want to use it again, rather than wake it up from memory. 

Thatâ€™s why there is no close all button. 
Is it less convenient? Yes.  But I honestly think the amount of scenarios in which you literally either have to choose between either playing music or having a dead phone are completely overblown. 

Most times, youâ€™ll have enough battery to play music for an hour or two, especially if you have phones that have great battery life.  Most times, youâ€™ll have an alternative that does the job well enough, like bluetooth, or a USB connection. Other times you wonâ€™t, but those times are more often not the case.  So it doesnâ€™t make much sense to put in two ports. 

But, yeah, I get your point. It was pretty slimy of Google to not even acknowledge the removal and preening and posturing about it last year. At least Apple had the *courage* to get on stage and not only announce they were removing it, but *why.*
Then they can use the added adapter and then just charge later. Or use Bluetooth, or play music and charge the phone through one lightning cable in your carâ€™s USB port, which is what I do. 

Itâ€™s not worth adding a whole other charging port for these rare eventualities.   So sometimes, yeah, I guess some poor wretch will have to listen to the radio like a fucking chump. But for the vast majority of use cases, itâ€™s not that big of a deal. 
Sounds pretty insignificant to me. 

For a manufacturer, it doesnâ€™t make much sense to have two charging ports just to cover rare events like the one you mentioned.  For the vast majority of cases, your average user will not be dealing with these sorts of dilemmas. So it doesnâ€™t make much sense to do something as ridiculous as having two charging ports on a phone. 

Will it be inconvenient sometimes? Sure. But is it so inconvenient that itâ€™ll significantly impact your usage of the phone on a day to day basis? No. And the consumers continued support of the iPhone show this. 

You better start getting used to a headphone jack-free world, because itâ€™s coming. 
I wasn't arguing anything.  The person above asked what "got it right" was referring to.
If they don't die, yeah.  PTSD is known to afflict survivors.  The pain can sometimes be felt months or even years after the initial sting.
[Looks familiar.](https://i.ytimg.com/vi/cdgBcqpmFDs/hqdefault.jpg)
Let's go for the irukandji jellyfish next. 
> *People who are really serious about software should make their own hardware.*

 - Alan Kay 
Cool, I'm glad you like your earbuds.
The fact that there isn't a wire to weigh down the earpiece makes it stay in better.

Not having to worry about wires when taking them out or putting them in adds A LOT to the convenience factor.  Which is part in parcel one of the most valuable qualities of BT earbuds.
Sorry, not all of us are aware of the amount of natural oils or moisture thatâ€™s on our fingers at all times. The only times we figure it out is when Touch ID gives us the error message, making us then wipe our fingers. 

Face ID simply works in more scenarios.  And depending on where you live, wearing gloves is not a rare occurance. 
Unless your hands are greasy or too sweaty, or wearing gloves. 
Being a product of the corporate ad machine has never been so fun!
Because they donâ€™t have enough space to include another port AND internally buffer it with water seals and gaskets.  The use case your speaking of is so insignificant that they thought it didnâ€™t need an entirely new port to address. 


So, having to wrap up and unwind cables each and every time you want to put away or retrieve your headphones is superior design to just plopping down your earbuds into your magnetic case?
Will you still be thinking that every time you have to spend time wrapping or unwrapping the cord from their relatively gigantic charging case every time you want to use them?

With AirPods, you just plop them in or pop them out. 
Hopefully nowhere near anyoneâ€™s wrists. Those things are fashion suicide. 
Yes. 
For as much shit that Apple got with their â€œcourageâ€ comment, what they said was pretty true. It does take courage for a company like that to go on stage, announce they are removing their legacy port, explain *why* they are doing it, and then stand by their decision. 

Google didnâ€™t even try to mention it, hoping people would just not ask any questions.  

Apple absorbed most of the public ire, allowing people to get used to a world without headphone jacks so companies like Google can just slink under the radar. 
The iPhone adapter comes in the box of the iPhone 7 and 8. And if you lose it, itâ€™s only $9, compared to Googleâ€™s $20 dongle. 
No, it has a bump. Thereâ€™s a raised metal ring surrounding it. 
â€œDiet version of Apple.â€

I wouldâ€™ve gone with â€œdiscount storeâ€ version of Apple, but I think yours works better. 
Does it also seamlessly pair your google earbuds to your computer and tablet like AirPods do?
Iâ€™m gonna have to call shenanigans on how the screen to body ratios are calculated, especially on the Galaxy phones. 

If weâ€™re using stock images from the manufacturer to calculate bezel size, thatâ€™s a mistake. Galaxy S8 bezels are larger in real life. Xiaomi was especially guilty of this with their Mi Mix series, often greatly exaggerating the size of the bezels in press shots. 
I wonder if you also appreciate the irony of your own post?
You could cut the condescension in this thread with a knife. 
Siri also could do this for a couple years.  Strange how it took so long. 
Or your $900 Pixel 2 XL!
To be fair, for gender studies though. Which is basically post-modernist garbage. 
Are you TouchWiz personified?  Youâ€™re stuttering all over the place!
Oh look, this comment again. 
Only a stupid person would infer that. 

What this photo *does* imply is that there is a limit to how much reliable depth information can be gained from a single lens. Googleâ€™s solution may be passable at best, but compared to Appleâ€™s dual camera solution, is a discount store version. 
Tell me about it, I practically die when I see people wearing things like the LG Sport unironically at these Google events.  They are absolutely massive. 
Next year on r/Android:

* Dual cameras are so much better. 
I always chuckle when I have to see Google presenters talking about how the back design is â€œboldâ€ and â€œstriking.â€

Like, is that what you call it?
Oh, bless your heart. 
You sure showed me. 
If itâ€™s all software, then why couldnâ€™t the original pixel get all these features via an update?

Face it, Google basically showed off the iPhone 7 from last year. 
Because Bixby blows. GA is actually good. 
As it should be. The right side is closer to the camera, but it still blurred. Itâ€™s simply a lesser degree of blur, which is what you should expect. 

Zoom in and tell me a blur isnâ€™t applied to the right background.  Also, the top part you referred to is more blurred because that section of the background is more recessed than the surrounding foliage. 

You donâ€™t know what youâ€™re talking about. 
The right side was completely blurred. Are you blind?

I donâ€™t think most people notice Android Wear to begin with. 
The photo looks great. What makes the Pixel photo so awful was that the blur was all over the place. It was inconsistent, with voids within blurred areas, and a consistent blur regardless of the depth of the scene. 

Appleâ€™s blur is way more true to life, with the background blur being a gradient, corresponding to the depth information of the scene. As you can see, the background to the right of the subject is gradually less blurred than the left scene, because it is closer to the camera. 

Or look at the bench to the left of the subject. On the Pixel, it is blurred the same as the background, which makes zero sense. Apple intelligently recognizes that the bench and the background are spectate layers and gives different levels of blur to each. 
Google didnâ€™t even bother addressing the removal during their presentation. Apple at least explained why they were removing it. I wonder what Googleâ€™s excuse is. 
What laptop from Apple costs $2000?
If youâ€™re a developer spending that kind of money, might as well spend it on a real laptop and get a MacBook. 
No?

It rendered background blur appropriately and consistently. 
Chrome OS? Google expects us to pay a grand for a glorified web browser?
That takes some real courage. 
Itâ€™s exactly that, except a bit more detailed and miniaturized into the space of a few cubic millimeters. 
Itâ€™s just an awful, cringey commercial. If you want to see a company that advertises to the general public well, look at Apple. 

This abomination is what nerds think casual people are like. 
Appleâ€™s stock photos app already does object and people detection, and pretty well too. And they have the added benefit that you donâ€™t have to sacrifice your privacy to do it, since all object and people detection is performed locally on device. 

Itâ€™s not a new thing. 
Answer to the AirPods? Itâ€™s not even fully wireless or have the seamless connection experience.
Lol. I remember saying the same thing about the side bezels of the iPhone X. Then I got downvoted into oblivion. 
Apple organizes photos in the same way, and also has object detection. The difference is that Apple does this all on-device. 
Apple included the adapter in the box for the iPhone 7, and still does with the 8. 
I guess r/Android is onboard with adapters now?

Funny world. 
OIS? Welcome to 2014, Google. 
Because truly wireless earbuds are better. Especially when they have the seamless connection of the AirPods. 
Arenâ€™t ads supposed to make you want the products?
[The Pixel 2â€™s Portrait Mode is truly awful, my god.](https://cdn.dxomark.com/wp-content/uploads/2017/10/ref2_Bokeh-Outdoor_GooglePixel2-1024x768.jpg)
Nope. Just checked the review again. The Pixel 2â€™s Portrait Mode is seriously that terrible. 
Except just using a phone would be much easier, as the phone speaker can handle both parties, whereas the earbuds are only relevant to one party. 
They also regulate the Bluetooth connection, squeezing out a bit more range and battery life compared to its contemporaries on the market. 
Yeah, $160 dollars for wireless earbuds that arenâ€™t even wireless. Totally reasonable. 
What an awful presentation. 

* Over-priced everything. $400 speaker, $100 pen, $1000 chromebook, $160 not-even-wireless wireless earbuds, $850 phone, $250 spy camera. They think theyâ€™re literally Apple. Hahaha. 

* Half baked software and hardware that we all know wonâ€™t go anywhere, thinking about you Google Lens and Clips. 

* Caused me to only speak â€œmachine learningâ€ in my subconscious 

* Completely ignoring the lack of the headphone jack. 

**NO COURAGE**

Edit: People have given Apple so much shit for the â€œcourageâ€ comment. But Google didnâ€™t even mention the missing headphone jack. Apple actually was courageous because they had the balls to come out onstage, address it, and explain *why* they removed it. 
Seems so. 
So, a literal spy camera?  As if Google canâ€™t  treat us like products any more if they wanted to. 
No, but youâ€™ll pay the same price for them anyway!
So, a worse version of AirPods?  
Really though. I doubt their single camera solution will be anywhere near as good as dual camera solutions. 
Itâ€™s just a coincidence that Portrait Modes have become so popular only right after the release of the 7 Plus. 
Glad to see Samsung and Google so impressed by Appleâ€™s Portrait Mode that they had to make their own. 
ARKit still looks way better. 
Google execs who want to make their new features appear more useful than they actually are. 
???
Oh, youâ€™ve listened to both?
Appleâ€™s speakers are $350 and the pencil is $100. You didnâ€™t think too hard about this one, did ya?
$400  speaker, $100 pen, $1000 laptop.  Do they think theyâ€™re Apple? Hahaha. 
So a HomePod ripoff?
Very important indeed. 
Thereâ€™s a pretty significant difference. Besides working a lot faster, itâ€™s much harder to spoof. Which is kinda important for a security measure. 
What the hell are you talking about? Neither of those phones have 3D facial mapping technology. 

No phone before the iPhone X has. Youâ€™re spouting pure horseshit. 
Face ID isnâ€™t just software, itâ€™s an IR camera, an IR flood illuminator, a regular 8mp camera, and an IR dot projector. All this in order to create a 30,000 point 3D facial model of your face instantly and in any lighting condition. 
Whatâ€™s wrong with the notch? You get more screen real estate and get as close to completely bezeless as is feasible. Iâ€™d rather have more screen than less screen. 
Why is it rude to question peopleâ€™s religious beliefs, but not their political beliefs?
Thatâ€™s not an answer. 
ðŸ‘
K 
You asked whatâ€™s wrong with the rice. Why on earth would you ask a question to which you apparently already know the answer to?

> Maybe just learn when to shit the fuck is next time.

The next time you are feverishly replying to a post about sushi rice, maybe calm down a bit and check your spelling. 
Not enough of it. 
ðŸ˜¢
It takes a lot to make me cry, sunshine. 
Why do they need a wake up? Sales are high, the iPhone 8 is a great phone, and the iPhone X is the design revamp everyoneâ€™s been clamoring for. 
[https://daringfireball.net/2017/09/iphone_x_event_thoughts_and_observations](https://daringfireball.net/2017/09/iphone_x_event_thoughts_and_observations)
Hopefully this turns out better than the similar post on r/Android did during the Apple announcement.  Smug android users were practically tripping over themselves whining about how Android had everything first and that more power doesnâ€™t matter. 
Yes. 

Reports from Jon Gruber state that Apple decided a long time ago, very early during the iPhone X development, that FaceID was the future and abandoned any tentative plans to incorporate the fingerprint sensor under the glass. 

From the previews at Appleâ€™s hands-on media event, it seems to be the right way forward. Itâ€™s extremely fast, and works at a lot of angles in any lighting condition, not too mention harder to spoof than Touch ID. 

Will there be some cases were itâ€™s not as convenient as Touch ID? Sure, but the same goes for Touch ID as well. Fingerprint scanners donâ€™t work while wearing gloves and even the best ones on the market still have trouble with sweaty and/or greasy fingers. 
To add to that, Bonedrewd's actions become a bit more rationalized when you understand that [Spoiler!](/s "his consciousness has been split and distributed onto so many people that it barely has any resemblance to his original mind, getting more and more detached each time he splits his soul.")

Not to mention that his goals are fairly noble, in that [Spoiler!](/s "the results of the science being conducted at Idofront is to prepare humanity for the impending apocalyptic event that appears to happen every 2,000 years, which the deadline is fast approaching.")
Shinsekei Yori (From the New World)

Similar in that while the premise seems innocuous at first, there lies a persistent foreboding and ominous atmosphere about the true nature of the world.  Quickly gets pretty dark.  
Gross.  Is that cream cheese?
Done.
The iPhone SE probably sells more units than some flagship android lines.  There's definitely a market out there.
Have to agree with this.  

Current solutions aren't even "wireless" to begin with.  If you have to leave the phone set down on small plate, there's nothing really wireless about it.  You can't pick it up and use it without losing the charge.

Until we have ranged wireless charging (not holding my breath) the feature is more a gimmick than anything.
Bezels.  People here are honestly circle-jerking over screen-to-body ratio differences of 1-2%.
None of those are really beneficial to the overall user experience.

Wireless charging, while marginally more convenient in some ways (and less in others) doesn't really affect the overall, average user experience of using the phone.  This is the same thing as water proofing.  Android manufacturers liked to tout their waterproofing as some sort of advantage over Apple, probably because there weren't many *real* areas they could say the same. Waterproofing, while a nice safety net, doesn't impact everyday user experience of actually using the phone.

So while Apple continues their lead in areas that *actually* matter, they can quietly tick off the boxes of other less important features that Android manufacturers won't shut up about, like waterproofing and wireless charging.

You know what actually matters for user experience?  Stability, unmatched performance, 3D Touch, the Taptic Engine, stereo speakers, quality 3rd party apps, etc...
Because that's where the battery is.
* Video is generally better on iPhones, especially stabilization.  Most other manufacturers rely too heavily on EIS, causing backgrounds to turn into jello.  This is noticeable on Galaxy phones.

* Windows Hello is not like FaceID at all.  Windows Hello in phones were only limited to easily spoofed 2D facial recognition or iris scanning.  FaceID works on an entirely different level, and even provides bonuses, like advanced facial tracking.

* For long sustained tasks, like youtube watching, iPhones don't do as well.  But who the hell watches videos for 5 hours straight?  In overall day long use, iPhones are competitive, if not better than many android phones due to superior standby battery management.  And they do all this while still having significantly smaller batteries.


When resources are delegated to those based on need rather than contribution or value, that tends to happen. 
> There have been *communist parties* that have tried to guide different countries towards communism but the way they do it usually gets a lot of people killed, even though they do succeed in massively increasing production

Wasnâ€™t the issue in Maoâ€™s China that they were running out of food and iron production took a massive dive after a good first year?

30-50 million people died of starvation. Whatâ€™s up with that?
Capitalism and western civilization have also produced a world where we can have access to electricity, water, and food at any time of day with no serious concern of any of those running out. 

You can thank all of the worldâ€™s technological advancements and economic mobility to capitalism. 
That doesnâ€™t change the fact that the Marxist ideology has been responsible for numerous attempts to establish communism and have always ended up miserable failures with millions of people either starving to death or executed at the hands of authoritarian regimes. 

Marxism almost seems to predicate itself on inevitable resource shortages and authoritarian governments. 
Because Appleâ€™s bezels actually do things. You have a front facing speaker and a front fingerprint scanner. 
Well, yeah, Apple did introduce this specific kind of design language first with iOS 7.   Metro is its own thing, the only thing the two have in common is the abandonment of heavy textures. Other than that, the UI is organized very differently. 

Also, you still havenâ€™t explained what paradigm shift happened with iOS 8. 

Or are you just arguing from an Android centric point of view, and youâ€™re desperately trying to make some completely unwarranted kind of â€œAndroid and Microsoft did it firstâ€ point here?
>While earlier iOS versions used skeuomorphism precisely to convey hierarchy via depth. Element that sticks out from the background via highlights and shadows and real-world-ness: clickable.

That's not hierarchy.  At least not in an organizational sense.  That was skeumorphic, grabs attention and lets people know what's clickable or not based on analogies with real world objects.  

iOS 7 changed the paradigm in the sense that rather than an abstract digital world being given context through skeuomorphic materials that hint to the users what can be done, iOS 7 let users know *where they are* in the digital plane with a design language that literally has depth to it.  Layers were visualized through focus blurs and frosted materials that allowed content underneath to bleed through.  

>iOS 7 merely translated that into a visual style that did away with lense effects and highlights and dropshadows and perceived haptic properties

If anything, iOS 7 was more "haptic" than iOS had ever been.  Liberal use of sweeping and zooming animations, with actual weight and physics given to things like the dropdown shade, iOS 7 needed more physicality because the entire design language was about how things operate in space.  

>It wasn't a paradigm change. It was merely tagging on to a visual style that was already gaining popularity across operating systems and apps.

So tell me about the supposed "paradigm change" that happened with iOS 8.  I don't really think you know what you're talking about.  Again, a design language is more than how individual elements appear, aesthetically, but the guiding principles in how things are organized and express clarity.
Samsungâ€™s video stabilization has always been awful.  Samsung gets too aggressive with the software and backgrounds suffer heavily, looking like theyâ€™re made of jello. 
Thatâ€™s incorrect. App extensions really changed the paradigm of iOS, which was introduced in iOS 8. But the real UI change which introduced hierarchy via depth came with iOS 7. 
UI is much more than visual elements. Flatness in icons is just one aspect. Marerial design, as a cohesive design language, didnâ€™t really make its debut until 2014 with Lollipop. 
Yeah, because your performance will rise up as Apple releases a better version of iOS 11. 
Are we still in denial about LagWiz taking the shit as time goes on?

Thatâ€™s very interesting. 
Yeah. For some users, iOS 11 has made their phones about as stable as your average Galaxy phone 4 months after buying it. 


I donâ€™t know if anyone followed Metro. Metro was creating order and hierarchy through bold typography, geometry, and colors to attract different degrees of focus and â€œimportance.â€  This design is actually flat. 

iOS and material design are only flat in the sense that individual elements generally abandoned the use of heavy textures and skeuomorphic properties.  But the way those individual elements function and are organized are anything but flat. 

iOS 7 was officially unveiled early June of 2014, and released to the general public in September 2014.  Material design was then officially rolled out with Lollipop in November. If anything, material design followed iOS. 
iOS isnâ€™t really â€œflat.â€  Flat is not the opposite of skeuomorphic. With is heavy use of frosted glass, Gaussian blurs, and parallax motion, iOS is organized through a perception of depth, which creates hierarchy and a sense of context. Thatâ€™s essentially the opposite of flat. 
Yeah, less precise. Like with the iPhones, itâ€™ll probably be able to produce good enough depth maps for objects within a close enough distance for things like portrait mode. 

But you will not be able to determine enough accurate depth data to do things like plane detection. Itâ€™s simply not possible. 
Except they never said that in your source. For all we know, they are only referring to their active depth sensors for use in AR. 

Itâ€™s literally not possible to do reliable plane detection with only standard stereo cameras.  Youâ€™d get essentially the same data as just one regular camera due to a lack of parallax.  

You should understand what parallax is and how it relates to depth information.  
You can create a depth map, just like the iPhone 7 and 8 Plus do. 

But creating depth maps via stereo cameras only works when thereâ€™s sufficient parallax to separate subjects from their backgrounds. Thatâ€™s not enough for AR tracking.  When you scan for AR, you are scanning the floor and walls. There is no parallax for floors and walls at those kinds of distances.  Plane detection is going to be very difficult with stereo cameras. 


Dedicated IR cameras are needed for depth information. If you just have two regular cameras, itâ€™s not going to do you that much good. Depth information gained from having two camera lens is created by the subtle parallax of the focused subject and the background. Such a distinctive parallax doesnâ€™t exist if all youâ€™re scanning is background, which is what youâ€™re doing when your establishing a scene for world tracking. 


> I am not doubting that faster CPUs are better for AR tasks all I am saying is that current CPUs are more than capable of handling AR tasks. 

So then why are you here arguing? No one put that in dispute. Current CPUs are more than capable of handling all sorts of things, they have been for many years.  But they still noticeably improve a lot of things too. 

Thatâ€™s all my point was, that AR is an area that is noticeably improved by better CPUs, especially as theyâ€™re getting more complex.  

> Case in point the new Android One phone from Xiaomi has dual cameras and does portrait mode which essentially is creating a depth map. 

Dual cameras cannot ascertain depth information for the purposes of AR. The amount of parallax you get at those distances wonâ€™t be doing you any good.  Thereâ€™s a reason Apple style portrait modes only work at specific distances. 


Even basic AR tasks like the ones you mentioned can still be improved significantly with faster processors. Determining scenes and locations even when the users looks away and moves around is pretty CPU intensive stuff, especially without a depth camera.  Your set up process for AR will go much smoother with a fast CPU. 

And of course, it matters even more so when AR apps start getting more complex and graphically demanding. 

This is an area CPU development will be helpful in. Deal with it.  
AR is an inherently CPU intensive process.  A strong CPU will always help you, gaming or otherwise. 

The fact that AR can be done on older, weaker devices doesnâ€™t mean they run well or couldnâ€™t run better and more efficiently. 

This is the whole point. A strong SoC will see immediate benefits in AR applications. 
The point is that AR is very CPU intensive, especially without dedicated hardware and utilizing world tracking. Pushing advanced 3D images on top of that, along with a game engine is even worse. 

Therefore, since this is a CPU intensive type of task, more power SoCs would be an obvious benefit. If you agree with that statement, then what are you even doing here?
It really doesnâ€™t. 

The 652 devices would not be able to attain that level of graphical fidelity, plus retaining all objects when zoomed out, plus world tracking, all while maintaining 60fps. 

I donâ€™t see it happening. 
Have you seen The Machines?
I really donâ€™t see any 652 device running something like The Machines well at all. 
AR, power users, gaming, device longevity. 
Seems needlessly pretentious. 
What are you talking about? The iPhone beat out the Note is almost every app load. The only reason it â€œlostâ€ overall was because iOS 11 is bad at RAM management. 

You know thereâ€™s something fishy going on when the iPhone 7 running iOS 10 beats the Note 8 in these same tests. 

Also, you really revived a two week old thread? Hahaha. 
Screen to body ratio and bezel size are not the same thing. You can have a greater screen to body ratio even if you have thicker bezels so long as your screen size is large enough. 
And the bezels on the top and bottom of the X are thinner than the Note 8.  What's your point?

When you price your phone that high with flagship specs, you are competing with the iPhone whether you like it or not, especially when you are setting yourself up as a showcase for Android. 
The difference is that itâ€™ll get fixed in a software update pretty soon. 
Selling 1-2 million in a year while your rival sells 40 million in one quarter is not "ok."
I don't know, it's all about context.

Google went up and announced an iPhone competitor, especially at the level of pricing they thought they could get away with.  And what happened?  Apple sold 20-40x more in a one quarter than Google will have in an entire year.
Again, it's due to iOS 11, not the hardware.  We know this to be true considering that the iPhone 7 easily beats the Note 8 in these kinds of tests.

And it's not just video editing.  I just used video editing as an example were animation speeds are not a bottle neck.  

Frankly, I think the Galaxy phones could really benefit from this kind of power.  Maybe then they'd be able to handle simple animations and scrolling without dropping frames everywhere.

Well, we know itâ€™s not because of the Actual hardware, since the A10 powered iPhone 7 handily best the Note 8 in similar tests. 

Ina few months when iOS 11 is updated, the iPhone 8 and X will easily beat the Note 8. We all know this. 
Weâ€™re having a discussion about phone speed and power.  Your question is irrelevant.  
iPhones 7 and 8 both significantly beat the Note 8 in tasks that are actually CPU intensive, like video editing. 

Areas where the Note 8 â€œbeatsâ€ the iPhones, like small applications are solely due to animation speeds, not CPU capability. 
Or just say octopuses. Octopi makes no sense. Itâ€™s not a latin root word. 
Remember, the iPhone 7 Plus  beats the Note 8 by a significant margin in these tests. 

This is just a case of bad optimization of iOS 11.  How else would you explain it?
Not octopi, octo*puses*. 

Octopus is an English word, therefore it is pluralized like English words. Furthermore, itâ€™s doubly wrong because octopus isnâ€™t a latin word to begin with, itâ€™s Greek. So even if we were to treat it as a foreign word, itâ€™d be octopodes, not octopi. 


How the hell well most people be able to remember how much pressure to input among 6 different levels?

Sounds like pretty terrible UX. 
Thatâ€™s because squeeze functionality **is** a gimmick. At least in its current implementations. Itâ€™s an over-engineered solution to a problem that can be more easily solved with a dedicated side button. 

One click for one action, two clicks for another, hold for more actions, and maybe even triple click for another. 
Apple has never made its own displays. Whatâ€™s so â€œgoodâ€ about this?
Because every September on Reddit after an Apple media event, insecure android users are compelled to shit on everything Apple related. 

Itâ€™s normal. 
Of course a Samsung spokesperson would say that. 

Hereâ€™s another source that explains what it really takes to break through the security. 

[http://bgr.com/2017/05/23/galaxy-s8-iris-unlock-hacked-video/](http://bgr.com/2017/05/23/galaxy-s8-iris-unlock-hacked-video/)

> Whatâ€™s particularly interesting is that the photo itself doesnâ€™t even have to be particularly high quality, and the image used in the demonstration was shot from several feet away.

> Then, the photo is cropped and sized so that the iris is roughly the size of a real-life human eye. After that, a contact lens is placed on the printed photo, and the Galaxy S8 instantly recognizes it as being a â€œrealâ€ human eye and unlocks the phone.

Thereâ€™s even a video showing how the process works. 
Donâ€™t embellish. 

The original iris scanner hacked didnâ€™t use a high resolution photo, going as far to say that the photo doesnâ€™t need to be of any great quality. Then you just lay disposable contacts onto the picture and then point the phone at the picture. You donâ€™t need to print anything on contacts. 

[Source](https://www.theguardian.com/technology/2017/may/23/samsung-galaxy-s8-iris-scanner-german-hackers-biometric-security)
Not for iris scanning. 
Except this year it seems. Most camera reviews seem to be unanimous in the fact that the iPhone 8/8 Plus have better cameras and image quality than the current Galaxy and Note phones. The iPhone X is probably going to be even better due to increased aperture and added OIS for the telephoto lens. 
Most of it is software.  Computational photography will be driving the smartphone camera industry more than image sensors at this point. 

Appleâ€™s ISP (dedicated chip for image processing) is pretty killer in the A11 Bionic chip.  This is what handles HDR processing and scene optimization. 
Samsungâ€™s iris scanner can be fooled with a photograph and a pair of disposable contact lenses. 

So much for security. 
Which really says a lot about the shit quality of android as a platform
More like, thank god my phone can perform simple animations without dropping frames everywhere. 


There a few things that benefit from the insane horsepower:

* Future proofing. When the iPhone X is still getting software updates 4 years from now, your average Galaxy will crap out at the 2 year mark. 

* Portrait Lighting. Appleâ€™s Portrait lighting feature, which operates in real time, features computationally very heavy algorithms. And this is all done instantly in real time on the subjects in your viewfinder. 

* Augmented Reality. 

* Overall gaming performance 

* Battery efficiency via race to idle. 

* UI stability

* Photo/video editing.   Like those beautification apps that are super popular. 

* Instant rendering of a 30,000 point 3D model of your face for Face ID and the 3D face tracking APIs. 
Except in terms of performance and apparently, according to recent reviews, cameras. 

The A11 Bionic chip is about twice as fast as whatever garbage Samsung managed to stuff in their thousand dollar phone. 
You could just close your eyes or look somewhere else Face ID requires active attention. 
r/Android didnâ€™t seem to get the memo when the pixel was the highest rated. You guys couldnâ€™t stop jerking yourselves off. 
???
So you think Face ID will actually be getting *worse* from the 11.1 beta they were running?

Give me a fucking break.
Essentially, yes. 

The only difference is they were probably running a beta of iOS 11.1, and will ship with the final version. 
Are you assuming beta devices for media events will be *worse* when it officially goes on sale for the general public?

You guys are grasping at straws. 
Glad you keep ignoring the argument. It makes it easier for me. 
> The printout needs to be a specific size because the next step requires you place a contact lens over the image. According to the group that discovered this workaround, this gives the iris scanner the curvature it's expecting from a real eye, while you present it with a flat, 2D image.

Wow, such security!

ðŸ˜œðŸ˜œðŸ˜œ
Uh no. No need to put anything on anyoneâ€™s actual eyes. 

You literally print out a copy of someoneâ€™s face, which doesnâ€™t even need to be high quality, then literally just get some contacts and place them on the paper where the iris is. 

Itâ€™s like you didnâ€™t even read the article but were instead feverishly looking for anything you could spin in your favor.  You really are predictable. 
Itâ€™s actually pretty easy. Most digital cameras have a night mode. All you would need to do is take a zoom picture of their face from far away. Remember, the picture doesnâ€™t even need to be the good quality. 

Print that out, put some disposable contact on their eye, and show it to the camera. 

Done. 


Like thereâ€™s any difference. Itâ€™s an inherent problem with the technology itself. 
[https://www.forbes.com/sites/ianmorris/2017/05/23/samsung-galaxy-s8-iris-scanner-hacked-in-three-simple-steps/](https://www.forbes.com/sites/ianmorris/2017/05/23/samsung-galaxy-s8-iris-scanner-hacked-in-three-simple-steps/)
Yeah, because Iâ€™m saying an expensive SoC is the sole reason for high production costs, not many factors put together like a sane person. 


> And back to this again. Nothing you can put a price on or even prove is true, like for the camera. 


 I can quote you on the plethora of threads where youâ€™ve been doing damage control for Qualcomm that â€œAppleâ€™s cores are much larger and much more expensive.â€
Ahahaha. 

The quality of the arguments in this sub is really inspiring. 
Glad to know Iâ€™ve affected your life enough for you to continue responding to me. 

Youâ€™re new to this stuff, arenâ€™t you?
Did I hurt your fee-fees?
No, itâ€™s not a software trick. Samsungâ€™s copy of Appleâ€™s Portrait Mode works that same way Appleâ€™s version does, using two lenses to determine depth, then isolating the subject from the background to great realistic blur. 

The Portrait Mode selfie on the iPhone X works in a similar way. Using the TrueDepth camera system, which works much like an Xbox Kinect, it can determine depth and pluck out the subject from the background. 

Single lens bokeh features have existed for a while now, but theyâ€™re never as good as what can be done with dedicated hardware. 
Sure, you may not care for better gaming, UI smoothness, overall speed, Portrait Lighting, speakers, and cameras, but those are certainly enough areas that iPhone is objectively better in to render your previous statement as silly and fanboyish. 

> Reality Apple really has nothing that makes it superior. 

Well, besides all those things I pointed out. I guess. But if youâ€™re gonna complain about fanboys, look in a mirror. 
The processor. 

The A11 Bionic is the most advanced SoC on the market. Instant real-time rendering of simulated light on subjects takes a lot of horsepower. Current snapdragons and exynos processors arenâ€™t capable of those things yet. 
It has been proven at the Apple hands on media event. Worked flawlessly in most previews. 

> Yes the processor is faster,  but is that how one uses a phone daily?  Opening and closing apps?

Well, opening and closing apps has more to do with storage speeds, which funny enough the iPhones still have the edge on Samsung. 

Other areas where the end user would notice performance improvements would be:

* Instant Portrait Lighting. Real time rendering of shadows and light sources on your subjects is pretty advanced and would require powerful processors to operate in real time. 

* Much better AR world tracking and graphics rendering. 

* Better game performance, especially AR games. 

* Faster photo and video editing. Like those super popular beautification apps. 

* Future proofing. For Godâ€™s sake, the iPhone 5s is still being supported. 

* Smoother UI. iOS dedicates an entire thread at all times to UI animation. Thereâ€™s a reason iOS has been smoother than TouchWiz garbage for a while now. 

Thereâ€™s other areas in which performance will be noticeable, but I think you get the idea by now. 

Also, hereâ€™s some other areas we know the new iPhones are better:

* Camera.  Reviews are pouring in, and most are in agreement, the 8 Plus out performs the Note 8 in most areas and overall picture quality. DxO confirms. And the iPhone X will be even better due to decreasing aperture and OIS on the telephoto lens. 

* Haptic Feedback. The Taptic Engine is a much more sophisticated vibration motor than the one found in the Note, with iOS incorporating it throughout the user experience. 

* Speakers. Stereo, front facing, significantly louder with deeper bass and clearer mids. 




Nothing? Câ€™mon now. 

Besides better facial recognition, take a look at this. 

[iPhone 8/X faster than i5 2017 MacBook; leaves Galaxy S8 for dead](https://9to5mac.com/2017/09/22/iphone-8-geekbench-test-scores/)
Who cares about options when most the options are crap?  Samsung face unlock can be fooled by a photo, iris unlock can be fooled by a photo + contact lenses, and I wonâ€™t even start with voice pattern recognition. 
No wonder this garbage sub is no longer default. 
And support themselves how?
DxO revamped their review method. When fixed for adjustment, S8 individual metrics would be lower as well. 
Have you seen the A11 Bionic?
â€œB-b-but we donâ€™t even need all this power!!  Itâ€™s not like my Samsung needs all those frames!â€
The women I know arenâ€™t really in nerd culture.  I know guys can be creeps, but not on this severe autistic level. 
Insecure much?
Youâ€™re making my skin crawl.  Stop it. 
When Samsung phones stop crapping out for the average user a year after purchase, then we can start talking about user experience. 
Is it the 835 being more efficient, or android phones having substantially bigger batteries?
They most likely probably could pull off 4K at 60fps if they wanted to, but pulled back due to other limitations. I donâ€™t think either the snapdragon or exynos chips are capable of doing 4K60 in tandem with HDR, EIS assist, and other post-processing effects that are normally used. 
Samsung could sure benefit from some of that power. Maybe then they could finally make a phone that wonâ€™t drop frames for every animation. 
What kind of fucking moron would actually do that in real life? I refuse to accept that.  It canâ€™t be. 
When an iPhone 8 only takes 40 seconds to export a 4K video file while a Galaxy Note 8 takes three and a half minutes, which one do you think is actually using more battery?


We should see something similar with the inevitable A11X chip thatâ€™ll probably be thrown into the new iPadâ€™s, where it benefits from higher power draw and more space to cool. 
Things Iâ€™ve heard from Kathy Griffin and some personal friends who work in the industry, make her out to be just a nasty, mean girl whoâ€™s rude to everyone. 
Of course, cricket chirps from r/technology. 
That first example is real r/thathappened material. 
Is also, apparently, one of the biggest cunts in Hollywood. 
He asked a question, no one answered, so I decided to. 

Piss off. 
This is the only right answer. 
The faster you complete the task, the faster you can return to idle. 

Generally, Appleâ€™s A-Series chips have been pretty good at avoiding serious throttling while sustaining performance. 
Yeah. A11 equipped devices are generally exceeding 4000 in single core and 10K for multicore in Geekbench 4. 

[Source](http://www.techrepublic.com/article/iphone-8-crushing-samsung-galaxy-note-8-in-benchmark-tests-for-speed-performance/)
Yeah, that could work.  I guess it would work better as an extra viewing mode, rather than basing the gameplay around it entirely.
I'm not going to be taking design lessons from out of touch nerds on r/technology, thanks.
Itâ€™s not bullshit. Ever since the introduction of Bluetooth LE, itâ€™s overall effects on battery life have been so small as to be insignificant. 

Now, itâ€™s important to understand the differences between an inactive, but powered BT radio, and an actively streaming BT radio source. 

Simply having the Bluetooth radio on while itâ€™s not actively streaming information to another device, has effectively zero impact on your battery life. This is not debatable. 

What does impact your battery is when Bluetooth is actively doing something, like streaming audio to wireless headphones. 

But the point is, disconnecting from a device and turning off Bluetooth completely creates no significant difference in battery life. 

You guys are simply wrong, get over it. My god.  
Such as?
Modern Bluetooth radios consume so little energy as to be insignificant. 
It might be interesting to look at a virtual theme park for a bit, but I canâ€™t see how youâ€™d make a good game out of it. Wouldnâ€™t it get tiring holding up your phone all the time?  Scenarios in RCT can be pretty time consuming. 
I guess I should've specified that I am referring to Bluetooth LE and up devices.
>The Note8 can already do this. It displays the blurs in the viewfinder even before you take a photo.

You don't know what you're talking about.  I'm not talking about Samsung's copy of iOS's portrait mode, I'm talking about iOS's new Portrait Mode *lighting*.  In real time, you can change and view different portrait style lightings on your subject's face.  This kind of real time rendering of light is very CPU intensive.

>Where are you getting that the iPhone X's 4K 60fps recording uses HDR as well?

HDR is no longer optional on the new iPhones.  When filming, HDR is on by default according to The Verge.

>Nothing new. Snapdragon chips had support for this a long time.

Software based, sure.  Not hardware based, and certainly not without some hacks. While the 800 series snapdragons can technically support it, I'm not sure of any android smartphones that official allow it.

>99.9% of iPhone/Android users don't edit videos on their phone. Editing video on a phone is just a joke. For a very small niche market, your argument will be true, but for the vast majority of people nope.

I think you're a bit out of touch.  Small touch ups to videos and especially photos are incredibly common.  Beautification apps are also one of the most popular apps in both app stores.  "Editing" goes beyond Lightroom and Photoshop, you know.

>Probably negligible considering Snapchat's AR works pretty well even on very old devices. We need more objective data.

Compared to Snapchat AR using iOS's TrueDepth camera API, previous iterations of Snapchat's AR functionality look several generations behind.  Demos at Apple's hands-on media event showcased this pretty well.  Or just look at Apple's other functionality that uses 3D face tracking, Animoji.  That type of sophisticated, and 1:1 face tracking is impossible on older hardware with just one camera.  That face tracking also is pretty CPU intensive.

>99% games won't even have a difference. Don't be silly.

Uhh.... yeah, they will.  Especially advanced AR games Apple is really pushing for.  

>That's honestly less of a hardware issue. Most of smoothness and fluidity is dependent on system animations and how they're laid out. iOS priorities them. This overall shouldn't matter since Android is still way faster at getting things done.

Please, even the Note 8 gets noticeably laggy just navigating around the UI when there are heavy tasks being performed. 

>Light apps not really. In fact, android loads lighter apps much faster than iOS.

No, they don't.  In cases where apps are so small that storage speeds don't matter, the only bottle neck is animations. 

>Yes, maybe few seconds of edge on the iOS. But don't tell me "my iPhone is better because it loads games few seconds faster than yours". That's just a retarded argument.

If all else is equal, then yes, my phone is better than yours because mine is significantly more responsive when I'm loading apps, which is a big part of a smartphone experience. 
That solution seems a bit overly complicated and not something iOS is known to be. 

iOS 11â€™s issue is good enough for the majority of users, as needing to completely shut off Bluetooth is generally unnecessary. 
Ok? Good for you. 
How about no?

Manufacturers need to stop pushing out half-baked gimmicks, and start focusing on what can improve their experience of the end user in significant ways. 

I think weâ€™re all tired of manufacturer pumping out novel crap just so they can tick off a box on their marketing sheet. 
What do you expect? This is r/technology. 
Not even arguably at this point. 

Appleâ€™s A-Series of Mobile SoCs are the best chips on the planet.  There is no debate to be had at this point. 
Inactive, but powered on Bluetooth radios consume so little electricity as to be completely insignificant. 
Exactly. 

This is a much better solution for the end user.  More often than not, people just want to disconnect from a device, not shut off BT completely. 
Thatâ€™s pretty shortsighted. 

Hereâ€™s a few things the iPhone 8/X can do with this kind of power:

* Real time computational Portrait Mode Lighting done near instantly. 
* Aforementioned 4K 60 recording with HDR, EIS assist, and neural net powered scene optimization. 
* Hardware HEVC encoding/decoding, reduces files size significantly. 
* Significantly faster video/photo processing and editing. 
* Smoother AR world tracking and graphics rendering. 
* Better overall gaming performance. 
* Near instantaneous analysis and rendering of a 30,000 point face model for Face ID and face tracking APIs. 
* Overall UI stability and smoothness, which we all know things like TouchWiz has issues with, especially when under load. 
* Battery efficiency via race to sleep. 

r/Android only whines about devices being â€œgood enoughâ€ because theyâ€™re losing substantially in this race. 


Letâ€™s also not forget to mention that higher storage speeds is something immediately noticeable for user experience, as apps and media load much faster. 
What region of the US are you in? Iâ€™m from the Los Angeles area, and itâ€™s worked fine for me since the betas.  
Thereâ€™s no option to enable or disable it. It just appears when youâ€™re using turn-by-turn navigation. 
â€œAndroid doesnâ€™t lag!â€

â€œX & Y is a hot mess.â€

â€œThatâ€™s not Android!!!â€

â€œX & Y run just fine on iOS...â€

â€œGGGAAAHHHHHH!!!â€
Except the Geekbench scores.  You know, the literal charts of data. 
[iPhone X more than double the performance of Note 8; generations ahead of all Android competitors](http://bgr.com/2017/09/18/iphone-x-specs-vs-galaxy-s8-iphone-8-more/)
The iPhone 7 has significantly better CPU performance and storage speeds. 

The iPhone 8 and X will have DOUBLE the performance of the Note 8. 
It depends on how you normally use your phone. 

If youâ€™re a fan of taking scenic pictures of cities and landscapes, ultra-wide angle is for you, as it gives a better sense of scale and space. 

If, however, you value taking pictures of people and objects, then telephoto is the better option. Generally speaking, telephoto lenses will provide better details in faces and fabrics. Not to mention that itâ€™s superior for taking portrait photographs with. Portrait shots taken with wide angle lenses usually turns your subjects into eggheads due to the warping of the edges. 

For most instances, I think telephoto is the better option for dual camera setups. Better for close up details and portraits, while most â€œnormalâ€ lenses on smart phones are pretty wide to begin with. 
Perfect for r/Android!
Itâ€™s not old. Itâ€™s simply profits of one quarter, while yours is of the year overall. 

The fact is, the high tier market is what actually matters, considering practically every manufacturer, except for Apple and Samsung take a an overall loss every year. 


[Apple takes 92% of industry profits as of February 2017](http://www.investors.com/news/technology/click/apple-took-92-of-smartphone-industry-profits-in-q4/). 

[iPhone gains market share in 7 out of 9 key markets](https://9to5mac.com/2017/01/11/ios-market-share-kantar/)


My initial premise was that among the *flagship* tier, which is the only market Apple competes in, they sell more than any one else combined. 

> They do. They represent a huge sum of money all together, and an installed user base that is exceedingly attractive to developers and accessory manufacturers. 

If this is the case, why does Apple make more than 85% of the entire smartphone industryâ€™s profits?
Considering that iPhones with 2gb of ram outperform Androids with equivalents amounts of ram, or in Samsungâ€™s case, double, I donâ€™t really see the need for more RAM. Especially when more ram means more power consumption, as it is always â€œon.â€

If anything, iPhones are the least likely to feel slow after one year. Thatâ€™s typically an android problem, especially with Samsung phones. 
Not of their flagships. The vast majority of phones Samsung sells are garbage low tier phones. 
The only thing thatâ€™s two generations behind is Samsung, at least in CPU performance. 

[http://m.gsmarena.com/a11_in_iphone_x_crushes_top_android_competition_in_geekbench-news-27302.php](http://m.gsmarena.com/a11_in_iphone_x_crushes_top_android_competition_in_geekbench-news-27302.php)
â€œInnovativeâ€ is an incredibly overrated term.   Itâ€™s easy to be innovative. Itâ€™s hard to genuinely improve on what came before. Being novel and unique isnâ€™t hard at all.

Is Android, as a whole, more innovative? Sure. But thatâ€™s only because Android, and most manufacturers, are sloppy and marketing obsessed. 

Samsungâ€™s facial recognition can be fooled by a picture and canâ€™t be used in the dark. Itâ€™s essentially **useless.** Appleâ€™s Face ID works in all lighting conditions, and is more secure than a fingerprint. In this example, Samsung only cares about checking an item off the â€œinnovationsâ€ checklist for their marketing department while Apple honed the technology and software to actually make a great user experience. 

So I really donâ€™t buy this whole â€œinnovativeâ€ argument.  And even so, itâ€™s not like Apple isnâ€™t producing tech that continues to inspire other manufacturers.  Apple revolutionized security with Touch ID, finally making fingerprint scanners *usable*.  Appleâ€™s 3D Touch and Taptic Engine are slowly making its way into Android land. And Appleâ€™s telephoto lens-powered portrait modes are one of Samsungâ€™s highlight features for their new flagship, errr... I mean â€œLive Focus.â€

If Apple wanted too, Iâ€™m sure they could show off their secret R&D projects like Microsoft and Google. Iâ€™m sure they could ship premature features in their phones like Samsung. 

But they have a bit more self-respect than that. Shipping a good product that actually works is more important than trying to impress nerds with â€œinnovativeâ€ tech that barely works well. 
Afraid not. 

While this software is able to produce a 3D image out of a single photo, itâ€™s doesnâ€™t accurately map the actual face. All it does it make a normal looking 3D approximation. For any accurate mapping, you need a second angle or some extra data, like from IR sources. 
Considering that the iPhone X has double the CPU performance than the $930 Note 8, that sounds like a value to me. 
iPhones typically do more with less RAM than Android phones.  Android is significantly worse at RAM management, especially when it comes to app killing. 
They really donâ€™t.  Take all the sales of high end flagship phones  from Android manufacturers and combine them, you still wont get close to Appleâ€™s annual iPhone sales. 
By default, you Face ID only unlocks when you actively look at the display. Although, this active attention feature can be disabled for the visually impaired. 
While Apple definitely is making increasing revenue year over year on â€œservicesâ€ and advertising, they are still primarily a hardware company. 
Holy shit, people honestly put up with Android?
You think humming to unlock is banal? 
How does it become limited?
Instead of fingerprint or face authentication, phones could use their microphones to authenticate their users.  Humming different tunes could be your password. 

Wow. That was easy.  What a novel idea thatâ€™s never been attempted before. 
Thatâ€™s not impressive in the slightest. My iPhone 7 Plus only has 3% battery wear after a year. 
I donâ€™t understand this mentality. 

A good feature today doesnâ€™t suddenly become worse to use because a future product improves on it. It still works the same. 
Considering how much of a beast the A11 is, I think the iPhone X is going to last a long time. 
> Innovative (of a product, idea, etc.) featuring new methods; advanced and original.

Yeah, thatâ€™s incredibly easy to do. Itâ€™s easy to be novel, itâ€™s hard to actually improve something in a meaningful way. 

Just because somethingâ€™s novel and unique, doesnâ€™t mean itâ€™s any good or has actually improved anything. 

What do *you* think innovation means, sweetie?
â€œInnovativeâ€ is a very overrated term. 

Itâ€™s easy to be innovative.  Itâ€™s harder to make something thatâ€™s genuinely better than what came before it. 

Was the Motorola Atrix â€œInnovativeâ€ when it implemented the worldâ€™s first fingerprint scanner in a phone? Sure. Was it any good? Hell no.  It was slow, terrible finicky and inaccurate.  It offered no substantial value over traditional PINs, considering how often it didnâ€™t work. 

Appleâ€™s Touch ID may not have been first to market or â€œInnovative,â€ but it was the first to work *well* and set the standard for how all fingerprint authentication should work. 

If Apple wanted to, Iâ€™m sure they could be an incredibly â€œinnovativeâ€ company by pushing out products with brand new never before seen technology that isnâ€™t ready for consumer use yet. 

Theyâ€™d make for some shitty products, but itâ€™s certainly satisfy the nerds online who think being first to something means anything. 
Name one phone besides the iPhone X that uses 3D facial reconstructions as itâ€™s method of authentication. 

Edit: Hey, android dorks, maybe instead of downvoting you could actually name any other phone besides the iPhone X that has this technology. 
Thatâ€™s not true. 

3D depth mapping of your face has never appeared on any windows phone, Hello enabled or otherwise. Hello for Windows Phone has only ever gotten iris sensors and basic 2D face scanning, both of which can be spoofed using only photographs and contact lenses. 
The Face ID unlock only failed because handlers backstage tried to open the device, not realizing only Craig Federighi was authorized to unlock the device. Face ID worked as intended. 

Also, letâ€™s not forget that Face ID worked flawlessly in the hands-on media area after the presentation.

Face ID is the most advanced face authentication system ever put in a smart phone. Unlike Samsungâ€™s facial ID, which can be fooled with a photograph, Face ID creates a 3D scan of your face to authenticate. 
Looks like I struck a nerve. 

Oops. 
Itâ€™s a common feature in many IR based facial scanning technologies. Apple also made mention of it in their presentation. 
How self absorbed do you have to be to have this thought race through your head in the middle of playing this?
I donâ€™t think anyone who plays Destiny is in a position to be shitting on other peoplesâ€™ gaming preferences. 
Face ID also measures your skin texture, which cannot be easily recreated through any currently known 3D process. 
I guess it depends on how you like to use your camera. If you like to take a lot of landscape/scenery photos, ultra-wide is a great choice, since it captures more of a scene and gives a better sense of space and scale. 

If, however, you take more photos of people, zoom is the better choice. The â€œtelephotoâ€ lenses are better at getting detail in faces and fabrics, and wonâ€™t turn your subjects into eggheads, which ultra-wides tend to do since they warp at the edges. 
Hackers have been able to force the phone to record in 4K60, but at the cost of a few things, which is probably why Samsung disabled it. 

You canâ€™t have EIS assist, HDR, or fast auto focus.  The exynos and snapdragon simply canâ€™t handle all that. 
Apparently, the false positive rate for Face ID is substantially lower than fingerprint authentication. Unless you have an identical twin. 
The Master Sword; legendary sentient blade of evilâ€™s bane, wielded by Hyruleâ€™s chosen heroes throughout millennia, and glows with a sacred luster that can oppose the Calamity.  

Thatâ€™ll be $250 bucks, please...
Seriously. 

Put all the flagship androids combined, and their yearly annual sales donâ€™t even come close to the iPhone.  The vast majority of android sales come from cheap, garbage handsets. 
No, the bottom pic and the top pic simply show different moments in time. One moment where her foot is in frame, the other moment where itâ€™s cut off, you can tell because of Dianaâ€™s position relative to the islands in the background. 

Please, for the love of god, think about this for a second. What purpose would there ever be to letterbox an image but still crop out content?
The article makes no mention that it needs to match the curvature of the subjectâ€™s eye. It just says that the contact is there to simulate the curvature of eyes in general. 

The iris scanner is looking for curvature. Thatâ€™s all. 
All contact lenses match the curvature of the eye, thatâ€™s how youâ€™re able to put them on. 

Sorry, but your Samsungâ€™s iris recognition just isnâ€™t that safe.  Deal with it. 
[http://bgr.com/2017/05/23/galaxy-s8-iris-unlock-hacked-video/](http://bgr.com/2017/05/23/galaxy-s8-iris-unlock-hacked-video/)
You donâ€™t need to do any of that. 

Just get their photo, doesnâ€™t even have to be that high quality, get some cheap disposable lenses, print out the photo and affix the lenses over the eyes. Thatâ€™s it. 
Samsungs face scanning tech can be fooled by a photograph. And their iris scanning tech can be fooled by a photograph and a pair of contact lenses. 
Islamlphobia, or their misogynistic culture that enforces modesty on women through societal pressure, indoctrination, or violence. 
The above picture is letterboxed and shows the full content of the media. 

What exactly do you think is the point of letterboxing in the first place?
Face ID was stated to have no problems with hats, scarves, haircuts, or facial hair.  Weight/loss gain is gradual, and the neural engine will likely start adapting to it without needing a recalibration. 

> What stops a judge or police officer from unlocking my phone with my face without my permission?

The same thing that stop police officers from getting your fingerprint without your permission. You could also just looks away or close your eyes. Face ID requires active attention. 

> Can people look at my phone passively and it treats them as failed unlock attempts? 

How often are people within the focal point of your front cameraâ€™s view when raising to unlock?

I think he was referring to what Android manufactures have done. 
The iOS UI and functionality is changing for the iPhone X. 
Just consider the fact that ever since the introduction of Touch ID and the secure enclave 4 years ago, no one has ever lifted out the hash of the fingerprint information. 
I think women wearing those beekeeper suits have more things to worry about then nice alternatives to passwords on their iPhones. 
Face ID requires active attention. By closing your eyes or looking somewhere else, it wonâ€™t work. 

Also, Apple couldnâ€™t get professional Hollywood mask makers to fool the system. I doubt any police department could. 
That was an iPhone 5c, which had hardware and software that was obsolete even at that time. 

The exploit used to crack the iPhone 5c wouldnâ€™t work on any newer phones due to the secure enclave. 

Look, there are plenty of reasons to not like Apple products. Security is not one of them. 
There have been extensive studies about the iPhoneâ€™s secure enclave (the bit in their processors that stores biometric data and passwords) and nothingâ€™s ever been found that works of suggest dataâ€™s been leaking out of it. 
Youâ€™re a fucking liar. 
Uh, no.  The point of letterboxing is to preserve content. Thatâ€™s why it exists. 
And? 

That doesnâ€™t mean itâ€™s UI is finalized. Appleâ€™s UI guidelines specifically advise against that practice. 
No. By default, videos play in their normal 16:9 ratios through letterboxing. 
â€œNo.â€

 - Tim Cook, swimming in a pile of $100 bills. 
The iPhone X is twice as fast as the S8. Not to mention it doesnâ€™t run shitty TouchWiz. 
The latter option doesnâ€™t cut out anything. 
Beta software. 
You canâ€™t compare screen sizes with bringing up the aspect ratio. 

Although, I think the iPhone X does indeed have slightly more total area. 
Not for 16:9 media, as the screen is slightly narrower. 
Appleâ€™s method of facial authentication on mobile phones is new.  There is no other phone that has ever used 3D facial mapping of the face and skin textures. 
Premium items donâ€™t go on sale. 
You need to brush up on your reading comprehension. 

â€œ1. make changes in something established, especially by introducing new methodsâ€
I wouldnâ€™t worry too much about it.  Samsung likes to pretend their products are seen as â€œpremium,â€ but they arenâ€™t. They go on huge sales and incorporate giveaways a month or two after launch because no one wants to buy their inventory at those â€œweâ€™re just as cool as Appleâ€ prices. 

Premium items donâ€™t go on sale. 
Augmented reality, fast Portrait Lighting, fast 3D facial reconstruction and authentication, gets to idle faster. 
If someone demands â€œproofâ€ to accept something as innocuous as â€œplastic gaskets are softer than aluminumâ€ such a person is a serious fucking retard. 

I really pity such a person who needs to go to such obtuse lengths to protect their fragile ego. 

This hypothetical person should really pick up a physics book. 
Plastic is softer than aluminum.  This is a fact, not an opinion.  

Thatâ€™s really all you need to know. It acts as a bumper between the aluminum frame and the glass, when the phone is dropped on itâ€™s side and the frame impacts with the ground. 

Without the bumper, the aluminum would transfer the full kinetic energy of the drop into the screen, which can potential shatter it. With the soft plastic gasket, the aluminum frame will transfer itâ€™s energy into the gasket, which absorbs some of that energy, and therefore passes on a lower amount of kinetic energy into the glass then what it would have had the gasket not been there. 

This is simple physics.  Itâ€™s how bumpers work. 
Yeah, when the phone is dropped on itâ€™s side. If a phone is dropped on itâ€™s side, it transfers much of that kinetic energy into the glass, since they are connected. Thatâ€™s why a plastic gasket helps, since the aluminum frame will instead transfer itâ€™s kinetic energy into the plastic, where some of that kinetic energy is lost as heat before getting transferred to the glass. 

Quit being stupid. 

These arenâ€™t opinions, these are facts about physics, kid. 
The bumpers arenâ€™t there to protect against the ground, but to protect against the aluminum frame transferring too much kinetic energy into the glass. 

I havenâ€™t met anyone this dense in a long time. 
What are you talking about? The plastic liner is in between the glass and the aluminum. Itâ€™s not there to protect the glass against direct impacts with the ground, but against the aluminum frame, which transfers kinetic energy pretty easily. 

I canâ€™t believe I have to spell this out for you. 
Itâ€™s not an opinion that soft plastics are softer than anodized aluminum. 

I donâ€™t understand the need to be so purposefully obtuse.  Is it an ego thing? 
I didnâ€™t say iPhones survive cracks easier. There are other factors at play, such as shape and composition of the glass. 

Do you get it?
Itâ€™s simple physics. Hard things transfer kinetic energy much easier than soft squishy things, where much of that kinetic energy is lost as heat. 

Do you not understand how bumpers work? 
Seriously, what the hell is going on here. I feel like Iâ€™m in a loony bin. 
Itâ€™s not an opinion that the plastic gaskets Apple uses are softer than aluminum.  Are you fucking serious?


Do you not see the irony here?
Plastic is softer and provides more give than bare metal. 

What do you think absorbs more force from an impact: a plastic gasket or aluminum?
Galaxies crack all the time. 

Plastic is much softer than bare metal, so it slightly helps by absorbing shocks from impacts. Common sense. 
It helps prevent the screen from cracking when dropped on its side. 
Itâ€™s not that Apple canâ€™t forgo a plastic liner, they choose to have it because itâ€™s better for preventing screen cracks. 
The handlers tried to open it. Didnâ€™t work. And I guess they didnâ€™t bother to tell Craig. 
Such as?

Before you say videos, the default has videos properly sized. 
The hands on event area had hundreds of people in close quarters, yet every preview only has positive things to say about it. 
Thatâ€™s because they are incorrectly counting the curved stainless steel edges as part of the bezel, which is crazy. The bezel is only the black coverglass. And if thatâ€™s how youâ€™re gonna measure the screen to body ratio, the Mi Mix definitely wouldnâ€™t beat it out. 
How do we know that? 

From trusted sources, like Mark Gurman, Apple knew as far back as a year ago that Face ID would be the future And that Touch ID was over. 
Perceiving that with depth sorta mitigates that.  
It failed once. However, Apple later explained that the device had been handled earlier backstage by crew members. They had tried to open the phone with their face, causing it to lock up. 

Or are we forgetting how it performed flawlessly every other time in the presentation and in the hands on event later?
Youâ€™re right, itâ€™s because of the metal chassis of the iPhone X. However, itâ€™s curved so it doesnâ€™t look that big in real life. 
Sources close to Apple, like Mark Gurman have stated that Face ID has been in development for over a year, and that Touch ID is not coming back. 
Whether or not a feature is available first, and whether it actually works well are two entirely different things. 

For example, facial unlock systems have been on android phones for years. But Appleâ€™s Face ID is poised to become the standard. 
Shhhh! Donâ€™t disrupt the narrative! 
Exactly. 

The iPhone X has a very curvy chassis. If you look straight on, youâ€™ll see the edge of the curved chassis jut out. But you donâ€™t count this as part of the bezel. The black cover glass is the bezel, not the metal sides that are further down  
Literally unplayable. Preorder cancelled. 
â€œ1% difference in bezels is significant!â€

 - SinkTube, 9/15/17*


(*paraphrase)
Thatâ€™s Reddit for you. Lots of insecure android users being salty in September isnâ€™t a new thing. 
Apple does that all the time, especially with battery life estimates. Under promise and over deliver. 
The technology for Windows Hello enables phones are completely different from Appleâ€™s Face ID. Whereas Windows Hello equipped phones unlock via iris scanning and/or fingerprint sensors, Face ID takes a 3D map of your face. Face ID works in all light conditions and canâ€™t be fooled with a simple photograph of your face. 
How embarrassing. 
And this is only for the iPhone 5s, which is more than three years old. 
It took them all that to decrypt which was even then an outdated iPhone on an older version of iOS. 